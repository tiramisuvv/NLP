
# 1. Introduction

## 1.1 å®šä¹‰

NLP = Natural language processing 


## 1.2 How do we represent the meaning of a word?

**meaning**

- ç”¨ä¸€ä¸ªè¯ï¼Œè¯ç»„è¡¨ç¤ºçš„æ¦‚å¿µ
- ä¸€ä¸ªäººæƒ³ç”¨è¯­éŸ³ã€ç¬¦å·ç­‰è¡¨è¾¾çš„æƒ³æ³•
- æ–‡ç« ï¼Œè‰ºæœ¯ç­‰ä½œå“ä¸­è¡¨è¾¾çš„æ€æƒ³

<img src="./word2vec/semantics_def.png" alt="semantics_def" style="zoom:30%;" />

## 1.3 How do we have usable meaning in a computer?

### 1.3.1 WordNet

#### Idea ğŸ’¡ 

- **ç”¨åŒ…å«åŒä¹‰è¯ï¼Œä¸Šä½è¯(hypernyms)çš„è¯å…¸**

#### ä¾‹å­ ğŸŒ°

![wordnet](./word2vec/wordnet.png)

#### ç¼ºç‚¹ â

- å¿½ç•¥è¯ä¹‹é—´çš„ç»†å¾®å·®åˆ«

  - 'proficient' ä¸å®Œå…¨ç­‰åŒäº 'good'

- ç¼ºå°‘å•è¯çš„æ–°å«ä¹‰,ä¸å¯èƒ½æŒç»­æ›´æ–°

- éœ€è¦äººå·¥åˆ›é€ ï¼Œè°ƒæ•´

- ä¸èƒ½è®¡ç®—å•è¯ç›¸ä¼¼åº¦

  
### 1.3.2 one-hot vector

#### Idea ğŸ’¡ 

åœ¨ä¼ ç»ŸNLPé‡Œï¼ŒæŠŠå•è¯ä½œä¸ºç¦»æ•£ç¬¦åˆï¼šç”¨one-hot vectorè¡¨ç¤º

#### Definition

ç”¨åªæœ‰ä¸€ä½ä¸º1ï¼Œå…¶ä½™éƒ½æ˜¯0çš„ï¼Œé•¿åº¦ä¸º `|V|` çš„å‘é‡è¡¨ç¤ºï¼Œ(å…¶ä¸­`|V|` = è¯æ±‡é‡ï¼‰ã€‚

#### ä¾‹å­ ğŸŒ°

```
motel = [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]
hotel = [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]
```

#### ç¼ºç‚¹ â

- ä¸èƒ½è¡¨è¾¾è¯ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚
  -   ä»»æ„ä¸¤ä¸ªone-hot å‘é‡éƒ½å‚ç›´
-   å‘é‡ç»´åº¦å¾ˆå¤§ï¼Œéå¸¸sparse

#### Solution âœ…

- ç”¨ WordNet ä¸­çš„åŒä¹‰è¯æ¥è·å¾—ç›¸ä¼¼æ€§ï¼ˆä½†ä¸å¯è¡Œï¼Œæ¯”å¦‚WordNetä¸­ä¸å¯èƒ½æ”¶å½•æ‰€æœ‰åŒä¹‰è¯ï¼‰  
- **Instead : learn to encode similarity in the vectors themselves**




## 1.4 Distributional semantics
#### Idea ğŸ’¡ æ ¹æ®ä¸Šä¸‹æ–‡æ¨æµ‹å•è¯å«ä¹‰
#### Definition

- **Distributional semantics** := A wordâ€™s meaning is given by the words that frequently appear close-by

#### ä¾‹å­ ğŸŒ°

![banking_meaning](./word2vec/banking_meaning.png)banking_meaning.jpg

- **context** = When a word w appears in a text, its ***context*** is the set of words that appear nearby (within a fixed-size window).

- If you can explain what context it's correct to use a certain word, versus in what context would be the wrong world to use. => understand the meaning of the word 

- ##### æ³¨æ„âš ï¸ è¿™é‡Œå‡è®¾ï¼šä¸­å¿ƒè¯cçš„å«ä¹‰åªä¸ä¸Šä¸‹æ–‡ç›¸å…³ï¼Œi.e. ä¸è¿œè·ç¦»çš„å•è¯æ— å…³ã€‚

  - åœ¨å®é™…ä¸­ï¼Œä¸å®Œå…¨æˆç«‹ã€‚æ¯”å¦‚äººç§°ä»£è¯å¯èƒ½æŒ‡ä»£çš„æ˜¯å‡ å¥ç”šè‡³å‡ æ®µä¹‹å‰çš„äººç‰©ã€‚

## 1.5 è¯­è¨€æ¨¡å‹(Language Models)
#### Idea ğŸ’¡

è¯­è¨€æ¨¡å‹æ˜¯ç”¨æ¥è®¡ç®—ä¸€ä¸ªå¥å­çš„æ¦‚ç‡çš„æ¦‚ç‡æ¨¡å‹ã€‚æ¯”å¦‚*"The cat jumped over the puddle."* è¿™æ ·å®Œæ•´ä¸”åˆç†çš„å¥å­åº”è¯¥æœ‰é«˜çš„æ¦‚ç‡ï¼Œè€Œ *"stock boil fish is toy"* æœ‰ä½æ¦‚ç‡ã€‚

å¯¹äºä»»æ„ä¸€åˆ—nä¸ªå•è¯ï¼Œå®ƒçš„æ¦‚ç‡ç”¨ $P(w_1, w_2, ..., w_T)$ è¡¨ç¤ºã€‚

ä¸€èˆ¬çš„ï¼Œæ ¹æ®Bayes å…¬å¼ï¼Œ
$$
P(w_1,...,w_T) = P(w_1)P(w_2|w_1)P(w_3|w_1,w_2)...P(w_T|w_1,...,w_{T-1})
$$
å…¶ä¸­æ¡ä»¶æ¦‚ç‡ $p(w1), p(w_2|w_1), ..., p(w_T|w_1,...,w_{T-1})$ æ˜¯æ¨¡å‹çš„å‚æ•°ã€‚å¯¹äºä¸€ä¸ªé•¿åº¦ä¸º$T$ çš„å¥å­ï¼Œéœ€è¦è®¡ç®— $T$  ä¸ªå‚æ•°ã€‚åœ¨è¯æ±‡é‡ä¸º$|V|$ çš„è¯­æ–™åº“ï¼Œä»»æ„é•¿åº¦ T çš„å¥å­å…±ç”¨ $|V|^T$ ç§å¯èƒ½ï¼Œè¿›ä¸€æ­¥å°±æœ‰ $T \cdot |V|^T$ ä¸ªå‚æ•°ã€‚ï¼ˆè¿™é‡Œå¿½ç•¥äº†é‡å¤å‚æ•°ï¼Œåªçœ‹é‡çº§å°±å¥½ï¼‰ã€‚

### 1.5.1 Unigram æ¨¡å‹

å‡è®¾å•è¯ä¹‹é—´å®Œå…¨ç‹¬ç«‹ï¼Œé‚£ä¹ˆ
$$
P(w_1,...,w_T) = \prod_{i = 1}^T P(w_i)
$$
æ˜¾ç„¶ï¼Œæˆ‘ä»¬çŸ¥é“è¿™ä¸ªæ˜¯ä¸åˆç†çš„ï¼Œå› ä¸ºå½“å‰å•è¯è‚¯å®šæ˜¯ä¾èµ–äºå‰ä¸€ä¸ªå•è¯çš„ã€‚

### 1.5.2 Bigram æ¨¡å‹

å‡è®¾å½“å‰å•è¯åªä¾èµ–äºå®ƒå‰ä¸€ä¸ªå•è¯ï¼Œé‚£ä¹ˆ
$$
P(w_1,...,w_T) = \prod_{i = 2}^TP(w_i|w_{i-1})
$$

### 1.5.3 N-gram æ¨¡å‹

ç±»ä¼¼çš„ï¼Œå‡è®¾å½“å‰å•è¯ä¾èµ–äºå®ƒå‰n-1ä¸ªå•è¯ï¼Œç§°ä¸ºN- gram æ¨¡å‹ã€‚ç±»ä¼¼çš„
$$
P(w_1, ..., w_T) = \prod_{i=n}^{T} P(w_i|w_{i-n+1},...,w_{i-1})
$$
è¿›ä¸€æ­¥ï¼Œæ ¹æ®å¤§æ•°å®šå¾‹ï¼Œå½“è¯­æ–™åº“è¶³å¤Ÿå¤§æ—¶ï¼Œ
$$
P(w_i|w_{i-n+1}, ..., w_{i-1}) \approx \frac{\text{count}(w_{i-n+1}, ..., w_i)}{\text{count}(w_{i-n+1}, ..., w_{i-1})}
$$
ä»¥ä¸Šé¢Bigram ä¸ºä¾‹ (n=2)ï¼Œæœ‰
$$
P(w_i|w_{i-1}) \approx \frac{\text{count}(w_{i-1},w_i)}{\text{count}(w_{i-1})}
$$
ä¸ä»…ä½¿å¾—å•ä¸ªå‚æ•°çš„ç»Ÿè®¡å˜å¾—æ›´å®¹æ˜“ï¼ˆç»Ÿè®¡æ—¶éœ€è¦åŒ¹é…çš„è¯ä¸²æ›´çŸ­ï¼‰ï¼Œä¹Ÿä½¿å¾—å‚æ•°çš„æ€»æ•°å˜å°‘äº†ã€‚

|     n      |   æ¨¡å‹å‚æ•°çš„æ•°é‡   |
| :--------: | :----------------: |
| 1(unigram) |   $2\times 10^5$   |
| 2(bigram)  | $4\times 10^{10}$  |
| 3(trigram) | $8\times 10^{15}$  |
| 4(4-gram)  | $16\times 10^{20}$ |

ä¸Šè¡¨ç»™å‡ºäº†n-gramæ¨¡å‹ä¸­æ¨¡å‹å‚æ•°æ•°é‡éšç€çš„é€æ¸å¢å¤§è€Œå˜åŒ–çš„æƒ…å†µï¼Œå…¶ä¸­å‡å®šè¯æ±‡é‡ä¸º $|V| = 200,000$(æ±‰è¯­çš„è¯æ±‡é‡å¤§è‡´æ˜¯è¿™ä¸ªé‡çº§)ã€‚å®é™…åº”ç”¨ä¸­ï¼Œæœ€å¤šé‡‡ç”¨n=3çš„trigramæ¨¡å‹ã€‚

#### å¹³æ»‘åŒ–

è€ƒè™‘ä¸¤ä¸ªé—®é¢˜ï¼ˆæç«¯æƒ…å†µï¼‰ï¼š

- å¦‚æœ $\text{count}(w_{i-n+1}, ..., w_i) = 0$ï¼Œæ˜¯å¦è®¤ä¸º $P(w_i|w_{i-n+1}, ..., w_{i-1}) = 0$?

- å¦‚æœ$\text{count}(w_{i-n+1}, ..., w_i) = \text{count}(w_{i-n+1}, ..., w_{i-1}) $, æ˜¯å¦è®¤ä¸º $P(w_i|w_{i-n+1}, ..., w_{i-1}) = 1$?



ã€TODOã€‘å¹³æ»‘åŒ–

æ€»ç»“èµ·æ¥ï¼Œn-gramæ¨¡å‹æ˜¯è¿™æ ·ä¸€ç§æ¨¡å‹ï¼Œå…¶ä¸»è¦å·¥ä½œæ˜¯åœ¨è¯­æ–™ä¸­ç»Ÿè®¡å„ç§è¯ä¸²å‡ºç°çš„æ¬¡æ•°ä»¥åŠå¹³æ»‘åŒ–å¤„ç†ã€‚æ¦‚ç‡å€¼è®¡ç®—å¥½ä¹‹åå°±å­˜å‚¨èµ·æ¥ï¼Œä¸‹æ¬¡éœ€è¦è®¡ç®—ä¸€ä¸ªå¥å­çš„æ¦‚ç‡æ—¶ï¼Œåªéœ€æ‰¾åˆ°ç›¸å…³çš„æ¦‚ç‡å‚æ•°ï¼Œå°†å®ƒä»¬è¿ä¹˜èµ·æ¥å°±å¥½äº†ã€‚



### 1.5.4 ç›®æ ‡å‡½æ•°

å¯¹äºç»Ÿè®¡è¯­è¨€æ¨¡å‹è€Œè¨€ï¼Œåˆ©ç”¨æœ€å¤§ä¼¼ç„¶ï¼Œå¯æŠŠç›®æ ‡å‡½æ•°è®¾ä¸ºï¼š
$$
\prod_{w \in C} P(w|\text{contex}(w))
$$
å…¶ä¸­ Cè¡¨ç¤º è¯­æ–™åº“ï¼ˆCorpus)ï¼ŒContex(w)è¡¨ç¤ºå•è¯wçš„ä¸Šä¸‹æ–‡



# 2. Word Embedding è¯åµŒå…¥


## 2.2 Word vector
### 2.2.1 å®šä¹‰

å¯¹äºè¯å…¸Dé‡Œçš„ä»»æ„å•è¯wï¼Œéƒ½æŒ‡å®šä¸€ä¸ªå›ºå®šé•¿åº¦çš„å‘é‡$v(w) \in \mathbb{R}^{n}$ï¼Œå°±ç§° $v(w)$ æ˜¯wçš„è¯å‘é‡ï¼Œnä¸ºè¯å‘é‡çš„é•¿åº¦ã€‚ 

- è¿™ä¸ªå‘é‡è¢«ç§°ä¸ºè¯å‘é‡ï¼ˆword vectors)ï¼Œè¯åµŒå…¥ï¼ˆword embeddingï¼‰æˆ–è€… word representations

æ˜¾ç„¶ï¼Œä¸Šé¢æåˆ°çš„ one-hot vector å°±æ˜¯æœ€ç®€å•çš„è¯å‘é‡ã€‚åŸºäºone-hotï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿå¾—åˆ°æ»¡è¶³ä¸‹é¢æ¡ä»¶çš„è¯å‘é‡ï¼š

- é•¿åº¦ $n \ll |V|$ 
- è¯å‘é‡æ˜¯ç¨ å¯†çš„
- è¯è¯­ä¹‹é—´çš„ç›¸ä¼¼æ€§å¯ä»¥é€šè¿‡è¯å‘é‡æ¥ä½“ç° => wçš„è¯å‘é‡ä¸å®ƒä¸Šä¸‹æ–‡ä¸­çš„å•è¯çš„å‘é‡ç›¸ä¼¼

### 2.2.2 ä¾‹å­ ğŸŒ°

<img src="./word2vec/banking_vector.png" alt="banking_vector" style="zoom:50%;" />

banking_vector.jpg

è¿›å…¥è®¡ç®—ï¼Œå­¦ä¹ è¯å‘é‡çš„ç®—æ³•å‰ï¼Œæˆ‘ä»¬çœ‹ä¸¤ä¸ªè¯å‘é‡çš„å¯è§†åŒ–ã€‚ï¼ˆä¸ºäº†æ–¹ä¾¿ï¼Œä¸‹å›¾æŠŠ9ç»´è¯å‘é‡æŠ•å°„åˆ°äºŒç»´ç©ºé—´ï¼‰

![word_vector_visualization](./word2vec/word_vector_visualization.png)word_vector_visualization.jpg

- å«ä¹‰ç›¸è¿‘çš„è¯ï¼Œæœ‰ä¸€å®šçš„èšé›†æ€§ï¼Œè¡¨ç°äº†è¯ä¹‹é—´çš„ç›¸ä¼¼æ€§

è¿™é‡Œçš„ **ç›¸ä¼¼æ€§**æ˜¯æŒ‡**â€œç»“æ„â€ç›¸ä¼¼æ€§**ï¼šå¦‚æœä¸¤ä¸ªè¯å‡ºç°åœ¨åŒæ ·çš„ä¸Šä¸‹æ–‡ï¼Œæˆ‘ä»¬å°±å½“åšå®ƒä»¬ç›¸ä¼¼ã€‚å¦‚ä¸Šå›¾come å’Œ goï¼Œå«ä¹‰ä¸Šæ˜¯åä¹‰è¯ï¼Œä½†åœ¨è¯å‘é‡ç©ºé—´å¾ˆæ¥è¿‘ã€‚

![word_vector_visualization2](./word2vec/word_vector_visualization2.png)

word_vector_visualization2.jpg

- åˆ†åˆ«ç”»å‡ºè‹±è¯­å’Œè¥¿è¯­çš„æ•°å­—å•è¯çš„è¯å‘é‡ï¼ŒåŒæ ·å‘ç°æœ‰å¾ˆé«˜çš„ç›¸ä¼¼æ€§ã€‚



## 2.3 Word2vec 

### 2.3.1 Overview
Word2vec (Mikolov et al. 2013) æ˜¯ä¸€ä¸ªå­¦ä¹ è¯å‘é‡çš„æ¡†æ¶ã€‚

#### Idea ğŸ’¡
- æˆ‘ä»¬æœ‰ä¸€ä¸ªå¤§çš„è¯­æ–™åº“ï¼ˆå¤§é‡æ–‡æœ¬ï¼‰ï¼ˆa large corpus of textï¼‰
- è¯æ±‡è¡¨ä¸­çš„æ¯ä¸ªå•è¯éƒ½ç”±ä¸€ä¸ªå‘é‡è¡¨ç¤º
- éå†æ–‡æœ¬ä¸­çš„æ¯ä¸ªä½ç½® tï¼Œéƒ½æœ‰å¯¹åº”çš„ä¸­å¿ƒè¯ï¼ˆcenter wordï¼‰ c å’Œä¸Šä¸‹æ–‡ï¼ˆ context (â€œoutsideâ€)ï¼‰ o
- é€šè¿‡å•è¯ c å’Œ o çš„è¯å‘é‡çš„ç›¸ä¼¼åº¦ï¼Œæ¥è®¡ç®—ç»™å®šcå‡ºç°oçš„æ¦‚ç‡ï¼ˆåä¹‹äº¦ç„¶ï¼‰
- é€šè¿‡ä¸æ–­è°ƒæ•´è¯å‘é‡ï¼Œæœ€å¤§åŒ–è¿™ä¸ªæ¦‚ç‡



"å®Œå½¢å¡«ç©º"

<img src="/Users/Wei/Library/Application Support/typora-user-images/Screen Shot 2021-06-20 at 8.55.21 AM.png" alt="Screen Shot 2021-06-20 at 8.55.21 AM" style="zoom:50%;" />

ä¸‹å›¾ä¸º$P(w_{t+j}|w_t)$çš„è®¡ç®—è¿‡ç¨‹ï¼Œwindow size = 2ï¼Œ ä¸­å¿ƒè¯ = into

![into_example](./word2vec/into_example.png)into_example.jpg

<img src="./word2vec/banking_example.png" alt="banking_example" style="zoom:70%;" />

banking_example



### 2.3.2 Details

Word2vec åŒ…å«ï¼š
- **ä¸¤ä¸ªå­¦ä¹ è¯å‘é‡çš„æ¨¡å‹** 
  
  - CBOW é€šè¿‡ä¸Šä¸‹æ–‡é¢„æµ‹ä¸­å¿ƒè¯
  - Skip-gram é€šè¿‡ä¸­å¿ƒè¯é¢„æµ‹ä¸Šä¸‹æ–‡

 cbow&skipgram.png.jpg

![cbow&skipgram](./word2vec/cbow&skipgram.png) ç”±å›¾å¯è§ï¼Œä¸¤ä¸ªæ¨¡å‹éƒ½åŒ…å«ä¸‰å±‚ï¼š**è¾“å…¥å±‚**ã€**æŠ•å½±å±‚**å’Œ**è¾“å‡ºå±‚**

- **ä¸¤ä¸ªè®­ç»ƒæ–¹æ³•** é™ä½æ¨¡å‹å­¦ä¹ è¿‡ç¨‹ä¸­çš„è¿ç®—é‡
  
  - negative sampling è´Ÿé‡‡æ ·
  - hierarchical softmax

### 2.3.3 Continuous Bag of Words Model (CBOW)

#### 2.3.3.1 ç»“æ„
1. simple CBOW
simple_cbow.jpg

2. Multi-Word Context Model
multi_cbow.jpg

cbow.jpg

**ç›®æ ‡** å¯¹æ¯ä¸€ä¸ªå•è¯ wï¼Œæˆ‘ä»¬è¦å­¦ä¹ 2ä¸ªå‘é‡
- v (è¾“å…¥å‘é‡ï¼‰å½“ w åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼›
- uï¼ˆè¾“å‡ºå‘é‡ï¼‰å½“ w æ˜¯ä¸­å¿ƒè¯ 

**ä¾‹å­** ğŸŒ°
æœ‰ä¸‹é¢çš„å¥å­ï¼š

*"The cat jumped over the puddle."*

å¸Œæœ›é€šè¿‡ä¸Šä¸‹æ–‡ {"The", "cat", "over", "the", "puddle"} æ¥é¢„æµ‹/ç”Ÿæˆä¸­å¿ƒè¯ "jumped"

***In math*** å¸Œæœ› é€šè¿‡{"The", "cat", â€™over", "theâ€™, "puddle"}çš„è¯å‘é‡å¾—åˆ°å‘é‡ = "jumped"çš„è¯å‘é‡

**Notation for CBOW Model**
- `w_i` : Word i from vocabulary V
- `V \in R^{n\times |V|}: Input word matrix
- `v_i` : i-th column of V , the input vector representation of word `w_i`
- `U \in R^{|V|\times n}: Output word matrix
- `u_i` : i-th row of U , the output vector representation of word `w_i`

æˆ‘ä»¬ç”¨ `w_{c-m}, ..., w_{c-1}, w_{c+1},...,w_{c+m}` ä»£è¡¨ä¸Šä¸‹æ–‡ï¼Œ `w_c`ä»£è¡¨ä¸­å¿ƒè¯ã€‚å‡è®¾å·²æœ‰æ‰€æœ‰è¯çš„one-hot vectorè¡¨ç¤ºã€‚
Input:x, output:y

1. ç”Ÿæˆä¸Šä¸‹æ–‡ä¸­çš„å•è¯çš„ one-hot å‘é‡: `x^{c-m}, ..., x^{c-1}, x^{c+1},...,x^{c+m} \in R^{|V|}`.
2. å¾—åˆ°ä¸Šä¸‹æ–‡çš„è¯å‘é‡ï¼š`v_{c-m} = Vx^{c-m}, ..., v_{c-1} = Vx^{c-1}, v_{c+1} = Vx^{c+1},...,v_{c+m} = Vx^{c+m} \in R^{n}`
3. è®¡ç®—å¹³å‡å€¼ `\hat{v} = \frac{v_{c-m} +...+ v_{c-1} + v_{c+1} + ...+v_{c+m}}{2m} \in R^{|V|}`

  1. æœ‰çš„åœ°æ–¹ç›´æ¥æ±‚å’Œï¼Œä¸å½±å“

4. ç”Ÿæˆåˆ†æ•°å‘é‡ `z = U\hat(v) \in R^{|V|}`

  - è¿™æ ·ï¼Œç”±äºç›¸ä¼¼å‘é‡çš„ç‚¹ç§¯å¤§ï¼Œæ‰€ä»¥ä¸ºäº†å¾—åˆ°é«˜åˆ†ï¼Œä¼šå‘ç›¸ä¼¼å•è¯çš„è¯å‘é‡é è¿‘ã€‚

6. é€šè¿‡softmaxå‡½æ•°æŠŠåˆ†æ•°è½¬åŒ–ä¸ºæ¦‚ç‡ `\hat{y} = \text{softmax}(z) \in R^{|V|}`
7. æˆ‘ä»¬å¸Œæœ›ç”Ÿæˆçš„æ¦‚ç‡`\hat{y}\in R^{|V|}` ä¸çœŸå® `y\in R^{|V|}` ç›¸åŒ¹é…ï¼Œæ³¨æ„è¿™é‡Œ`y` ä¹Ÿæ˜¯one-hotã€‚

#### 2.3.3.2 ç›®æ ‡å‡½æ•° Obejective function 
For each position ğ‘¡ = 1, ... , ğ‘‡, predict center word `w_c`, given the context words within a window of fixed size m `w_{c-m}, ..., w_{c-1}, w_{c+1},...,w_{c+m}`. 

Data likelihood:

``
L(\theta) = \Pi_{c = 1} ^T  P(w_c|w_{c-m}, ..., w_{c-1}, w_{c+1},...,w_{c+m})
``

``
L(\theta) = \Pi_{t = 1} ^T \Pi_{-m\leq j \leq m, j\neq 0} P(w_c|w_{c-m}, ..., w_{c-1}, w_{c+1},...,w_{c+m})
``


- `\theta` åŒ…å«æ‰€æœ‰éœ€è¦ä¼˜åŒ–çš„å‚æ•°ï¼ˆi.e. ä¸¤ä¸ªè¯å‘é‡çŸ©é˜µï¼‰

The objective function `J_{\theta}` is the (average) negative log likelihood:

``
J = - \frac{1}{T}\log L(\theta) 
``

**Question: How to calculate `P(o|c)`

- `ğ‘ƒ(ğ‘¢_{ğ‘ğ‘Ÿğ‘œğ‘ğ‘™ğ‘’ğ‘šğ‘ } | ğ‘£_{ğ‘–ğ‘›ğ‘¡ğ‘œ}) short for P(ğ‘ğ‘Ÿğ‘œğ‘ğ‘™ğ‘’ğ‘šğ‘  | ğ‘–ğ‘›ğ‘¡ğ‘œ ; ğ‘¢_{ğ‘ğ‘Ÿğ‘œğ‘ğ‘™ğ‘’ğ‘šğ‘ }, ğ‘£_{ğ‘–ğ‘›ğ‘¡ğ‘œ},\theta)


prob.jpg

#### 2.3.3.3 Gradiant descent and stochastic gradient descent


### 2.3.4 Skip-gram

![into_example](./word2vec/into_example.png)



### 2.3.4.1 ç›®æ ‡å‡½æ•°  Obeject function 

å¯¹ä»»æ„çš„ä½ç½® $ğ‘¡ = 1, ... , ğ‘‡$, ç»™å®šä¸­å¿ƒè¯ wï¼Œé¢„æµ‹å®ƒçš„ä¸Šä¸‹æ–‡ï¼ˆwindow of fixed size mï¼‰ 

Likelihood:
$$
L(\theta) = \prod_{t= 1}^T\prod_{-m\leq j\leq m, j\neq 0}P(w_{t+j}|w_t; \theta)
$$
å…¶ä¸­ç”¨$\theta$ è¡¨ç¤ºæ‰€æœ‰ï¼ˆéœ€è¦ä¼˜åŒ–çš„ï¼‰å‚æ•°ï¼Œé‚£ä¹ˆç›®æ ‡å‡½æ•° $J(\theta)$ æ˜¯
$$
J(\theta) = - \frac{1}{T}\log L(\theta) = -\frac{1}{T}\sum_{t=1}^T\sum_{-m\leq j\leq m, j\neq 0}\log P(w_{t+j}|w_t; \theta)
$$


ä¸ºäº†æœ€å°åŒ–ç›®æ ‡å‡½æ•° $J(\theta)$ ï¼Œæœ‰ä¸‹é¢é—®é¢˜ï¼š

- **Questionï¼šå¦‚ä½•è®¡ç®—$P(w_{t+j}|w_t; \theta)$ï¼Ÿ**

- **Answer** å¯¹æ¯ä¸ªå•è¯wï¼Œä½¿ç”¨ä¸¤ä¸ªè¯å‘é‡

  - $v_w$ ï¼Œå½“wæ˜¯ä¸­å¿ƒè¯

  - $u_w$ï¼Œå½“wæ˜¯ä¸Šä¸‹æ–‡

  - å¯¹äºä¸­å¿ƒè¯ c å’Œä¸Šä¸‹æ–‡å•è¯ o(output)ï¼Œæœ‰ï¼š
    $$
    P(o|c) = \frac{\exp(u_o^Tv_c)}{\sum_{w\in V} \exp(u_w^Tv_c)}
    $$
  
  <img src="./word2vec/prob.png" alt="prob" style="zoom:50%;" />
  
   - Softmax function $\mathbb{R}^{n} \rightarrow (0, 1)^n$:
     $$
     \text{softmax}(x_i) = \frac{\exp(x_i)}{\sum_{j=1}^n \exp(x_j)}
     $$
  
     	- æŠŠä»»æ„çš„$x_i$ æ˜ å°„åˆ°ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒ $p_i$
     	- "max" :èƒ½å¤Ÿè®©vectorä¸­æœ€å¤§çš„æ•°$x_{max}$ è¢«å–åˆ°çš„æ¦‚ç‡éå¸¸å¤§
     	- "soft" å¯¹äºå°çš„å€¼$x_i$ ä»ç„¶æœ‰æ¦‚ç‡

### 2.3.4.2 è®­ç»ƒæ¨¡å‹

#### 2.3.4.2.1 Gradiant descent

- æ²¿ç€æ¢¯åº¦ä¸‹é™çš„æ–¹å‘ä¼˜åŒ–å‚æ•°<img src="./word2vec/gd.png" alt="gd" style="zoom:30%;" />(gd.jpg)

- **Idea** å¯¹ç°åœ¨çš„å‚æ•° $\theta$ ï¼Œè®¡ç®—ç›®æ ‡å‡½æ•° $J(\theta)$ çš„æ¢¯åº¦ï¼Œç„¶åæ²¿ç€è´Ÿå‘æ¢¯åº¦å°æ­¥å‘ä¸‹ã€‚ï¼ˆé‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œç›´åˆ°ç»“æŸï¼‰<img src="./word2vec/gr2.png" alt="gr2" style="zoom:30%;" />

  

- 
  $$
  \theta^{new} = \theta^{old} - \alpha \nabla_{\theta} J(\theta)
  $$

  $$
  \text{i.e.}\quad \theta^{new}_j = \theta^{old}_j - \alpha \frac{\partial}{\partial \theta} J(\theta) \quad \text{for any parameter } \theta_j
  $$

  

  

- è¿™é‡Œï¼Œæˆ‘ä»¬çš„å‚æ•°$\theta$ ï¼šæ€»å…±$|V|$ å•è¯ï¼Œæ¯ä¸ªå•è¯éƒ½åŒ…å«ä¸¤ä¸ªdç»´çš„å‘é‡ï¼Œæ‰€ä»¥<img src="./word2vec/theta.png" alt="theta" style="zoom:30%;" />(theta.jpg)
- ä¸‹é¢æˆ‘ä»¬éœ€è¦å¯¹**æ‰€æœ‰**å‘é‡æ±‚å¯¼

#### 2.3.4.2.2 æ±‚å¯¼ç»†èŠ‚

$$
J(\theta) = -\frac{1}{T}\sum_{t=1}^T\sum_{-m\leq j\leq m, j\neq 0}\log P(w_{t+j}|w_t; \theta)
\quad \text{with } P(o|c) = \frac{\exp(u_o^Tv_c)}{\sum_{w\in V} \exp(u_w^Tv_c)}
$$

$$
\begin{align}
\frac{\partial}{\partial v_c}\log  P(o|c) &=
\frac{\partial}{\partial v_c}\log \frac{\exp(u_o^Tv_c)}{\sum_{w\in V} \exp(u_w^Tv_c)} \\
&= \frac{\partial}{\partial v_c}(u_o^Tv_c)-\frac{\partial}{\partial v_c}\log {\sum_{w\in V} \exp(u_w^Tv_c)} \\
&= u_o - \frac{1}{{\sum_{w\in V} \exp(u_w^Tv_c)}}\cdot {\sum_{x\in V} \exp(u_x^Tv_c)}u_x.
\end{align}
$$

æ³¨æ„åˆ° $J(\theta)$ 
$$
\begin{align}
\frac{\partial}{\partial v_c}\log  P(o|c) 
&= u_o - \sum_{x\in V} \frac{{\exp(u_x^Tv_c)}}{{\sum_{w\in V} \exp(u_w^Tv_c)}}\cdot u_x\\
&=u_o-\sum_{x\in V}P(x|c)u_X \\
&= \text{Actual context word} - \text{Expected context word} 
\end{align}
$$
æ³¨æ„åˆ°ï¼Œ$u_o$ æ˜¯ä¸Šä¸‹æ–‡å¯¹åº”çš„è¯å‘é‡ï¼Œè€Œ$\sum_{x\in V}P(x|c)u_X$  æ˜¯ä»¥æœŸæœ›å½¢å¼å­˜åœ¨çš„ï¼Œæ¨¡å‹â€œé¢„æµ‹â€œçš„ç»“æœ



## 2.4 åŸºäºHierarchical Softmaxçš„æ¨¡å‹

å›å¿†CBOWå’Œ

## 2.5  åŸºäºNegative Samplingçš„æ¨¡å‹

å›å¿†
$$
J(\theta) = -\frac{1}{T}\sum_{t=1}^T\sum_{-m\leq j\leq m, j\neq 0}\log P(w_{t+j}|w_t; \theta)\quad \text{with } P(o|c) = \frac{\exp(u_o^Tv_c)}{\sum_{w\in V} \exp(u_w^Tv_c)}
$$
æ³¨æ„åˆ°åˆ†æ¯å¯¼è‡´æ¯æ¬¡è®¡ç®—ç›®æ ‡å‡½æ•° $J(\theta)$éƒ½éœ€è¦éå†æ•´ä¸ªè¯å…¸ Vï¼Œè®¡ç®—é‡æ˜¯$O(|V|)$ çº§åˆ«çš„ï¼ˆmillions)ã€‚ä¸€ä¸ªç®€å•çš„æƒ³æ³•æ˜¯æˆ‘ä»¬ä¸å»ç›´æ¥éå†è®¡ç®—ï¼Œæˆ‘ä»¬é¢„ä¼°è¿™ä¸ªå’Œã€‚é‚£ä¹ˆç±»æ¯”ä¸Šé¢çš„å†…å®¹æœ‰ä¸‰é¡¹éœ€è¦æ›´æ–°ï¼š

- ç›®æ ‡å‡½æ•° $J(\theta)$
- å¯¼æ•° $\nabla_{\theta} J(\theta)$
- æ›´æ–°è§„åˆ™

ï¼ˆä¸‹é¢ä»¥skip gramä¸ºä¾‹ï¼‰

### 2.5.1 Idea ğŸ’¡

è¿™é‡Œæˆ‘ä»¬æ”¹å˜é¢„æµ‹ç›¸é‚»å•è¯è¿™ä¸€ä»»åŠ¡ï¼š

- ï¼ˆå·¦å›¾ï¼‰æ˜¯åŸå§‹çš„ï¼Œæˆ‘ä»¬æ ¹æ®ä¸­å¿ƒè¯ not é¢„æµ‹å®ƒçš„ä¸Šä¸‹æ–‡ thou 

- ï¼ˆå³å›¾ï¼‰æ˜¯æ ¹æ®è¾“å…¥çš„ï¼ˆä¸­å¿ƒè¯cï¼Œå•è¯wï¼‰ï¼Œåˆ¤æ–­å®ƒä»¬æ˜¯å¦æ˜¯é‚»å±…ï¼ˆ0è¡¨ç¤ºâ€œä¸æ˜¯é‚»å±…â€ï¼Œ1è¡¨ç¤ºâ€œé‚»å±…â€ï¼‰ã€‚
  - w æ˜¯ä¸Šä¸‹æ–‡ï¼š label = 1
  - w æ˜¯éšæœºç”Ÿæˆçš„å™ªå£°ï¼šlabel = 0

<img src="./word2vec/neg_sampling1.png" alt="neg_sampling1" style="zoom:40%;" /><img src="./word2vec/neg_sampling2.png" alt="neg_sampling2" style="zoom:40%;" />

ä¹Ÿå°±æ˜¯æˆ‘ä»¬éœ€è¦çš„æ¨¡å‹ä»ç¥ç»ç½‘ç»œæ”¹ä¸ºé€»è¾‘å›å½’æ¨¡å‹â€”â€”å› æ­¤å®ƒå˜å¾—æ›´ç®€å•ï¼Œè®¡ç®—é€Ÿåº¦æ›´å¿«ã€‚

**Main idea**: train binary logistic regressions for a true pair (center word and a word in its context window) versus several noise pairs (the center word paired with a random word)

è¿™ä¸ªæ–¹æ³•éœ€è¦æˆ‘ä»¬æ›´æ–°æ•°æ®é›†çš„ç»“æ„ï¼šå¢åŠ æ ‡ç­¾åˆ—ï¼ˆ0æˆ–è€…1ï¼‰æ˜¾ç„¶ï¼Œå¯¹äº(ä¸­å¿ƒè¯ï¼Œä¸Šä¸‹æ–‡)ï¼Œæ ‡ç­¾åˆ—éƒ½æ˜¯1ã€‚

<img src="./word2vec/neg_sampling3.png" alt="neg_sampling3" style="zoom:75%;" />

ä¸ºäº†è®©æœºå™¨èƒ½å¤Ÿæ­£ç¡®å­¦ä¹ ï¼Œè¿™é‡Œæˆ‘ä»¬éœ€è¦å¼•å…¥è´Ÿæ ·æœ¬ \- ä¸æ˜¯é‚»å±…çš„å•è¯æ ·æœ¬ã€‚ä¸‹ä¸€ä¸ªé—®é¢˜å°±æ˜¯è´Ÿæ ·æœ¬è¦æ€ä¹ˆé€‰å–ï¼Ÿç®€å•çš„è¯´æˆ‘ä»¬ä»è¯æ±‡è¡¨ä¸­åŸºäºè¯é¢‘éšæœºæŠ½å–å•è¯

<img src="./word2vec/neg_sampling4.png" alt="neg_sampling4" style="zoom:75%;" />

neg_sampling1.jpg
neg_sampling2.jpg
neg_sampling3.jpg
neg_sampling4.jpg

### 2.5.2 ç›®æ ‡å‡½æ•° $J(\theta)$

è€ƒè™‘ä¸€å¯¹ä¸­å¿ƒè¯cå’Œå¦ä¸€ä¸ªå•è¯wçš„è¯å¯¹ (w,c)ï¼Œ

- å¦‚æœ(w,c)æ¥è‡ªäºè¯­æ–™åº“ï¼Œæˆ–è€…æ˜¯ wæ˜¯cçš„ä¸Šä¸‹æ–‡ï¼Œç”¨$P(D=1|w,c)$è¡¨ç¤ºï¼›

- å¦‚æœ(w,c)ä¸åœ¨è¯­æ–™åº“ä¸­ï¼Œç”¨$P(D=0|w,c)$è¡¨ç¤ºï¼›

- å¯¹$P(D=1|w,c)$ ç”¨Sigmoidå‡½æ•°å»ºæ¨¡ï¼š
  $$
  P(D=1|w,c; \theta) = \sigma(u_c^Tv_w) = \frac{1}{1+\exp(-u_c^Tv_w)}
  $$

- 

æ˜¾ç„¶æˆ‘ä»¬å¸Œæœ›ï¼š

- å¦‚æœ(w,c)æ¥è‡ªäºè¯­æ–™åº“ï¼Œ$\max_{\theta}P(D=1|w,c;\theta) $ï¼› 

- å¦‚æœ(w,c)ä¸åœ¨è¯­æ–™åº“ä¸­ï¼Œ$\max_{\theta}P(D=0|w,c;\theta)$ ;

  

å¾—åˆ°ä¼¼ç„¶å‡½æ•°
$$
\begin{align}
L(\theta) &= \prod_{(w,c)\in D}P(D=1|w,c;\theta) \prod _{(w,c)\in \tilde{D}}P(D=0|w,c;\theta)\\
&= \prod_{(w,c)\in D}P(D=1|w,c;\theta) \prod _{(w,c)\in \tilde{D}}(1-P(D=1|w,c;\theta))\\
&= \prod_{(w,c)\in D}\frac{1}{1+\exp(-u_c^Tv_w)} \prod _{(w,c)\in \tilde{D}}(1-\frac{1}{1+\exp(-u_c^Tv_w)})\\
& = \prod_{(w,c)\in D}\frac{1}{1+\exp(-u_c^Tv_w)} \prod _{(w,c)\in \tilde{D}}\frac{1}{1+\exp(u_c^Tv_w)}\\
\end{align}
$$
å…¶ä¸­ $\tilde{D}$æ˜¯â€œå‡â€çš„è¯­æ–™ï¼Œ

æˆ‘ä»¬å¸Œæœ›æœ€å°åŒ–å¦‚ä¸‹ç›®æ ‡å‡½æ•°
$$
\begin{align}
J  &= -\log L(\theta)\\
&= -\sum_{(w,c)\in D}\log \frac{1}{1+\exp(-u_c^Tv_w)} -\sum _{(w,c)\in \tilde{D}}\log\frac{1}{1+\exp(u_c^Tv_w)}
\end{align}
$$

- **Skip-gram** å¯¹åº”ç»™å®šçš„ä¸­å¿ƒè¯å‘é‡$v_c$æ¥è§‚å¯Ÿä¸Šä¸‹æ–‡å•è¯$w_{c-m+j}$çš„ç›®æ ‡å‡½æ•°ï¼š
  $$
  -\log \sigma(u_{c-m+j}^Tv_c) -\sum _{k=1}^K\log\sigma(\tilde{u}_k^Tv_c)
  $$
  

- **CBOW** å¯¹åº”ç»™å®šçš„ä¸Šä¸‹æ–‡å‘é‡$\hat{v}=\frac{v_{c-m}+v_{c-1}+...+v_{c+1}+v_{c+m}}{2m}$æ¥è§‚å¯Ÿä¸­å¿ƒè¯å‘é‡ $u_c$çš„ç›®æ ‡å‡½æ•°ï¼š
  $$
  -\log \sigma(u_c^T\hat{v}) -\sum _{k=1}^K\log\sigma(\tilde{u}_k^T\hat{v})
  $$
  å…¶ä¸­ï¼Œ$\{\tilde{u}_k|k=1,...,K\}$æ˜¯ä»$P_n(w)$ä¸­æŠ½å–çš„Kä¸ªè´Ÿæ ·æœ¬ã€‚

- å®é™…ä¸­å‘ç°æ•ˆæœæœ€å¥½çš„æ˜¯3/4çš„Unigramæ¨¡å‹ï¼š
  $$
  P(w) = U(w)^{3/4}/Z
  $$
  å…¶ä¸­Zæ˜¯æ­£åˆ™åŒ–ç³»æ•°ï¼ˆæ²¡æœ‰ç‰¹åˆ«è¯´æ˜çš„æƒ…å†µä¸‹ï¼Œåç»­ä¼šå§‹ç»ˆç”¨Zè¡¨ç¤ºæ­£åˆ™åŒ–ç³»æ•°ï¼‰

- è¿™ä¸ªæŒ‡æ•°$\alpha = 3/4$ä½¿å¾—ä½é¢‘è¯æ›´å®¹æ˜“è¢«æŠ½åˆ°
  -  is: $0.9^{3/4} = 0.92$
  - constitution: $0.09^{3/4} = 0.16$
  - bombastic: $0.01^{3/4} = 0.032$

### 2.5.3 è´Ÿé‡‡æ ·ä»‹ç»

**Question** å¯¹äºç»™å®šçš„è¯ wï¼Œå¦‚æœç”Ÿæˆwå¯¹åº”çš„è´Ÿæ ·æœ¬å‘¢ï¼Ÿ

**Answer** å¸¦æƒé‡‡æ ·ï¼Œä¸”æƒé‡ä¸å•è¯å‡ºç°é¢‘ç‡ç›¸å…³

- å¯¹äºé«˜é¢‘è¯ï¼Œå¸Œæœ›è¢«é€‰ä¸ºè´Ÿæ ·æœ¬çš„æ¦‚ç‡æ¯”è¾ƒå¤§
- åä¹‹ï¼Œå¯¹äºä½é¢‘è¯ï¼Œå¸Œæœ›é€‰ä¸­çš„æ¦‚ç‡å°±æ¯”è¾ƒå°ã€‚

è®¾è¯å…¸ä¸­çš„æ¯ä¸€ä¸ªè¯wéƒ½å¯¹åº”ä¸€ä¸ªçº¿æ®µ len(w)ï¼Œå…¶é•¿åº¦ä¸ºï¼š
$$
len(w) = \frac{\text{count}(w)}{\sum_{u\in C}\text{count}(u)} = \text{w åœ¨è¯­æ–™Cä¸­å‡ºç°çš„é¢‘ç‡}
$$
ç°åœ¨å°†è¿™äº›çº¿æ®µé¦–å°¾ç›¸è¿åœ°æ‹¼æ¥åœ¨ä¸€èµ·ï¼Œå½¢æˆä¸€ä¸ªé•¿åº¦ä¸º1çš„å•ä½çº¿æ®µã€‚å¦‚æœéšæœºåœ°å¾€è¿™ä¸ªå•ä½çº¿æ®µä¸Šæ‰“ç‚¹ï¼Œåˆ™å…¶ä¸­é•¿åº¦è¶Šé•¿çš„çº¿æ®µï¼ˆå¯¹åº”é«˜é¢‘è¯ï¼‰è¢«æ‰“ä¸­çš„æ¦‚ç‡å°±è¶Šå¤§ã€‚

ç”±æ­¤å¾—åˆ°åŒºé—´[0,1]ä¸Šçš„ä¸€ä¸ªéç­‰è·å‰–åˆ†ï¼Œå‰–åˆ†ç‚¹åˆ†åˆ«æ˜¯$\{l_i\}_{i=1}^{N}$, å…¶ä¸­ $l_0 = 0, l_k = \sum_{j=1}^k len(w_j)$,$k=1,...,N$ï¼Œ$w_j$æ˜¯è¯å…¸é‡Œçš„ç¬¬jä¸ªå•è¯ã€‚è®°ç¬¬iä¸ªå‰–åˆ†ä¸º $I_i = \{l_{i-1}, l_i\}$ã€‚

å¦åœ¨åŒºé—´[0,1]ä¸Šå–ä¸€ä¸ªMç­‰åˆ†çš„ç­‰è·é‡Œå‰–åˆ†ï¼Œå‰–åˆ†ç‚¹è®°ä¸º$\{m_j\}_{j=0}^M$ï¼Œå…¶ä¸­$M\gg N$ã€‚

<img src="./word2vec/neg_sampling5.png" alt="neg_sampling5" style="zoom:40%;" />

å®šä¹‰å¦‚ä¸‹æ˜ å°„ï¼šå°†å†…éƒ¨å‰–åˆ†ç‚¹$\{m_j\}_{j=1}^{M-1}$æŠ•å°„åˆ°$\{I_i\}$ï¼ˆä¸Šå›¾çº¢è‰²è™šçº¿ï¼‰
$$
\text{Table}(r) = w_k, \text{  if  } m_r \in I_k \text{ for } r = 1, 2,..., M-1
$$
æ¯æ¬¡ç”Ÿæˆä¸€ä¸ª[0, M-1]é—´çš„éšæœºæ•´æ•°rï¼Œå°±æ˜¯ä¸€ä¸ªæ ·è´Ÿæœ¬ Table(r)

- å½“å¯¹wè¿›è¡Œè´Ÿé‡‡æ ·æ—¶ï¼Œå¦‚æœç¢°å·§é€‰åˆ°wè‡ªå·±è¯¥æ€ä¹ˆåŠï¼Ÿè·³è¿‡ã€‚

- æ³¨æ„åˆ°è¿™ä¸ªè¿‡ç¨‹å…¶å®å°±æ˜¯Unigram U(w)
- Word2Vecæºç ä¸­
  - å–æŒ‡æ•°$\alpha = 3/4$ï¼Œå³ $U(w)^{\alpha}$
  - å– $M = 10^8$



### 2.5.4 å…¶ä»–

**çª—å£å¤§å°å’Œè´Ÿæ ·æœ¬æ•°é‡**

ref https://mp.weixin.qq.com/s?__biz=MjM5MTQzNzU2NA==&mid=2651669277&idx=2&sn=bc8f0590f9e340c1f1359982726c5a30&chksm=bd4c648e8a3bed9817f30c5a512e79fe0cc6fbc58544f97c857c30b120e76508fef37cae49bc&scene=0&xtrack=1#rd



## å…¶ä»–é—®é¢˜



10ï¼‰ä»‹ç»ä¸‹Hierarchical Softmaxçš„è®¡ç®—è¿‡ç¨‹ï¼Œæ€ä¹ˆæŠŠ Huffman æ”¾åˆ°ç½‘ç»œä¸­çš„ï¼Ÿå‚æ•°æ˜¯å¦‚ä½•æ›´æ–°çš„ï¼Ÿå¯¹è¯é¢‘ä½çš„å’Œè¯é¢‘é«˜çš„å•è¯æœ‰ä»€ä¹ˆå½±å“ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ

Hierarchical Softmaxåˆ©ç”¨äº†Huffmanæ ‘ä¾æ®è¯é¢‘å»ºæ ‘ï¼Œè¯é¢‘å¤§çš„èŠ‚ç‚¹ç¦»æ ¹èŠ‚ç‚¹è¾ƒè¿‘ï¼Œè¯é¢‘ä½çš„èŠ‚ç‚¹ç¦»æ ¹èŠ‚ç‚¹è¾ƒè¿œï¼Œè·ç¦»è¿œå‚æ•°æ•°é‡å°±å¤šï¼Œåœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­ï¼Œä½é¢‘è¯çš„è·¯å¾„ä¸Šçš„å‚æ•°èƒ½å¤Ÿå¾—åˆ°æ›´å¤šçš„è®­ç»ƒï¼Œæ‰€ä»¥æ•ˆæœä¼šæ›´å¥½ã€‚

ï¼ˆ11ï¼‰Word2Vecæœ‰å“ªäº›å‚æ•°ï¼Œæœ‰æ²¡æœ‰ä»€ä¹ˆè°ƒå‚çš„å»ºè®®ï¼Ÿ

- Skip-Gram çš„é€Ÿåº¦æ¯”CBOWæ…¢ä¸€ç‚¹ï¼Œå°æ•°æ®é›†ä¸­å¯¹ä½é¢‘æ¬¡çš„æ•ˆæœæ›´å¥½ï¼›
- Sub-Sampling Frequent Wordså¯ä»¥åŒæ—¶æé«˜ç®—æ³•çš„é€Ÿåº¦å’Œç²¾åº¦ï¼ŒSample å»ºè®®å–å€¼ä¸º ï¼›
- Hierarchical Softmaxå¯¹ä½è¯é¢‘çš„æ›´å‹å¥½ï¼›
- Negative Samplingå¯¹é«˜è¯é¢‘æ›´å‹å¥½ï¼›
- å‘é‡ç»´åº¦ä¸€èˆ¬è¶Šé«˜è¶Šå¥½ï¼Œä½†ä¹Ÿä¸ç»å¯¹ï¼›
- Window Sizeï¼ŒSkip-Gramä¸€èˆ¬10å·¦å³ï¼ŒCBOWä¸€èˆ¬ä¸º5å·¦å³ã€‚

ï¼ˆ12ï¼‰Word2Vecæœ‰å“ªäº›å±€é™æ€§ï¼Ÿ

Word2Vecä½œä¸ºä¸€ä¸ªç®€å•æ˜“ç”¨çš„ç®—æ³•ï¼Œå…¶ä¹ŸåŒ…å«äº†å¾ˆå¤šå±€é™æ€§ï¼š

- Word2Vecåªè€ƒè™‘åˆ°ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè€Œå¿½ç•¥çš„å…¨å±€ä¿¡æ¯ï¼›
- Word2Vecåªè€ƒè™‘äº†ä¸Šä¸‹æ–‡çš„å…±ç°æ€§ï¼Œè€Œå¿½ç•¥çš„äº†å½¼æ­¤ä¹‹é—´çš„é¡ºåºæ€§ï¼›

Ref

1. https://mp.weixin.qq.com/s/zDneR1BU6xvt8cndEF4_Xw 
2. https://mp.weixin.qq.com/s?__biz=MjM5MTQzNzU2NA==&mid=2651669277&idx=2&sn=bc8f0590f9e340c1f1359982726c5a30&chksm=bd4c648e8a3bed9817f30c5a512e79fe0cc6fbc58544f97c857c30b120e76508fef37cae49bc&scene=0&xtrack=1#rd
