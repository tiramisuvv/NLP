# Language Model è¯­è¨€æ¨¡å‹

## 1. Two views

- VIEW 1

<img src="./language_model/def1.png" alt="def1" style="zoom:50%;" />

- classifier ï¼Œç»™å‡º V ä¸­ä»»ä½•ä¸€ä¸ªå•è¯ w å‡ºç°çš„æ¦‚ç‡
- VIEW 2

<img src="./language_model/def2.png" alt="def2" style="zoom:50%;" />Some application

1. æ‰‹æœºæ‰“å•è¯ï¼Œä¼šè¡¥å…¨ï¼›
2. Google search

- 



#### Idea ğŸ’¡

è¯­è¨€æ¨¡å‹æ˜¯ç”¨æ¥è®¡ç®—ä¸€ä¸ªå¥å­çš„æ¦‚ç‡çš„æ¦‚ç‡æ¨¡å‹ã€‚æ¯”å¦‚*"The cat jumped over the puddle."* è¿™æ ·å®Œæ•´ä¸”åˆç†çš„å¥å­åº”è¯¥æœ‰é«˜çš„æ¦‚ç‡ï¼Œè€Œ *"stock boil fish is toy"* æœ‰ä½æ¦‚ç‡ã€‚

å¯¹äºä»»æ„ä¸€åˆ—nä¸ªå•è¯ï¼Œå®ƒçš„æ¦‚ç‡ç”¨ $P(w_1, w_2, ..., w_T)$ è¡¨ç¤ºã€‚

ä¸€èˆ¬çš„ï¼Œæ ¹æ®Bayes å…¬å¼ï¼Œ
$$
P(w_1,...,w_T) = P(w_1)P(w_2|w_1)P(w_3|w_1,w_2)...P(w_T|w_1,...,w_{T-1})
$$
å…¶ä¸­æ¡ä»¶æ¦‚ç‡ $p(w1), p(w_2|w_1), ..., p(w_T|w_1,...,w_{T-1})$ æ˜¯æ¨¡å‹çš„å‚æ•°ã€‚å¯¹äºä¸€ä¸ªé•¿åº¦ä¸º$T$ çš„å¥å­ï¼Œéœ€è¦è®¡ç®— $T$  ä¸ªå‚æ•°ã€‚åœ¨è¯æ±‡é‡ä¸º$|V|$ çš„è¯­æ–™åº“ï¼Œä»»æ„é•¿åº¦ T çš„å¥å­å…±ç”¨ $|V|^T$ ç§å¯èƒ½ï¼Œè¿›ä¸€æ­¥å°±æœ‰ $T \cdot |V|^T$ ä¸ªå‚æ•°ã€‚ï¼ˆè¿™é‡Œå¿½ç•¥äº†é‡å¤å‚æ•°ï¼Œåªçœ‹é‡çº§å°±å¥½ï¼‰ã€‚





**Question: How would you learn a language model?**

**Ans** (pre Deep Learning) n-gram language model

# 2. N-gram model

## 2.1 Introduction

**Definition** A **n-gram**  is a chunk of n consecutive words. 

**ä¾‹å­** ğŸŒ° The students opened their ...

- **uni**grams:  â€œtheâ€, â€œstudentsâ€, â€œopenedâ€, â€theirâ€ 
- **bi**grams: â€œthe studentsâ€, â€œstudents openedâ€, â€œopened theirâ€ 
- **tri**grams: â€œthe students openedâ€, â€œstudents opened theirâ€ 
- **4-**grams: â€œthe students opened theirâ€

**Idea ğŸ’¡** Collect statistics about how frequent different n-grams are and use these to predict next word.



## 2.3 æ¨¡å‹ç»†èŠ‚

### 2.3.1 Assumption

**Markov å‡è®¾** å½“å‰å•è¯$w_{t+1} = x^{(t+1)}$ åªä¾èµ–äºå®ƒå‰n-1ä¸ªå•è¯ã€‚

<img src="./language_model/Markov_Assumption.png" alt="Markov_Assumption" style="zoom:50%;" />

### 2.3.2 How to get thest n-gram and (n-1) grams probabilities?

**Ans** counting! ç”¨è®¡æ•°æ¯”ä½œä¸ºç»Ÿè®¡ä¼°å€¼
$$
\approx \frac{\text{count}(x^{(t+1)},x^{t},...,x^{(t-n+2)})}{\text{count}(x^{(t)},x^{t},...,x^{(t-n+2))}}
$$


### 2.3.3 Examples 

#### 2.3.3.1 Unigram æ¨¡å‹

å‡è®¾å•è¯ä¹‹é—´å®Œå…¨ç‹¬ç«‹ï¼Œé‚£ä¹ˆ
$$
P(w_1,...,w_T) = \prod_{i = 1}^T P(w_i)
$$
æ˜¾ç„¶ï¼Œæˆ‘ä»¬çŸ¥é“è¿™ä¸ªæ˜¯ä¸åˆç†çš„ï¼Œå› ä¸ºå½“å‰å•è¯è‚¯å®šæ˜¯ä¾èµ–äºå‰ä¸€ä¸ªå•è¯çš„ã€‚

#### 2.3.3.2 Bigram æ¨¡å‹

å‡è®¾å½“å‰å•è¯åªä¾èµ–äºå®ƒå‰ä¸€ä¸ªå•è¯ï¼Œé‚£ä¹ˆ
$$
P(w_1,...,w_T) = \prod_{i = 2}^TP(w_i|w_{i-1})
$$

#### 2.3.3.3 N-gram æ¨¡å‹

ç±»ä¼¼çš„
$$
P(w_1, ..., w_T) = \prod_{i=n}^{T} P(w_i|w_{i-n+1},...,w_{i-1})
$$
è¿›ä¸€æ­¥ï¼Œæ ¹æ®å¤§æ•°å®šå¾‹ï¼Œå½“è¯­æ–™åº“è¶³å¤Ÿå¤§æ—¶ï¼Œ
$$
P(w_i|w_{i-n+1}, ..., w_{i-1}) \approx \frac{\text{count}(w_{i-n+1}, ..., w_i)}{\text{count}(w_{i-n+1}, ..., w_{i-1})}
$$
ä»¥ä¸Šé¢Bigram ä¸ºä¾‹ (n=2)ï¼Œæœ‰
$$
P(w_i|w_{i-1}) \approx \frac{\text{count}(w_{i-1},w_i)}{\text{count}(w_{i-1})}
$$
ä¸ä»…ä½¿å¾—å•ä¸ªå‚æ•°çš„ç»Ÿè®¡å˜å¾—æ›´å®¹æ˜“ï¼ˆç»Ÿè®¡æ—¶éœ€è¦åŒ¹é…çš„è¯ä¸²æ›´çŸ­ï¼‰ï¼Œä¹Ÿä½¿å¾—å‚æ•°çš„æ€»æ•°å˜å°‘äº†ã€‚

### 2.3.4 Markov å‡è®¾çš„ä¸åˆç†ä¾‹å­

ä»¥4-gramå¤„ç†ä¸‹é¢å¥å­ï¼š

â€‹		 as the proctor started the clock, the students opened their 

- å› ä¸ºæ˜¯4-gramï¼Œæ‰€ä»¥å¿½ç•¥å‰é¢å†…å®¹ï¼šas the proctor started the clock, the

  <img src="./language_model/example_4gram.png" alt="example_4gram" style="zoom:50%;" />

$$
P(\bold{w}|\text{students opened their}) = \frac{\text{count}(\text{students open their }\bold{w})}{\text{count}(\text{students open their})}
$$

- For example, suppose that in the corpus: 

  - â€œstudents opened theirâ€ occurred 1000 times 
  - â€œstudents opened their booksâ€ occurred 400 times 
  - â€œstudents opened their examsâ€ occurred 100 times

- P(**exams** | students opened their) = 0.1  < P(**books** | students opened their) = 0.4

- ä½†æ˜¯ï¼æ ¹æ®ä¸Šä¸‹æ–‡ï¼Œåº”è¯¥å¡« **exam**

  

### 2.3.4 Sparsity Problems

<img src="./language_model/SparsityProblems.png" alt="SparsityProblems" style="zoom:50%;" />



è€ƒè™‘ä¸‰ä¸ªé—®é¢˜ï¼ˆæç«¯æƒ…å†µï¼‰ï¼š

- å¦‚æœ åˆ†å­ $\text{count}(w_{i-n+1}, ..., w_i) = 0$ï¼Œæ˜¯å¦è®¤ä¸º $P(w_i|w_{i-n+1}, ..., w_{i-1}) = 0$? 
  - ä¸Šå›¾ä¸­ Sparsity Problem1ï¼Œsmoothing
- å¦‚æœ $\text{count}(w_{i-n+1}, ..., w_i) = \text{count}(w_{i-n+1}, ..., w_{i-1}) $, æ˜¯å¦è®¤ä¸º $P(w_i|w_{i-n+1}, ..., w_{i-1}) = 1$?
- å¦‚æœ åˆ†æ¯ $\text{count}(w_{i-n+1}, ..., w_i-1) = 0$ï¼Œæ€ä¹ˆå¤„ç† $P(w_i|w_{i-n+1}, ..., w_{i-1})$? 
  - backoff 

**ç¨€ç–é—®é¢˜** - å°¤å…¶å½“nå˜å¤§æ—¶ï¼Œæ›´å®¹æ˜“å‡ºç°è¿™æ ·çš„çŠ¶å†µï¼šæŸäº›n-gramä»æœªå‡ºç°è¿‡ã€‚ï¼ˆæ—¥å¸¸ä¸ä¼šä½¿ç”¨å¤§äº5çš„nï¼‰

**ä¾‹å­ğŸŒ°** åœ¨bi-gramä¸­ï¼Œè‹¥è¯åº“ä¸­æœ‰20kä¸ªè¯ï¼Œé‚£ä¹ˆä¸¤ä¸¤ç»„åˆå°±æœ‰è¿‘2äº¿ä¸ªç»„åˆã€‚å…¶ä¸­çš„å¾ˆå¤šç»„åˆåœ¨è¯­æ–™åº“ä¸­éƒ½æ²¡æœ‰å‡ºç°ï¼Œæ ¹æ®æå¤§ä¼¼ç„¶ä¼°è®¡å¾—åˆ°çš„ç»„åˆæ¦‚ç‡å°†ä¼šæ˜¯0ï¼Œä»è€Œæ•´ä¸ªå¥å­çš„æ¦‚ç‡å°±ä¼šä¸º0ã€‚æœ€åçš„ç»“æœæ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åªèƒ½è®¡ç®—é›¶æ˜Ÿçš„å‡ ä¸ªå¥å­çš„æ¦‚ç‡ï¼Œè€Œå¤§éƒ¨åˆ†çš„å¥å­ç®—å¾—çš„æ¦‚ç‡æ˜¯0ï¼Œè¿™æ˜¾ç„¶æ˜¯ä¸åˆç†çš„ã€‚ 

### 2.3.4 Storage Problems

<img src="./language_model/StorageProblems.png" alt="StorageProblems" style="zoom:50%;" />

ä¸‹è¡¨ç»™å‡ºäº†n-gramæ¨¡å‹ä¸­æ¨¡å‹å‚æ•°æ•°é‡éšç€çš„é€æ¸å¢å¤§è€Œå˜åŒ–çš„æƒ…å†µï¼Œå…¶ä¸­å‡å®šè¯æ±‡é‡ä¸º $|V| = 200,000$(æ±‰è¯­çš„è¯æ±‡é‡å¤§è‡´æ˜¯è¿™ä¸ªé‡çº§)ã€‚

|     n      |   æ¨¡å‹å‚æ•°çš„æ•°é‡   |
| :--------: | :----------------: |
| 1(unigram) |   $2\times 10^5$   |
| 2(bigram)  | $4\times 10^{10}$  |
| 3(trigram) | $8\times 10^{15}$  |
| 4(4-gram)  | $16\times 10^{20}$ |

å®é™…åº”ç”¨ä¸­ï¼Œæœ€å¤šé‡‡ç”¨n=3çš„trigramæ¨¡å‹ã€‚

### 2.3.5 Smoothing

å› ä¸ºè¦è¿›è¡Œæ•°æ®å¹³æ»‘ï¼ˆdata Smoothingï¼‰ï¼Œæ•°æ®å¹³æ»‘çš„ç›®çš„æœ‰ä¸¤ä¸ªï¼š

1. æ‰€æœ‰çš„N-gramæ¦‚ç‡ä¹‹å’Œä¸º1;

2. æ‰€æœ‰çš„n-gramæ¦‚ç‡éƒ½ä¸ä¸º0

   > **æœ¬è´¨ğŸ’¡ é‡æ–°åˆ†é…æ•´ä¸ªæ¦‚ç‡ç©ºé—´-- åŠ«å¯Œæµè´«** 
   >
   > - é™ä½å·²ç»å‡ºç°è¿‡çš„n-gramçš„æ¦‚ç‡
   > - è¡¥å……ç»™æœªæ›¾å‡ºç°è¿‡çš„n-gramçš„æ¦‚ç‡
   > - æ¶ˆé™¤0æ¦‚ç‡ï¼Œæ”¹è¿›æ¨¡å‹çš„æ•´ä½“å‡†ç¡®ç‡ã€‚



æ€»ç»“èµ·æ¥ï¼Œn-gramæ¨¡å‹æ˜¯è¿™æ ·ä¸€ç§æ¨¡å‹ï¼Œå…¶ä¸»è¦å·¥ä½œæ˜¯åœ¨è¯­æ–™ä¸­ç»Ÿè®¡å„ç§è¯ä¸²å‡ºç°çš„æ¬¡æ•°ä»¥åŠå¹³æ»‘åŒ–å¤„ç†ã€‚æ¦‚ç‡å€¼è®¡ç®—å¥½ä¹‹åå°±å­˜å‚¨èµ·æ¥ï¼Œä¸‹æ¬¡éœ€è¦è®¡ç®—ä¸€ä¸ªå¥å­çš„æ¦‚ç‡æ—¶ï¼Œåªéœ€æ‰¾åˆ°ç›¸å…³çš„æ¦‚ç‡å‚æ•°ï¼Œå°†å®ƒä»¬è¿ä¹˜èµ·æ¥å°±å¥½äº†ã€‚

#### 2.3.4.1 æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ Add one

- IdeağŸ’¡ å¼ºåˆ¶è®©æ‰€æœ‰çš„n-gramè‡³å°‘å‡ºç°ä¸€æ¬¡ï¼Œåªéœ€è¦åœ¨åˆ†å­å’Œåˆ†æ¯ä¸Šåˆ†åˆ«åšåŠ æ³•å³å¯ã€‚
- ä»¥2-gramä¸ºä¾‹

$$
P(w_i|w_{i-1}) = \frac{\text{count}(w_{i-1},w_i)+1} {\text{count}(w_{i-1})+ |V|}
$$

- ç¼ºç‚¹â 

  - å¤§éƒ¨åˆ†n-graméƒ½æ˜¯æ²¡æœ‰å‡ºç°è¿‡çš„ï¼Œå¾ˆå®¹æ˜“ä¸ºä»–ä»¬åˆ†é…è¿‡å¤šçš„æ¦‚ç‡ç©ºé—´
  - å‡è®¾çš„æ‰€æœ‰æœªå‡ºç°çš„n-Gramæ¦‚ç‡ç›¸ç­‰ä¸ä¸€å®šåˆç†ã€‚
  - å¯¹äºå‡ºç°åœ¨è®­ç»ƒè¯­æ–™ä¸­çš„é‚£äº›n-Gramï¼Œéƒ½å¢åŠ åŒæ ·çš„é¢‘åº¦å€¼ï¼Œä¸ä¸€å®šåˆç†

- è¡ç”Ÿï¼šAdd kï¼ŒåŠ ä¸€ä¸ªå°äº1çš„æ­£æ•°k

  - 2-gramä¸ºä¾‹

  $$
  P(w_i|w_{i-1}) = \frac{\text{count}(w_{i-1},w_i)+k} {\text{count}(w_{i-1})+ k|V|}
  $$

  - æœ€å¸¸ä½¿ç”¨çš„kå€¼æ˜¯1/2
  - ä¼˜ç‚¹âœ…
    - é€šå¸¸ï¼Œadd-kç®—æ³•çš„æ•ˆæœä¼šæ¯”Add-oneå¥½
  - ç¼ºç‚¹â 
    - å®ƒä¸èƒ½å®Œå…¨è§£å†³é—®é¢˜ã€‚
    - å¸¸æ•°kéœ€è¦äººå·¥ç¡®å®šï¼Œå¯¹äºä¸åŒçš„è¯­æ–™åº“Kå¯èƒ½ä¸åŒã€‚

#### 2.3.4.2 Interpolation æ’å€¼ & Backoff å›é€€

##### a. Idea ğŸ’¡

- æ’å€¼å’Œå›é€€çš„æ€æƒ³å…¶å®éå¸¸ç›¸åƒã€‚

- è®¾æƒ³å¯¹äºä¸€ä¸ªtrigramçš„æ¨¡å‹ï¼Œæˆ‘ä»¬è¦ç»Ÿè®¡è¯­æ–™åº“ä¸­ â€œlike chinese foodâ€ å‡ºç°çš„æ¬¡æ•°ï¼Œç»“æœå‘ç°å®ƒæ²¡å‡ºç°è¿‡ï¼Œåˆ™è®¡æ•°ä¸º0ã€‚
  - åœ¨å›é€€ç­–ç•¥ä¸­ï¼Œå°†ä¼šè¯•ç€ç”¨ä½é˜¶gramæ¥è¿›è¡Œæ›¿ä»£ï¼Œä¹Ÿå°±æ˜¯ç”¨ â€œchinese foodâ€ å‡ºç°çš„æ¬¡æ•°æ¥æ›¿ä»£ã€‚
  - åœ¨ä½¿ç”¨å†…æ’å€¼æ³•æ—¶ï¼Œæˆ‘ä»¬æŠŠä¸åŒé˜¶åˆ«çš„n-Gramæ¨¡å‹çº¿å½¢åŠ æƒç»„åˆåå†æ¥ä½¿ç”¨ã€‚

##### b. Interpolation æ’å€¼æ³•

å¦‚ä¸‹æ˜¯ä¸€ä¸ªä¸‰é˜¶ç»„åˆï¼Œå‡è®¾$p(w_n|w_{nâˆ’1}w_{nâˆ’2})=0$ï¼Œè€Œ$p(w_n|w_{nâˆ’1})>0$ ï¼Œ$p(w_n)>0$ ï¼Œåˆ™åŠ æƒå¹³å‡åçš„æ¦‚ç‡ä¸ä¸º0ï¼Œä»è€Œè¾¾åˆ°å¹³æ»‘çš„æ•ˆæœã€‚
$$
\hat{p}(w_n|w_{nâˆ’1}w_{nâˆ’2})= \lambda_3 p(w_n|w_{nâˆ’1}w_{nâˆ’2}) + \lambda_2p(w_n|w_{nâˆ’1}) + \lambda_1p(w_n)
$$
å…¶ä¸­ $\lambda_1 + \lambda_2 + \lambda_3 = 1$ï¼Œ$\hat{p}(w_n|w_{nâˆ’1}w_{nâˆ’2})$ ä¸º3-gramæ¨¡å‹çš„æ’å€¼ç»“æœï¼Œ $p(w_n|w_{nâˆ’1})= \frac{\text{count}(w_{i-1},w_i)} {\text{count}(w_{i-1})}$ 

##### c. Backoff å›é€€æ³•

ä¼šå°½å¯èƒ½åœ°ç”¨æœ€é«˜é˜¶ç»„åˆè®¡ç®—æ¦‚ç‡ï¼Œå½“é«˜é˜¶ç»„åˆä¸å­˜åœ¨æ—¶ï¼Œé€€è€Œæ±‚å…¶æ¬¡æ‰¾æ¬¡ä½é˜¶ï¼Œç›´åˆ°æ‰¾åˆ°éé›¶ç»„åˆä¸ºæ­¢ã€‚

ä¸‹é¢ç”¨$P_{katz}^{(n)}(w_i|w_{i-n+1}^{i-1})$ è¡¨ç¤ºå…‰æ»‘åŒ–å $w_i$ çš„n-gramï¼Œæœ‰
$$
P_{katz}^{(n)}(w_i|w_{i-n+1}^{i-1})= \left\{\begin{split}
& P(w_i|w_{i-n+1}^{i-1}) , \quad\text{  if count}(w_{i-n+1}^{i-1}) > 0\\
&\alpha P_{katz}^{(n-1)}(w_i|w_{i-n+2}^{i-1}), \quad \quad\text{  if count}(w_{i-n+1}^{i-1}) = 0. \\ 
\end{split}\right.
$$
å…¶ä¸­$P(w_i|w_{i-n+1}^{i-1}) = \frac{\text{count}(w_{i-n+1}, ..., w_i)}{\text{count}(w_{i-n+1}, ..., w_{i-1})}$ï¼Œ $\alpha = \alpha({w_{i-n+1}, ..., w_{i-1}})$ æ˜¯å½’ä¸€åŒ–å› å­ã€‚

- ç¼ºç‚¹â 
  - åœ¨æœ‰äº›ç¯å¢ƒä¸­æ•ˆæœä¸å¥½ã€‚
    - ğŸŒ° æˆ‘ä»¬å·²ç»çœ‹åˆ° $w_i w_j$ å‡ºç°å¾ˆå¤šæ¬¡ï¼Œå¹¶ä¸” $w_k$ æ˜¯ä¸€ä¸ªå¸¸ç”¨è¯ï¼Œä½†ä»æ¥æ²¡æœ‰è§è¿‡çš„trigram $w_iw_jw_k$çš„ç»„åˆ => è¿™ä¸ªtrigramå¾ˆå¯èƒ½æ˜¯ä¸€ä¸ª"é›¶æ¦‚ç‡"çš„ä¾‹å­ï¼Œè€Œä¸åº”ç”¨å›é€€æ³•è®¡ç®—$P(w_k|w_j)$æ¥ä¼°è®¡$P(w_k|w_iw_j)$
  - æ·»åŠ æ–°æ•°æ®åˆ°è¯­æ–™åº“ä¼šå¯¼è‡´æ¦‚ç‡çš„çªç„¶æ”¹å˜ã€‚
    - æ–°æ·»åŠ çš„æ•°æ®å¯èƒ½å¯¼è‡´å›é€€æ¨¡å‹é€‰æ‹©ä¸åŒé˜¶çš„n-gramæ¨¡å‹ä½œä¸ºä¼°è®¡æ¦‚ç‡

#### 2.3.4.3 Good-Turing

**Idea ğŸ’¡: æŠ˜æ‰£æ€æƒ³**- æ‰€è°“æŠ˜æ‰£å°±æ˜¯ä»æ¡ä»¶æ¦‚ç‡ä¸ä¸ºé›¶çš„n-gramæ¦‚ç‡ä¸­æ‹¿ä¸€éƒ¨åˆ†æ¦‚ç‡ç»™è®­ç»ƒé›†æœªå‡ºç°çš„n-gram.

- å¯¹äºä»»ä½•ä¸€ä¸ªå‡ºç°rçš„n-gramï¼Œéƒ½å‡è®¾å®ƒå‡ºç°äº†$r^*$æ¬¡ï¼Œè¿™é‡Œ


$$
  r^*=(r+1)\frac{n_{r+1}}{n_r}
$$

  - å…¶ä¸­$n_r$ æ˜¯æ°å¥½å‡ºç°ræ¬¡çš„n-gramçš„æ•°ç›®	

- è¿™æ ·ï¼Œå‡ºç°æ¬¡æ•°ä¸ºrçš„æ¦‚ç‡å°±å˜æˆäº†

$$
p_r = \frac{r^*}{N},\text{ with }N = \sum_{r=0}^{\infty}n_rr^*
$$

æ³¨æ„åˆ°Næ»¡è¶³ï¼š
$$
N = \sum_{r=0}^{\infty}n_rr^* = \sum_{r=0}^{\infty}(r+1)n_{r+1} = \sum_{r=1}^{\infty}r n_{r} 
$$
å³ï¼ŒNå°±æ˜¯è¿™ä¸ªåˆ†å¸ƒä¸­æœ€åˆçš„è®¡æ•°ã€‚



# 3. Neural Language Model

## 3.1 Window-based neural model

### 3.1.1 Basic Architecture

<img src="./language_model/Window-based1.png" alt="Window-based1" style="zoom:50%;" />

### 3.1.2 Apply to example

- window size = 4

  <img src="./language_model/Window-based2.png" alt="Window-based2" style="zoom:50%;" />

### 3.1.3 Summary

âœ… ï¼ˆvs n-gramï¼‰

- No sparsity problem 
- Donâ€™t need to store all observed n-grams

â

- Fixed window is too small 
- Enlarging window enlarges W
- Window can never be large enough! 
- ã€å­¦ä¹ æ•ˆç‡ä½ï¼Œå› ä¸ºæ¯ä¸ªå•è¯å¯¹Wçš„è´¡çŒ®ä¸å…±äº« ã€‘$x^{(1)}$ and $x^{(2)}$ are multiplied by completely different weights in W.
  - No symmetry in how the inputs are processed. 

We need a neural architecture that can process **any length input**

# 4. ç›®æ ‡å‡½æ•°

å¯¹äºç»Ÿè®¡è¯­è¨€æ¨¡å‹è€Œè¨€ï¼Œåˆ©ç”¨æœ€å¤§ä¼¼ç„¶ï¼Œå¯æŠŠç›®æ ‡å‡½æ•°è®¾ä¸ºï¼š
$$
L = \prod_{w \in C} P(w|\text{contex}(w))
$$
å…¶ä¸­ Cè¡¨ç¤º è¯­æ–™åº“ï¼ˆCorpus)ï¼ŒContex(w)è¡¨ç¤ºå•è¯wçš„ä¸Šä¸‹æ–‡