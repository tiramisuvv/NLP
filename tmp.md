input x: ä¸€ä¸ªå¥å­ï¼Œé•¿åº¦è®°ä¸º $T_x$

- $x^{<t>}$ word at position t

$x^{(i)<t>}$ ç¬¬iä¸ªinputçš„tä½ç½®ï¼Œ$T_{x}^{(i)}$ å¯¹ä¸åŒsampleå¯èƒ½ä¸åŒï¼Œå¦‚æœ‰çš„å¥å­9ä¸ªå•è¯ï¼Œæœ‰çš„15ä¸ªå•è¯



output y:

Vacabulary :30K-50K



# RNN

- ç›´æ¥ç”¨NN

  - input,outputs é•¿åº¦ä¸åŒï¼›
  - doesn't share features learned across different positions of text

  - a better model can reduce # of parameters 

- ç»“æ„ architectures 

  - $T_x = T_y$  

 <img src="/Users/Wei/Documents/NLP/NLP/rnn/structure.png" alt="structure" style="zoom:50%;" />

- åˆ©ç”¨å‰é¢çš„ä¿¡æ¯

- å…±äº«å‚æ•°

- ç¼ºç‚¹ åªç”¨å‰é¢ä¿¡æ¯ï¼Œä¸ç”¨åé¢ä¿¡æ¯

  - He said 
  - He said
  - -ã€‹ Bidirectional RNNï¼ˆBRNNï¼‰

  $W_{ax}$ ä¹˜ä»¥xï¼Œç”¨äºè®¡ç®—aï¼Œæ¯”å¦‚$a^{<1>} = g(W_{aa}a^{<0>}+W_{ax}x^{<1>}+b_a)$

  [TODO] $T_x \neq T_y$ æ˜¯æ€ä¹ˆåšçš„-> different types

  ## Different types

  1. many-to-many
     1. $T_x = T_y$
     2. $T_x \neq T_y$
        1. machine translation 
        2. 
  2. many-to-one
     1. ğŸŒ° setimant classification
  3. one-to-many
     1. music generation 
     2. 

  <img src="/Users/Wei/Documents/NLP/NLP/rnn/summary_types.png" alt="summary_types" style="zoom:50%;" />

  ## Backpropagation through time

