<img src="./rnn/plan.png" alt="plan" style="zoom:50%;" />







input x: ä¸€ä¸ªå¥å­ï¼Œé•¿åº¦è®°ä¸º $T_x$

- $x^{<t>}$ word at position t

$x^{(i)<t>}$ ç¬¬iä¸ªinputçš„tä½ç½®ï¼Œ$T_{x}^{(i)}$ å¯¹ä¸åŒsampleå¯èƒ½ä¸åŒï¼Œå¦‚æœ‰çš„å¥å­9ä¸ªå•è¯ï¼Œæœ‰çš„15ä¸ªå•è¯



output y:

Vacabulary :30K-50K



# RNN

- ç›´æ¥ç”¨NN

  - input,outputs é•¿åº¦ä¸åŒï¼›
  - doesn't share features learned across different positions of text

  - a better model can reduce # of parameters 

- ç»“æ„ architectures 

  - $T_x = T_y$

- <img src="./rnn/structure.png" alt="structure" style="zoom:50%;" />
- 

- åˆ©ç”¨å‰é¢çš„ä¿¡æ¯

- å…±äº«å‚æ•°

- ç¼ºç‚¹ åªç”¨å‰é¢ä¿¡æ¯ï¼Œä¸ç”¨åé¢ä¿¡æ¯

  - He said 
  - He said
  - -ã€‹ Bidirectional RNNï¼ˆBRNNï¼‰

  $W_{ax}$ ä¹˜ä»¥xï¼Œç”¨äºè®¡ç®—aï¼Œæ¯”å¦‚$a^{<1>} = g(W_{aa}a^{<0>}+W_{ax}x^{<1>}+b_a)$

  [TODO] $T_x \neq T_y$ æ˜¯æ€ä¹ˆåšçš„-> different types

  

  

  # Recurrent Neural Networks (RNN)

  <img src="./rnn/architecture.png" alt="architecture" style="zoom:50%;" />

  -  hidden **states**: a single state that's mutating over time(several version of the same thing) 
  - **Core idea** Apply the same weights W repeatedly

  

  ## Example 

  <img src="./rnn/simple_example.png" alt="simple_example" style="zoom:50%;" />

- current hidden state based on previous hidden state and current input 

âœ…

- Can process **any length** input
- Computation for step t can (in theory) use information from **many steps back**
- **Model size doesnâ€™t increase** for longer input context 

- Same weights applied on every timestep, so there is **symmetry** in how inputs are processed.

â 

- Recurrent computation is **slow** 
- In practice, difficult to access information from **many steps back**



## Training a RNN Language Model

<img src="/Users/Wei/Library/Application Support/typora-user-images/Screen Shot 2021-07-13 at 10.39.52 PM.png" alt="Screen Shot 2021-07-13 at 10.39.52 PM" style="zoom:50%;" />



<img src="./rnn/how_to_train.png" alt="how_to_train" style="zoom:50%;" />

<img src="./rnn/how_to_train2.png" alt="how_to_train2" style="zoom:50%;" />

**Problem** : Computing loss and gradients across entire corpus $$ is too expensive!

<img src="./rnn/Screen Shot 2021-07-13 at 10.43.36 PM.png" alt="Screen Shot 2021-07-13 at 10.43.36 PM" style="zoom:50%;" />

- ç¬¬äºŒæ¡è¡¥å……ï¼šshorter unit of text

## Backpropagation for RNN

<img src="./rnn/backprop.png" alt="backprop" style="zoom:50%;" />

<img src="./rnn/backprop2.png" alt="backprop2" style="zoom:50%;" />

<img src="./rnn/backprop3.png" alt="backprop3" style="zoom:50%;" />

<img src="./rnn/backprop4.png" alt="backprop4" style="zoom:50%;" />

[TODO]

## Generating text with an RNN Language Model

<img src="./rnn/generate_text_example.png" alt="generate_text_example" style="zoom:50%;" />



- RNN ä¸èƒ½è®°ä½overallçš„ä¿¡æ¯ï¼ˆæ¯”å¦‚ä¹‹å‰åœ¨åšæˆ–è€…åšè¿‡ä»€ä¹ˆäº‹æƒ…ï¼‰



[TODO: ç§»å…¥language modelçš„ç¬”è®°]

## Evaluating Language Models

<img src="./rnn/evaluation.png" alt="evaluation" style="zoom:50%;" />

=> min(J(\theta)) ç­‰ä»·äº min(perplexity)



## Why should we care about Language Modeling? 

- Language Modeling is a **benchmark task** that helps us **measure our progress** on understanding language
- Language Modeling is a **subcomponent** of many NLP tasks, especially those involving **generating text** or **estimating the probability of text**:
  - Predictive typing 
  - Speech recognition 
  - Handwriting recognition 
  - Spelling/grammar correction 
  - Authorship identification 
  - Machine translation 
  - Summarization 
  - Dialogue 
  - etc

<img src="./rnn/Screen Shot 2021-07-13 at 11.14.39 PM.png" alt="Screen Shot 2021-07-13 at 11.14.39 PM" style="zoom:50%;" />



## Other applications of RNN

### 1. tagging e.g., part-of-speech tagging, named entity recognition

<img src="./rnn/application_example1.png" alt="application_example1" style="zoom:50%;" />

### 2 .sentiment classification

<img src="./rnn/application_example2_1.png" alt="application_example2_1" style="zoom:50%;" />

<img src="./rnn/application_example2_2.png" alt="application_example2_2" style="zoom:50%;" />

### 3. encode model

<img src="./rnn/application_example3.png" alt="application_example3" style="zoom:50%;" />

æ–‡å­—ç‰ˆçš„é—®é¢˜ --é€šè¿‡RNN --> ä»£è¡¨questionçš„something

### 4. generate text

<img src="./rnn/application_example4.png" alt="application_example4" style="zoom:50%;" />

- åœ¨speech recognitioné‡Œï¼Œä¸€èˆ¬ç”¨WER ï¼ˆword error rateï¼‰ä½œä¸ºè¡¡é‡æ ‡å‡†ï¼Œæœ‰æ—¶å€™ä¹Ÿç”¨ perplexity
- conditional language model =there's some kind of input that we need to condition on

## Different types

1. many-to-many
   1. $T_x = T_y$
   2. $T_x \neq T_y$
      1. machine translation 
      2. 
2. many-to-one
   1. ğŸŒ° setimant classification
3. one-to-many
   1. music generation 
   2. 

<img src="./rnn/summary_types.png" alt="summary_types" style="zoom:50%;" />





## Backpropagation through time

Sampling a sequence from a **trained** RNN 

åŒºåˆ«äºä¹‹å‰ï¼Œç°åœ¨çš„input $x^{<2>} = \hat{y}^{<1>}$ è€Œä¸æ˜¯ $y^{<1>}$ ã€Qã€‘

Novel Sequences 

Character-level language model

- (+) ä¸ä¼š<unk>
- (-) match longer sequence
- (-) much computational expensive



## Vanishing Gradients with RNN

### Why happened

- may have very long term dependency 
  - The ==cat==. Which already ate ..., ==was== full

  - The ==cats==. Which already ate ..., ==were== full

- $\frac{\part \text{error}}{\part \text{å‰æ’å‚æ•°}} \approx 0$ 

- The basic RNN model has many local influence



1. vanishing gradient bigger problem
2. exploding graident also problem
   1. å®¹æ˜“å‘ç°
   2. solution: gradient clipping

### GRU gated recurrent unit 

**Motivation** cat---> was

= modification of hidden layer in RNN 

+ (+) capture long range connections
+ (+)Helps a lot with  vanishing gradient problem

<img src="/Users/weiwang/Documents/NLP/rnn/rnn_unit.png" alt="rnn_unit" style="zoom:50%;" />



å¢åŠ  c = memory cell æ¥å­˜å‚¨ä¿¡æ¯ï¼Œæ¯”å¦‚ catæ˜¯å•æ•°è¿˜æ˜¯å¤æ•°

$c^{<t>}:=$ memory cell value

$a^{<t>}:=$ output activation value

In GRUï¼Œå– $c^{<t>}:= a^{<t>}$ ï¼Œä½†åœ¨LSTMä¸­ä¸åŒã€‚

- $\tilde{C}^{<t>} = \tanh(W_c[c^{<t-1>, x^{<t>}}] + b_c)$
- é—å¿˜é—¨ $\Gamma_u = \sigma()$,between 0 and 1, u: undate ,
  - $\Gamma_u$ æ˜¯å¦è®°ä½æŸä¸ªä¿¡æ¯
- $C^{<t>} = \Gamma_u *\tilde{C}^{<t>}  +(1-\Gamma_u) * C^{<t-1>}$
  - $\Gamma_u = 0$ ä¿æŒæ—§ä¿¡æ¯
    - å› ä¸ºæ˜¯$\sigma()$ï¼Œå½“å†…éƒ¨æ¯”è¾ƒè´Ÿçš„æ—¶å€™ï¼Œä¼šå§‹ç»ˆä¿æŒ
  - $\Gamma_u = 1$ é—å¿˜æ—§ä¿¡æ¯ï¼Œè®°ä½tæ—¶åˆ»çš„æ–°ä¿¡æ¯ï¼›

<img src="/Users/weiwang/Documents/NLP/rnn/GRU_unit.png" alt="GRU_unit" style="zoom:50%;" />



=> æ˜¾è‘—å¸®åŠ©vanishing gradient  problem

ã€Qã€‘æ²¡ç†è§£ï¼š$\Gamma_u$ æ¥è¿‘0ï¼Œæ‰€ä»¥ $C^{<t>} \approx C^{<t-1>}$ ä¸å°±æ„å‘³ç€ä¸æ›´æ–°äº†ä¹ˆ

FULL GRU

1. å¢åŠ Gate $\Gamma_r$ å‘Šè¯‰å¦‚æœé€šè¿‡å‰ä¸€ä¸ªæ—¶åˆ»çš„$C^{<t-1>}$è®¡ç®—tæ—¶åˆ»çš„å€™é€‰$\tilde{C}^{<t>}$
2. å…¶ä»–paperçš„notation
   1. h,u,c,h...

<img src="/Users/weiwang/Documents/NLP/rnn/gru_full.png" alt="gru_full" style="zoom:50%;" />





## LSTM

- $a^{<t>} = C^{<t>}$ ä¸æˆç«‹ï¼›
- æ›´æ–°$C^{<t>}$ çš„Gatesæœ‰ä¸¤ä¸ªupdate $\Gamma_u$ å’Œforget  $\Gamma_f$  
- å¢åŠ output gate $\Gamma_o$
  - $a^{<t>} = \Gamma_o C^{<t>}$

![lstm](/Users/weiwang/Documents/NLP/rnn/lstm.png)

æ³¨æ„åˆ°ï¼Œå½“è®¾ç½®åˆç†çš„ $\Gamma_u, \Gamma_f$åï¼Œä¸Šé¢ä¸€è¡Œï¼š $c^{<0>}$åˆ°$c^{<3>}$ å¯ä»¥å¾ˆå¿«ä¼ é€’ï¼Œä¿è¯äº†LSTMå¯ä»¥é•¿æ—¶é—´å¾ˆå¥½çš„è®°ä½æŸäº›ä¿¡æ¯

### Peephole connection ï¼ˆLSTMçš„å˜å½¢ï¼‰

- Gates also depends on $c^{<t-1>}$



#### GRU VS LSTM

- GRU 
  - æ›´ç®€å•ï¼Œå¯ä»¥å»ºæ›´å¤§çš„æ¨¡å‹

- LSTM
  - more powerful, more effective 
  - more proven choice 



### Bidirectional RNN

- å•å‘ä¸å¤Ÿï¼åªç»™å‰ä¸¤ä¸ªå•è¯ "He said"ï¼Œæ— æ³•åˆ¤æ–­ "Teddy" æ˜¯äººå è¿˜æ˜¯å…¶ä»–
- <img src="/Users/weiwang/Documents/NLP/rnn/problem.png" alt="problem" style="zoom:50%;" />

<img src="/Users/weiwang/Documents/NLP/rnn/BRNN.png" alt="BRNN" style="zoom:50%;" />



- (-)éœ€è¦entire sentence 



## Deep RNNs

<img src="/Users/weiwang/Documents/NLP/rnn/deepRNN.png" alt="deepRNN" style="zoom:50%;" />

- å¯¹RNNæ¥è¯´ï¼Œ3å±‚å·²ç»å¾ˆæ·±äº†
  - å› ä¸ºæ°´å¹³å·²ç»å¾ˆé•¿äº†
- æ›´å¸¸è§çš„æ˜¯ï¼Œ3å±‚RNNåç»­åœ¨$y^{<t>}$ çš„ä½ç½®åŠ å¤šå±‚çš„NNï¼ˆæ·±ä½†æ²¡æœ‰æ°´å¹³è¿æ¥ï¼‰
- 
