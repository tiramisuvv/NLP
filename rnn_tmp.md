input x: ä¸€ä¸ªå¥å­ï¼Œé•¿åº¦è®°ä¸º $T_x$

- $x^{<t>}$ word at position t

$x^{(i)<t>}$ ç¬¬iä¸ªinputçš„tä½ç½®ï¼Œ$T_{x}^{(i)}$ å¯¹ä¸åŒsampleå¯èƒ½ä¸åŒï¼Œå¦‚æœ‰çš„å¥å­9ä¸ªå•è¯ï¼Œæœ‰çš„15ä¸ªå•è¯



output y:

Vacabulary :30K-50K



# RNN

- ç›´æ¥ç”¨NN

  - input,outputs é•¿åº¦ä¸åŒï¼›
  - doesn't share features learned across different positions of text

  - a better model can reduce # of parameters 

- ç»“æ„ architectures 

  - $T_x = T_y$  

 <img src="./rnn/structure.png" alt="structure" style="zoom:50%;" />

- åˆ©ç”¨å‰é¢çš„ä¿¡æ¯

- å…±äº«å‚æ•°

- ç¼ºç‚¹ åªç”¨å‰é¢ä¿¡æ¯ï¼Œä¸ç”¨åé¢ä¿¡æ¯

  - He said 
  - He said
  - -ã€‹ Bidirectional RNNï¼ˆBRNNï¼‰

  $W_{ax}$ ä¹˜ä»¥xï¼Œç”¨äºè®¡ç®—aï¼Œæ¯”å¦‚$a^{<1>} = g(W_{aa}a^{<0>}+W_{ax}x^{<1>}+b_a)$

  [TODO] $T_x \neq T_y$ æ˜¯æ€ä¹ˆåšçš„-> different types

  ## Different types

  1. many-to-many
     1. $T_x = T_y$
     2. $T_x \neq T_y$
        1. machine translation 
        2. 
  2. many-to-one
     1. ğŸŒ° setimant classification
  3. one-to-many
     1. music generation 
     2. 

  <img src="./rnn/summary_types.png" alt="summary_types" style="zoom:50%;" />

  
  
  

## Backpropagation through time

Sampling a sequence from a **trained** RNN 

åŒºåˆ«äºä¹‹å‰ï¼Œç°åœ¨çš„input $x^{<2>} = \hat{y}^{<1>}$ è€Œä¸æ˜¯ $y^{<1>}$ ã€Qã€‘

Novel Sequences 

Character-level language model

- (+) ä¸ä¼š<unk>
- (-) match longer sequence
- (-) much computational expensive



## Vanishing Gradients with RNN

### Why happened

- may have very long term dependency 
  - The ==cat==. Which already ate ..., ==was== full

  - The ==cats==. Which already ate ..., ==were== full

- $\frac{\part \text{error}}{\part \text{å‰æ’å‚æ•°}} \approx 0$ 

- The basic RNN model has many local influence



1. vanishing gradient bigger problem
2. exploding graident also problem
   1. å®¹æ˜“å‘ç°
   2. solution: gradient clipping

### GRU gated recurrent unit 

**Motivation** cat---> was

= modification of hidden layer in RNN 

+ (+) capture long range connections
+ (+)Helps a lot with  vanishing gradient problem

<img src="/Users/weiwang/Documents/NLP/rnn/rnn_unit.png" alt="rnn_unit" style="zoom:50%;" />



å¢åŠ  c = memory cell æ¥å­˜å‚¨ä¿¡æ¯ï¼Œæ¯”å¦‚ catæ˜¯å•æ•°è¿˜æ˜¯å¤æ•°

$c^{<t>}:=$ memory cell value

$a^{<t>}:=$ output activation value

In GRUï¼Œå– $c^{<t>}:= a^{<t>}$ ï¼Œä½†åœ¨LSTMä¸­ä¸åŒã€‚

- $\tilde{C}^{<t>} = \tanh(W_c[c^{<t-1>, x^{<t>}}] + b_c)$
- é—å¿˜é—¨ $\Gamma_u = \sigma()$,between 0 and 1, u: undate ,
  - $\Gamma_u$ æ˜¯å¦è®°ä½æŸä¸ªä¿¡æ¯
- $C^{<t>} = \Gamma_u *\tilde{C}^{<t>}  +(1-\Gamma_u) * C^{<t-1>}$
  - $\Gamma_u = 0$ ä¿æŒæ—§ä¿¡æ¯
    - å› ä¸ºæ˜¯$\sigma()$ï¼Œå½“å†…éƒ¨æ¯”è¾ƒè´Ÿçš„æ—¶å€™ï¼Œä¼šå§‹ç»ˆä¿æŒ
  - $\Gamma_u = 1$ é—å¿˜æ—§ä¿¡æ¯ï¼Œè®°ä½tæ—¶åˆ»çš„æ–°ä¿¡æ¯ï¼›

<img src="/Users/weiwang/Documents/NLP/rnn/GRU_unit.png" alt="GRU_unit" style="zoom:50%;" />



=> æ˜¾è‘—å¸®åŠ©vanishing gradient  problem

ã€Qã€‘æ²¡ç†è§£ï¼š$\Gamma_u$ æ¥è¿‘0ï¼Œæ‰€ä»¥ $C^{<t>} \approx C^{<t-1>}$ ä¸å°±æ„å‘³ç€ä¸æ›´æ–°äº†ä¹ˆ

FULL GRU

1. å¢åŠ Gate $\Gamma_r$ å‘Šè¯‰å¦‚æœé€šè¿‡å‰ä¸€ä¸ªæ—¶åˆ»çš„$C^{<t-1>}$è®¡ç®—tæ—¶åˆ»çš„å€™é€‰$\tilde{C}^{<t>}$
2. å…¶ä»–paperçš„notation
   1. h,u,c,h...

<img src="/Users/weiwang/Documents/NLP/rnn/gru_full.png" alt="gru_full" style="zoom:50%;" />





## LSTM

- $a^{<t>} = C^{<t>}$ ä¸æˆç«‹ï¼›
- æ›´æ–°$C^{<t>}$ çš„Gatesæœ‰ä¸¤ä¸ªupdate $\Gamma_u$ å’Œforget  $\Gamma_f$  
- å¢åŠ output gate $\Gamma_o$
  - $a^{<t>} = \Gamma_o C^{<t>}$

![lstm](/Users/weiwang/Documents/NLP/rnn/lstm.png)

æ³¨æ„åˆ°ï¼Œå½“è®¾ç½®åˆç†çš„ $\Gamma_u, \Gamma_f$åï¼Œä¸Šé¢ä¸€è¡Œï¼š $c^{<0>}$åˆ°$c^{<3>}$ å¯ä»¥å¾ˆå¿«ä¼ é€’ï¼Œä¿è¯äº†LSTMå¯ä»¥é•¿æ—¶é—´å¾ˆå¥½çš„è®°ä½æŸäº›ä¿¡æ¯

### Peephole connection ï¼ˆLSTMçš„å˜å½¢ï¼‰

- Gates also depends on $c^{<t-1>}$



#### GRU VS LSTM

- GRU 
  - æ›´ç®€å•ï¼Œå¯ä»¥å»ºæ›´å¤§çš„æ¨¡å‹

- LSTM
  - more powerful, more effective 
  - more proven choice 



### Bidirectional RNN

- å•å‘ä¸å¤Ÿï¼åªç»™å‰ä¸¤ä¸ªå•è¯ "He said"ï¼Œæ— æ³•åˆ¤æ–­ "Teddy" æ˜¯äººå è¿˜æ˜¯å…¶ä»–
- <img src="/Users/weiwang/Documents/NLP/rnn/problem.png" alt="problem" style="zoom:50%;" />

<img src="/Users/weiwang/Documents/NLP/rnn/BRNN.png" alt="BRNN" style="zoom:50%;" />



- (-)éœ€è¦entire sentence 



## Deep RNNs

<img src="/Users/weiwang/Documents/NLP/rnn/deepRNN.png" alt="deepRNN" style="zoom:50%;" />

- å¯¹RNNæ¥è¯´ï¼Œ3å±‚å·²ç»å¾ˆæ·±äº†
  - å› ä¸ºæ°´å¹³å·²ç»å¾ˆé•¿äº†
- æ›´å¸¸è§çš„æ˜¯ï¼Œ3å±‚RNNåç»­åœ¨$y^{<t>}$ çš„ä½ç½®åŠ å¤šå±‚çš„NNï¼ˆæ·±ä½†æ²¡æœ‰æ°´å¹³è¿æ¥ï¼‰
- 
