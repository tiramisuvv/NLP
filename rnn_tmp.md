<img src="./rnn/plan.png" alt="plan" style="zoom:50%;" />







input x: ä¸€ä¸ªå¥å­ï¼Œé•¿åº¦è®°ä¸º $T_x$

- $x^{<t>}$ word at position t

$x^{(i)<t>}$ ç¬¬iä¸ªinputçš„tä½ç½®ï¼Œ$T_{x}^{(i)}$ å¯¹ä¸åŒsampleå¯èƒ½ä¸åŒï¼Œå¦‚æœ‰çš„å¥å­9ä¸ªå•è¯ï¼Œæœ‰çš„15ä¸ªå•è¯



output y:

Vacabulary :30K-50K



# RNN

- ç›´æ¥ç”¨NN

  - input,outputs é•¿åº¦ä¸åŒï¼›
  - doesn't share features learned across different positions of text

  - a better model can reduce # of parameters 

- ç»“æ„ architectures 

  - $T_x = T_y$

- <img src="./rnn/structure.png" alt="structure" style="zoom:50%;" />
- 

- åˆ©ç”¨å‰é¢çš„ä¿¡æ¯

- å…±äº«å‚æ•°

- ç¼ºç‚¹ åªç”¨å‰é¢ä¿¡æ¯ï¼Œä¸ç”¨åé¢ä¿¡æ¯

  - He said 
  - He said
  - -ã€‹ Bidirectional RNNï¼ˆBRNNï¼‰

  $W_{ax}$ ä¹˜ä»¥xï¼Œç”¨äºè®¡ç®—aï¼Œæ¯”å¦‚$a^{<1>} = g(W_{aa}a^{<0>}+W_{ax}x^{<1>}+b_a)$

  [TODO] $T_x \neq T_y$ æ˜¯æ€ä¹ˆåšçš„-> different types

  

  

  # Recurrent Neural Networks (RNN)

  <img src="./rnn/architecture.png" alt="architecture" style="zoom:50%;" />

  -  hidden **states**: a single state that's mutating over time(several version of the same thing) 
  - **Core idea** Apply the same weights W repeatedly

  

  ## Example 

  <img src="./rnn/simple_example.png" alt="simple_example" style="zoom:50%;" />

- current hidden state based on previous hidden state and current input 

âœ…

- Can process **any length** input
- Computation for step t can (in theory) use information from **many steps back**
- **Model size doesnâ€™t increase** for longer input context 

- Same weights applied on every timestep, so there is **symmetry** in how inputs are processed.

â 

- Recurrent computation is **slow** 
- In practice, difficult to access information from **many steps back**



## Training a RNN Language Model

<img src="/Users/Wei/Library/Application Support/typora-user-images/Screen Shot 2021-07-13 at 10.39.52 PM.png" alt="Screen Shot 2021-07-13 at 10.39.52 PM" style="zoom:50%;" />



<img src="./rnn/how_to_train.png" alt="how_to_train" style="zoom:50%;" />

<img src="./rnn/how_to_train2.png" alt="how_to_train2" style="zoom:50%;" />

**Problem** : Computing loss and gradients across entire corpus $$ is too expensive!

<img src="./rnn/Screen Shot 2021-07-13 at 10.43.36 PM.png" alt="Screen Shot 2021-07-13 at 10.43.36 PM" style="zoom:50%;" />

- ç¬¬äºŒæ¡è¡¥å……ï¼šshorter unit of text

## Backpropagation for RNN

<img src="./rnn/backprop.png" alt="backprop" style="zoom:50%;" />

<img src="./rnn/backprop2.png" alt="backprop2" style="zoom:50%;" />

<img src="./rnn/backprop3.png" alt="backprop3" style="zoom:50%;" />

<img src="./rnn/backprop4.png" alt="backprop4" style="zoom:50%;" />

[TODO]

## Generating text with an RNN Language Model

<img src="./rnn/generate_text_example.png" alt="generate_text_example" style="zoom:50%;" />



- RNN ä¸èƒ½è®°ä½overallçš„ä¿¡æ¯ï¼ˆæ¯”å¦‚ä¹‹å‰åœ¨åšæˆ–è€…åšè¿‡ä»€ä¹ˆäº‹æƒ…ï¼‰



[TODO: ç§»å…¥language modelçš„ç¬”è®°]

## Evaluating Language Models

<img src="./rnn/evaluation.png" alt="evaluation" style="zoom:50%;" />

=> min(J(\theta)) ç­‰ä»·äº min(perplexity)



## Why should we care about Language Modeling? 

- Language Modeling is a **benchmark task** that helps us **measure our progress** on understanding language
- Language Modeling is a **subcomponent** of many NLP tasks, especially those involving **generating text** or **estimating the probability of text**:
  - Predictive typing 
  - Speech recognition 
  - Handwriting recognition 
  - Spelling/grammar correction 
  - Authorship identification 
  - Machine translation 
  - Summarization 
  - Dialogue 
  - etc

<img src="./rnn/Screen Shot 2021-07-13 at 11.14.39 PM.png" alt="Screen Shot 2021-07-13 at 11.14.39 PM" style="zoom:50%;" />



## Other applications of RNN

### 1. tagging e.g., part-of-speech tagging, named entity recognition

<img src="./rnn/application_example1.png" alt="application_example1" style="zoom:50%;" />

### 2 .sentiment classification

<img src="./rnn/application_example2_1.png" alt="application_example2_1" style="zoom:50%;" />

<img src="./rnn/application_example2_2.png" alt="application_example2_2" style="zoom:50%;" />

### 3. encode model

<img src="./rnn/application_example3.png" alt="application_example3" style="zoom:50%;" />

æ–‡å­—ç‰ˆçš„é—®é¢˜ --é€šè¿‡RNN --> ä»£è¡¨questionçš„something

### 4. generate text

<img src="./rnn/application_example4.png" alt="application_example4" style="zoom:50%;" />

- åœ¨speech recognitioné‡Œï¼Œä¸€èˆ¬ç”¨WER ï¼ˆword error rateï¼‰ä½œä¸ºè¡¡é‡æ ‡å‡†ï¼Œæœ‰æ—¶å€™ä¹Ÿç”¨ perplexity
- conditional language model =there's some kind of input that we need to condition on

## Different types

1. many-to-many
   1. $T_x = T_y$
   2. $T_x \neq T_y$
      1. machine translation 
      2. 
2. many-to-one
   1. ğŸŒ° setimant classification
3. one-to-many
   1. music generation 
   2. 

<img src="./rnn/summary_types.png" alt="summary_types" style="zoom:50%;" />





## Backpropagation through time

Sampling a sequence from a **trained** RNN 

åŒºåˆ«äºä¹‹å‰ï¼Œç°åœ¨çš„input $x^{<2>} = \hat{y}^{<1>}$ è€Œä¸æ˜¯ $y^{<1>}$ ã€Qã€‘

Novel Sequences 

Character-level language model

- (+) ä¸ä¼š<unk>
- (-) match longer sequence
- (-) much computational expensive



## Vanishing Gradients with RNN

### 1. Why happened

<img src="/Users/weiwang/Documents/NLP/rnn/vanishing_gradient.png" alt="vanishing_gradient" style="zoom:50%;" />

- may have very long term dependency 
  - The ==cat==. Which already ate ..., ==was== full

  - The ==cats==. Which already ate ..., ==were== full
- $\frac{\part \text{error}}{\part \text{å‰æ’å‚æ•°}} \approx 0$ 
- The basic RNN model has many local influence

### 2 why a problem

#### 1. ä¸§å¤± long term effect



<img src="/Users/weiwang/Documents/NLP/rnn/vanishing_gradient2.png" alt="vanishing_gradient2" style="zoom:50%;" />



#### 2.  å½“gradientå¾ˆå°çš„æ—¶å€™ï¼Œæ— æ³•åˆ¤æ–­æ˜¯å®Œæˆå­¦ä¹ è¿˜æ˜¯é”™è¯¯å‚æ•°

Gradient = a measure of the effect of the past on the future

it the gradient becomes vanishingly small over longer distances (step t to t+n), we can't tell whether:

a. There's **no dependency** between step t and t+n in the data -> è¿™ç§æƒ…å†µä¸‹ï¼Œæœ¬èº«t å’Œ t+n æ²¡æœ‰å…³è”ï¼Œæ‰€ä»¥ gradient å¾ˆå°æ˜¯åˆç†çš„ï¼›

b. We have **wrong parameters** to capture the true dependency between t and t+n -> è¿™ç§æƒ…å†µä¸‹ï¼Œæœ¬èº«æ˜¯æœ‰å…³è”çš„ï¼Œç†è®ºä¸Šä¹Ÿåº”è¯¥å­¦ä¹ åˆ°è¿™ç§å…³ç³»ï¼Œä½†æ˜¯ç”±äºæˆ‘ä»¬é”™è¯¯çš„å‚æ•°ï¼Œè®©æ¨¡å‹è®¤ä¸ºä»–ä»¬ä¹‹é—´æ˜¯æ²¡æœ‰å…³ç³»çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¹Ÿå­¦ä¸åˆ°ä¸¤è€…ä¹‹é—´çš„ä¾èµ–å…³ç³»

Gradient vanishing è®©æˆ‘ä»¬æ— æ³•ç¡®å®šä¸ŠäºŒè€…å“ªä¸ªå‘ç”Ÿ

### 3. Example ğŸŒ°

1. **LM task**: When she tried to print her ==tickets==, she found that the printer was out of toner. She went to the stationery store to buy more toner. It was very overpriced. After installing the toner into the printer, she finally printed her ________

- æ¨¡å‹éœ€è¦å­¦ä¹ åˆ° 7th step çš„ **"tickets"** å’Œ ç›®æ ‡å•è¯  ________ çš„å…³ç³»ã€‚

2. **LM task**: The writer of the boos ________ 
   - IS
   - ARE

- syntactic recency (è¯­æ³•æ–°è¿‘åº¦) ï¼šThe  <u>writer</u> of the books <u>is</u> ï¼ˆcorrect)
- sequential recency (é¡ºåºæ–°è¿‘åº¦) :The writer of the <u>books</u> <u>are</u> (incorrect)
- RNN æ›´æ“…é•¿å­¦ä¹   sequential recencyï¼Œè€Œä¸æ˜¯ syntactic recency



### 4. Solution

- The main problem = RNN ä¸èƒ½åœ¨é•¿æ—¶é—´å†…ä¿å­˜ä¿¡æ¯ã€‚

  -  **itâ€™s too difficult for the RNN to learn to preserve information over many timesteps.**

- In a vanilla RNN, the hidden state is **constantly being rewritten**
  $$
  h^{(t)} = \sigma\left ( W_hh^{(t-1)} + W_x x^{(t)} + b\right)
  $$

  - æ‰€ä»¥å¾ˆéš¾æŠŠä¿¡æ¯ä»å‰ä¸€ä¸ªhidden state ä¿å­˜åˆ° ä¸‹ä¸€ä¸ªhidden state

- **Idea**ï¼šAdd separate **memory** to  a RNN 

## Exploding gradient

<img src="/Users/weiwang/Documents/NLP/rnn/sgd.png" alt="sgd" style="zoom:30%;" />

- å½“gradient å¾ˆå¤§çš„æ—¶å€™ï¼Œæ›´æ–°å˜åŒ–å¾ˆå¤§
- å¯èƒ½å‡ºç° **inf** æˆ–è€… **NaN**

### Solution: Gradient clipping

å½“ gradient > æŸä¸ªthreshold, ä½¿ç”¨ scaleåçš„ gradient å¯¹SGDæ›´æ–°

<img src="/Users/weiwang/Documents/NLP/rnn/grad_clipping.png" alt="grad_clipping" style="zoom:50%;" />

**Idea** ä¸æ”¹å˜æ–¹å‘ï¼Œåªæ›´æ–°ä¸€å°æ­¥

<img src="/Users/weiwang/Documents/NLP/rnn/grad_clipping2.png" alt="grad_clipping2" style="zoom:50%;" />

1. exploding graident also problem
   1. å®¹æ˜“å‘ç°
   2. solution: gradient clipping

### GRU gated recurrent unit 

**Motivation** cat---> was

= modification of hidden layer in RNN 

+ (+) capture long range connections
+ (+)Helps a lot with  vanishing gradient problem

<img src="/Users/weiwang/Documents/NLP/rnn/rnn_unit.png" alt="rnn_unit" style="zoom:50%;" />



å¢åŠ  c = memory cell æ¥å­˜å‚¨ä¿¡æ¯ï¼Œæ¯”å¦‚ catæ˜¯å•æ•°è¿˜æ˜¯å¤æ•°

$c^{<t>}:=$ memory cell value

$a^{<t>}:=$ output activation value

In GRUï¼Œå– $c^{<t>}:= a^{<t>}$ ï¼Œä½†åœ¨LSTMä¸­ä¸åŒã€‚

- $\tilde{C}^{<t>} = \tanh(W_c[c^{<t-1>, x^{<t>}}] + b_c)$
- é—å¿˜é—¨ $\Gamma_u = \sigma()$,between 0 and 1, u: undate ,
  - $\Gamma_u$ æ˜¯å¦è®°ä½æŸä¸ªä¿¡æ¯
- $C^{<t>} = \Gamma_u *\tilde{C}^{<t>}  +(1-\Gamma_u) * C^{<t-1>}$
  - $\Gamma_u = 0$ ä¿æŒæ—§ä¿¡æ¯
    - å› ä¸ºæ˜¯$\sigma()$ï¼Œå½“å†…éƒ¨æ¯”è¾ƒè´Ÿçš„æ—¶å€™ï¼Œä¼šå§‹ç»ˆä¿æŒ
  - $\Gamma_u = 1$ é—å¿˜æ—§ä¿¡æ¯ï¼Œè®°ä½tæ—¶åˆ»çš„æ–°ä¿¡æ¯ï¼›

<img src="/Users/weiwang/Documents/NLP/rnn/GRU_unit.png" alt="GRU_unit" style="zoom:50%;" />



=> æ˜¾è‘—å¸®åŠ©vanishing gradient  problem

ã€Qã€‘æ²¡ç†è§£ï¼š$\Gamma_u$ æ¥è¿‘0ï¼Œæ‰€ä»¥ $C^{<t>} \approx C^{<t-1>}$ ä¸å°±æ„å‘³ç€ä¸æ›´æ–°äº†ä¹ˆ

FULL GRU

1. å¢åŠ Gate $\Gamma_r$ å‘Šè¯‰å¦‚æœé€šè¿‡å‰ä¸€ä¸ªæ—¶åˆ»çš„$C^{<t-1>}$è®¡ç®—tæ—¶åˆ»çš„å€™é€‰$\tilde{C}^{<t>}$
2. å…¶ä»–paperçš„notation
   1. h,u,c,h...

<img src="/Users/weiwang/Documents/NLP/rnn/gru_full.png" alt="gru_full" style="zoom:50%;" />

## LSTM

### 1. Structrue

é™¤äº† hidden stateï¼Œç›¸æ¯”RNN å¢åŠ äº† cell stateæ¥å­˜å‚¨é•¿æœŸä¿¡æ¯ï¼›

- On step t, there is a **hidden state** $h^{(t)}$ and a **cell state** $c^{(t)}$ 
  - Both are vectors length n 
  - The cell stores **long-term information**
  - The LSTM can **read**, **erase**, and **write** information from the cell
    -  The cell becomes conceptually rather like RAM in a computer

ä½¿ç”¨3ä¸ª ğŸšª æ¥æ§åˆ¶å¯¹ä¿¡æ¯çš„è¯»ï¼Œæ“¦ï¼Œå†™

- The selection of which information is erased/written/read is controlled by three corresponding **gates**
  - The gates are also vectors length n 
  - On each timestep, each element of the gates can be **open** (1),** closed** (0), or somewhere in-between
  - The gates are **dynamic**: their value is computed based on the current context 

### 2. Equations

<img src="/Users/weiwang/Documents/NLP/rnn/lstm2.png" alt="lstm2" style="zoom:50%;" />

é—®é¢˜1: ä¸ºä»€ä¹ˆ forget gate åªä½¿ç”¨ $h^{(t-1)}$ çš„ä¿¡æ¯ï¼Œè€Œä¸ç”¨ $c^{(t-1)}$ï¼Œå³ç›´æ¥çœ‹ä¹‹å‰çš„åŸå§‹æ•°æ®æ¥å†³å®šè¦ä¸è¦èˆå¼ƒï¼›

é—®é¢˜2: ä¸ºä»€ä¹ˆhidden state é‡Œï¼Œè¦å¯¹ $c^{(t)}$ å» $\tanh$ 

### 3. Graph Version



<img src="/Users/weiwang/Documents/NLP/rnn/lstm3.png" alt="lstm3" style="zoom:50%;" />

<img src="/Users/weiwang/Documents/NLP/rnn/lstm4.png" alt="lstm4" style="zoom:50%;" />







- $a^{<t>} = C^{<t>}$ ä¸æˆç«‹ï¼›
- æ›´æ–°$C^{<t>}$ çš„Gatesæœ‰ä¸¤ä¸ªupdate $\Gamma_u$ å’Œforget  $\Gamma_f$  
- å¢åŠ output gate $\Gamma_o$
  - $a^{<t>} = \Gamma_o C^{<t>}$

![lstm](/Users/weiwang/Documents/NLP/rnn/lstm.png)

æ³¨æ„åˆ°ï¼Œå½“è®¾ç½®åˆç†çš„ $\Gamma_u, \Gamma_f$åï¼Œä¸Šé¢ä¸€è¡Œï¼š $c^{<0>}$åˆ°$c^{<3>}$ å¯ä»¥å¾ˆå¿«ä¼ é€’ï¼Œä¿è¯äº†LSTMå¯ä»¥é•¿æ—¶é—´å¾ˆå¥½çš„è®°ä½æŸäº›ä¿¡æ¯

### Peephole connection ï¼ˆLSTMçš„å˜å½¢ï¼‰

- Gates also depends on $c^{<t-1>}$



#### GRU VS LSTM

- GRU 
  - æ›´ç®€å•ï¼Œå¯ä»¥å»ºæ›´å¤§çš„æ¨¡å‹

- LSTM
  - more powerful, more effective 
  - more proven choice 



### Bidirectional RNN

- å•å‘ä¸å¤Ÿï¼åªç»™å‰ä¸¤ä¸ªå•è¯ "He said"ï¼Œæ— æ³•åˆ¤æ–­ "Teddy" æ˜¯äººå è¿˜æ˜¯å…¶ä»–
- <img src="/Users/weiwang/Documents/NLP/rnn/problem.png" alt="problem" style="zoom:50%;" />

<img src="/Users/weiwang/Documents/NLP/rnn/BRNN.png" alt="BRNN" style="zoom:50%;" />



- (-)éœ€è¦entire sentence 



## Deep RNNs

<img src="/Users/weiwang/Documents/NLP/rnn/deepRNN.png" alt="deepRNN" style="zoom:50%;" />

- å¯¹RNNæ¥è¯´ï¼Œ3å±‚å·²ç»å¾ˆæ·±äº†
  - å› ä¸ºæ°´å¹³å·²ç»å¾ˆé•¿äº†
- æ›´å¸¸è§çš„æ˜¯ï¼Œ3å±‚RNNåç»­åœ¨$y^{<t>}$ çš„ä½ç½®åŠ å¤šå±‚çš„NNï¼ˆæ·±ä½†æ²¡æœ‰æ°´å¹³è¿æ¥ï¼‰
- 
