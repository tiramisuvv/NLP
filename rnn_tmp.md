<img src="./rnn/plan.png" alt="plan" style="zoom:50%;" />







input x: ä¸€ä¸ªå¥å­ï¼Œé•¿åº¦è®°ä¸º $T_x$

- $x^{<t>}$ word at position t

$x^{(i)<t>}$ ç¬¬iä¸ªinputçš„tä½ç½®ï¼Œ$T_{x}^{(i)}$ å¯¹ä¸åŒsampleå¯èƒ½ä¸åŒï¼Œå¦‚æœ‰çš„å¥å­9ä¸ªå•è¯ï¼Œæœ‰çš„15ä¸ªå•è¯



output y:

Vacabulary :30K-50K



# RNN

- ç›´æ¥ç”¨NN

  - input,outputs é•¿åº¦ä¸åŒï¼›
  - doesn't share features learned across different positions of text

  - a better model can reduce # of parameters 

- ç»“æ„ architectures 

  - $T_x = T_y$

- <img src="./rnn/structure.png" alt="structure" style="zoom:50%;" />
- 

- åˆ©ç”¨å‰é¢çš„ä¿¡æ¯

- å…±äº«å‚æ•°

- ç¼ºç‚¹ åªç”¨å‰é¢ä¿¡æ¯ï¼Œä¸ç”¨åé¢ä¿¡æ¯

  - He said 
  - He said
  - -ã€‹ Bidirectional RNNï¼ˆBRNNï¼‰

  $W_{ax}$ ä¹˜ä»¥xï¼Œç”¨äºè®¡ç®—aï¼Œæ¯”å¦‚$a^{<1>} = g(W_{aa}a^{<0>}+W_{ax}x^{<1>}+b_a)$

  [TODO] $T_x \neq T_y$ æ˜¯æ€ä¹ˆåšçš„-> different types

  

  

  # Recurrent Neural Networks (RNN)

  <img src="./rnn/architecture.png" alt="architecture" style="zoom:50%;" />

  -  hidden **states**: a single state that's mutating over time(several version of the same thing) 
  - **Core idea** Apply the same weights W repeatedly

  

  ## Example 

  <img src="./rnn/simple_example.png" alt="simple_example" style="zoom:50%;" />

- current hidden state based on previous hidden state and current input 

âœ…

- Can process **any length** input
- Computation for step t can (in theory) use information from **many steps back**
- **Model size doesnâ€™t increase** for longer input context 

- Same weights applied on every timestep, so there is **symmetry** in how inputs are processed.

â 

- Recurrent computation is **slow** 
- In practice, difficult to access information from **many steps back**



## Training a RNN Language Model

<img src="/Users/Wei/Library/Application Support/typora-user-images/Screen Shot 2021-07-13 at 10.39.52 PM.png" alt="Screen Shot 2021-07-13 at 10.39.52 PM" style="zoom:50%;" />



<img src="./rnn/how_to_train.png" alt="how_to_train" style="zoom:50%;" />

<img src="./rnn/how_to_train2.png" alt="how_to_train2" style="zoom:50%;" />

**Problem** : Computing loss and gradients across entire corpus $$ is too expensive!

<img src="./rnn/Screen Shot 2021-07-13 at 10.43.36 PM.png" alt="Screen Shot 2021-07-13 at 10.43.36 PM" style="zoom:50%;" />

- ç¬¬äºŒæ¡è¡¥å……ï¼šshorter unit of text

## Backpropagation for RNN

<img src="./rnn/backprop.png" alt="backprop" style="zoom:50%;" />

<img src="./rnn/backprop2.png" alt="backprop2" style="zoom:50%;" />

<img src="./rnn/backprop3.png" alt="backprop3" style="zoom:50%;" />

<img src="./rnn/backprop4.png" alt="backprop4" style="zoom:50%;" />

[TODO]

## Generating text with an RNN Language Model

<img src="./rnn/generate_text_example.png" alt="generate_text_example" style="zoom:50%;" />



- RNN ä¸èƒ½è®°ä½overallçš„ä¿¡æ¯ï¼ˆæ¯”å¦‚ä¹‹å‰åœ¨åšæˆ–è€…åšè¿‡ä»€ä¹ˆäº‹æƒ…ï¼‰



[TODO: ç§»å…¥language modelçš„ç¬”è®°]

## Evaluating Language Models

<img src="./rnn/evaluation.png" alt="evaluation" style="zoom:50%;" />

=> min(J(\theta)) ç­‰ä»·äº min(perplexity)



## Why should we care about Language Modeling? 

- Language Modeling is a **benchmark task** that helps us **measure our progress** on understanding language
- Language Modeling is a **subcomponent** of many NLP tasks, especially those involving **generating text** or **estimating the probability of text**:
  - Predictive typing 
  - Speech recognition 
  - Handwriting recognition 
  - Spelling/grammar correction 
  - Authorship identification 
  - Machine translation 
  - Summarization 
  - Dialogue 
  - etc

<img src="./rnn/Screen Shot 2021-07-13 at 11.14.39 PM.png" alt="Screen Shot 2021-07-13 at 11.14.39 PM" style="zoom:50%;" />



## Other applications of RNN

### 1. tagging e.g., part-of-speech tagging, named entity recognition

<img src="./rnn/application_example1.png" alt="application_example1" style="zoom:50%;" />

### 2 .sentiment classification

<img src="./rnn/application_example2_1.png" alt="application_example2_1" style="zoom:50%;" />

<img src="./rnn/application_example2_2.png" alt="application_example2_2" style="zoom:50%;" />

### 3. encode model

<img src="./rnn/application_example3.png" alt="application_example3" style="zoom:50%;" />

æ–‡å­—ç‰ˆçš„é—®é¢˜ --é€šè¿‡RNN --> ä»£è¡¨questionçš„something

### 4. generate text

<img src="./rnn/application_example4.png" alt="application_example4" style="zoom:50%;" />

- åœ¨speech recognitioné‡Œï¼Œä¸€èˆ¬ç”¨WER ï¼ˆword error rateï¼‰ä½œä¸ºè¡¡é‡æ ‡å‡†ï¼Œæœ‰æ—¶å€™ä¹Ÿç”¨ perplexity
- conditional language model =there's some kind of input that we need to condition on

## Different types

1. many-to-many
   1. $T_x = T_y$
   2. $T_x \neq T_y$
      1. machine translation 
      2. 
2. many-to-one
   1. ğŸŒ° setimant classification
3. one-to-many
   1. music generation 
   2. 

<img src="./rnn/summary_types.png" alt="summary_types" style="zoom:50%;" />





## Backpropagation through time

Sampling a sequence from a **trained** RNN 

åŒºåˆ«äºä¹‹å‰ï¼Œç°åœ¨çš„input $x^{<2>} = \hat{y}^{<1>}$ è€Œä¸æ˜¯ $y^{<1>}$ ã€Qã€‘

Novel Sequences 

Character-level language model

- (+) ä¸ä¼š<unk>
- (-) match longer sequence
- (-) much computational expensive



## Vanishing Gradients with RNN

### 1. Why happened

<img src="./rnn/vanishing_gradient.png" alt="vanishing_gradient" style="zoom:50%;" />

- may have very long term dependency 
  - The ==cat==. Which already ate ..., ==was== full

  - The ==cats==. Which already ate ..., ==were== full
- $\frac{\part \text{error}}{\part \text{å‰æ’å‚æ•°}} \approx 0$ 
- The basic RNN model has many local influence

### 2 why a problem

#### 1. ä¸§å¤± long term effect



<img src="./rnn/vanishing_gradient2.png" alt="vanishing_gradient2" style="zoom:50%;" />



#### 2.  å½“gradientå¾ˆå°çš„æ—¶å€™ï¼Œæ— æ³•åˆ¤æ–­æ˜¯å®Œæˆå­¦ä¹ è¿˜æ˜¯é”™è¯¯å‚æ•°

Gradient = a measure of the effect of the past on the future

it the gradient becomes vanishingly small over longer distances (step t to t+n), we can't tell whether:

a. There's **no dependency** between step t and t+n in the data -> è¿™ç§æƒ…å†µä¸‹ï¼Œæœ¬èº«t å’Œ t+n æ²¡æœ‰å…³è”ï¼Œæ‰€ä»¥ gradient å¾ˆå°æ˜¯åˆç†çš„ï¼›

b. We have **wrong parameters** to capture the true dependency between t and t+n -> è¿™ç§æƒ…å†µä¸‹ï¼Œæœ¬èº«æ˜¯æœ‰å…³è”çš„ï¼Œç†è®ºä¸Šä¹Ÿåº”è¯¥å­¦ä¹ åˆ°è¿™ç§å…³ç³»ï¼Œä½†æ˜¯ç”±äºæˆ‘ä»¬é”™è¯¯çš„å‚æ•°ï¼Œè®©æ¨¡å‹è®¤ä¸ºä»–ä»¬ä¹‹é—´æ˜¯æ²¡æœ‰å…³ç³»çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¹Ÿå­¦ä¸åˆ°ä¸¤è€…ä¹‹é—´çš„ä¾èµ–å…³ç³»

Gradient vanishing è®©æˆ‘ä»¬æ— æ³•ç¡®å®šä¸ŠäºŒè€…å“ªä¸ªå‘ç”Ÿ

### 3. Example ğŸŒ°

1. **LM task**: When she tried to print her ==tickets==, she found that the printer was out of toner. She went to the stationery store to buy more toner. It was very overpriced. After installing the toner into the printer, she finally printed her ________

- æ¨¡å‹éœ€è¦å­¦ä¹ åˆ° 7th step çš„ **"tickets"** å’Œ ç›®æ ‡å•è¯  ________ çš„å…³ç³»ã€‚

2. **LM task**: The writer of the boos ________ 
   - IS
   - ARE

- syntactic recency (è¯­æ³•æ–°è¿‘åº¦) ï¼šThe  <u>writer</u> of the books <u>is</u> ï¼ˆcorrect)
- sequential recency (é¡ºåºæ–°è¿‘åº¦) :The writer of the <u>books</u> <u>are</u> (incorrect)
- RNN æ›´æ“…é•¿å­¦ä¹   sequential recencyï¼Œè€Œä¸æ˜¯ syntactic recency



### 4. Solution

- The main problem = RNN ä¸èƒ½åœ¨é•¿æ—¶é—´å†…ä¿å­˜ä¿¡æ¯ã€‚

  -  **itâ€™s too difficult for the RNN to learn to preserve information over many timesteps.**

- In a vanilla RNN, the hidden state is **constantly being rewritten**
  $$
  h^{(t)} = \sigma\left ( W_hh^{(t-1)} + W_x x^{(t)} + b\right)
  $$

  - æ‰€ä»¥å¾ˆéš¾æŠŠä¿¡æ¯ä»å‰ä¸€ä¸ªhidden state ä¿å­˜åˆ° ä¸‹ä¸€ä¸ªhidden state

- **Idea**ï¼šAdd separate **memory** to  a RNN 

## Exploding gradient

<img src="./rnn/sgd.png" alt="sgd" style="zoom:30%;" />

- å½“gradient å¾ˆå¤§çš„æ—¶å€™ï¼Œæ›´æ–°å˜åŒ–å¾ˆå¤§
- å¯èƒ½å‡ºç° **inf** æˆ–è€… **NaN**

### Solution: Gradient clipping

å½“ gradient > æŸä¸ªthreshold, ä½¿ç”¨ scaleåçš„ gradient å¯¹SGDæ›´æ–°

<img src="./rnn/grad_clipping.png" alt="grad_clipping" style="zoom:50%;" />

**Idea** ä¸æ”¹å˜æ–¹å‘ï¼Œåªæ›´æ–°ä¸€å°æ­¥

<img src="./rnn/grad_clipping2.png" alt="grad_clipping2" style="zoom:50%;" />

1. exploding graident also problem
   1. å®¹æ˜“å‘ç°
   2. solution: gradient clipping

## LSTM

### 1. Structrue

é™¤äº† hidden stateï¼Œç›¸æ¯”RNN å¢åŠ äº† cell stateæ¥å­˜å‚¨é•¿æœŸä¿¡æ¯ï¼›

- On step t, there is a **hidden state** $h^{(t)}$ and a **cell state** $c^{(t)}$ 
  - Both are vectors length n 
  - The cell stores **long-term information**
  - The LSTM can **read**, **erase**, and **write** information from the cell
    -  The cell becomes conceptually rather like RAM in a computer

ä½¿ç”¨3ä¸ª ğŸšª æ¥æ§åˆ¶å¯¹ä¿¡æ¯çš„è¯»ï¼Œæ“¦ï¼Œå†™

- The selection of which information is erased/written/read is controlled by three corresponding **gates**
  - The gates are also vectors length n 
  - On each timestep, each element of the gates can be **open** (1),** closed** (0), or somewhere in-between
  - The gates are **dynamic**: their value is computed based on the current context 

### 2. Equations

<img src="./rnn/lstm2.png" alt="lstm2" style="zoom:50%;" />

é—®é¢˜1: ä¸ºä»€ä¹ˆ forget gate åªä½¿ç”¨ $h^{(t-1)}$ çš„ä¿¡æ¯ï¼Œè€Œä¸ç”¨ $c^{(t-1)}$ï¼Œå³ç›´æ¥çœ‹ä¹‹å‰çš„åŸå§‹æ•°æ®æ¥å†³å®šè¦ä¸è¦èˆå¼ƒï¼›

é—®é¢˜2: ä¸ºä»€ä¹ˆhidden state é‡Œï¼Œè¦å¯¹ $c^{(t)}$ å» $\tanh$ 

### 3. Graph Version



<img src="./rnn/lstm3.png" alt="lstm3" style="zoom:50%;" />

<img src="./rnn/lstm4.png" alt="lstm4" style="zoom:50%;" />

### 4. Why solve vanishing gradients?

- The LSTM architecture makes it easier for the RNN to preserve information over many timesteps
  - In practice, you get about 100 timesteps rather than about 7
- LSTM doesnâ€™t guarantee that there is no vanishing/exploding gradient, but it does provide an easier way for the model to learn long-distance dependencies





- $a^{<t>} = C^{<t>}$ ä¸æˆç«‹ï¼›
- æ›´æ–°$C^{<t>}$ çš„Gatesæœ‰ä¸¤ä¸ªupdate $\Gamma_u$ å’Œforget  $\Gamma_f$  
- å¢åŠ output gate $\Gamma_o$
  - $a^{<t>} = \Gamma_o C^{<t>}$

![lstm](./rnn/lstm.png)

æ³¨æ„åˆ°ï¼Œå½“è®¾ç½®åˆç†çš„ $\Gamma_u, \Gamma_f$åï¼Œä¸Šé¢ä¸€è¡Œï¼š $c^{<0>}$åˆ°$c^{<3>}$ å¯ä»¥å¾ˆå¿«ä¼ é€’ï¼Œä¿è¯äº†LSTMå¯ä»¥é•¿æ—¶é—´å¾ˆå¥½çš„è®°ä½æŸäº›ä¿¡æ¯

### Peephole connection ï¼ˆLSTMçš„å˜å½¢ï¼‰

- Gates also depends on $c^{<t-1>}$



### GRU (Gated recurrent unit) 

### 1. Equations

<img src="/Users/Wei/Documents/NLP/NLP/rnn/gru.png" alt="gru" style="zoom:50%;" />Motivation** cat---> was

= modification of hidden layer in RNN 

+ (+) capture long range connections
+ (+)Helps a lot with  vanishing gradient problem

<img src="./rnn/rnn_unit.png" alt="rnn_unit" style="zoom:50%;" />



å¢åŠ  c = memory cell æ¥å­˜å‚¨ä¿¡æ¯ï¼Œæ¯”å¦‚ catæ˜¯å•æ•°è¿˜æ˜¯å¤æ•°

$c^{<t>}:=$ memory cell value

$a^{<t>}:=$ output activation value

In GRUï¼Œå– $c^{<t>}:= a^{<t>}$ ï¼Œä½†åœ¨LSTMä¸­ä¸åŒã€‚

- $\tilde{C}^{<t>} = \tanh(W_c[c^{<t-1>, x^{<t>}}] + b_c)$
- é—å¿˜é—¨ $\Gamma_u = \sigma()$,between 0 and 1, u: undate ,
  - $\Gamma_u$ æ˜¯å¦è®°ä½æŸä¸ªä¿¡æ¯
- $C^{<t>} = \Gamma_u *\tilde{C}^{<t>}  +(1-\Gamma_u) * C^{<t-1>}$
  - $\Gamma_u = 0$ ä¿æŒæ—§ä¿¡æ¯
    - å› ä¸ºæ˜¯$\sigma()$ï¼Œå½“å†…éƒ¨æ¯”è¾ƒè´Ÿçš„æ—¶å€™ï¼Œä¼šå§‹ç»ˆä¿æŒ
  - $\Gamma_u = 1$ é—å¿˜æ—§ä¿¡æ¯ï¼Œè®°ä½tæ—¶åˆ»çš„æ–°ä¿¡æ¯ï¼›

<img src="./rnn/GRU_unit.png" alt="GRU_unit" style="zoom:50%;" />



=> æ˜¾è‘—å¸®åŠ©vanishing gradient  problem

ã€Qã€‘æ²¡ç†è§£ï¼š$\Gamma_u$ æ¥è¿‘0ï¼Œæ‰€ä»¥ $C^{<t>} \approx C^{<t-1>}$ ä¸å°±æ„å‘³ç€ä¸æ›´æ–°äº†ä¹ˆ

FULL GRU

1. å¢åŠ Gate $\Gamma_r$ å‘Šè¯‰å¦‚æœé€šè¿‡å‰ä¸€ä¸ªæ—¶åˆ»çš„$C^{<t-1>}$è®¡ç®—tæ—¶åˆ»çš„å€™é€‰$\tilde{C}^{<t>}$
2. å…¶ä»–paperçš„notation
   1. h,u,c,h...

<img src="./rnn/gru_full.png" alt="gru_full" style="zoom:50%;" />

## 

#### GRU VS LSTM

- GRU 
  - æ›´ç®€å•ï¼Œå¯ä»¥å»ºæ›´å¤§çš„æ¨¡å‹

- LSTM
  - more powerful, more effective 
  - more proven choice 

- Researchers have proposed many gated RNN variants, but LSTM and GRU are the most widely-used 
- The biggest difference is that GRU is **quicker to compute** and has fewer parameters
- There is no conclusive evidence that one consistently performs better than the other 
- LSTM is a **good default choice** (especially if your data has particularly long dependencies, or you have lots of training data) 
- **Rule of thumb**: start with LSTM, but switch to GRU if you want something more efficient



## Is vanishing/exploding gradient just a RNN problem?

==No!==

- It can be a problem for all neural architectures (including feed-forward and convolutional), especially very deep ones. 
  - Due to chain rule / choice of nonlinearity function, gradient can become vanishingly small as it backpropagates 
  - Thus, lower layers are learned very slowly (hard to train)
- Solution: lots of new deep feedforward/convolutional architectures that **add more direct connections** (thus allowing the gradient to flow) 

For example

- Residual connections aka â€œResNetâ€ 
- Also known as skip-connections
- The identity connection preserves information by default
- This makes deep networks much easier to train

<img src="/Users/Wei/Documents/NLP/NLP/rnn/resnet.png" alt="resnet" style="zoom:50%;" />



- Dense connections aka â€œDenseNetâ€ 

- Directly connect each layer to all future layers!

![dense](/Users/Wei/Documents/NLP/NLP/rnn/dense.png)

- Highway connections aka â€œHighwayNetâ€ 

- Similar to residual connections, but the identity connection vs the transformation layer is controlled by a dynamic gate
- Inspired by LSTMs, but applied to deep feedforward/convolutional networks

<img src="/Users/Wei/Documents/NLP/NLP/rnn/highway.png" alt="highway" style="zoom:50%;" />

**Conclusion**: Though vanishing/exploding gradients are a general problem, **RNNs are particularly** unstable due to the **repeated** multiplication by the **same** weight matrix 

### Bidirectional RNN

### 1. Motivation

#### 1.1 Example 1: He said, "Teddy ..."

- å•å‘ä¸å¤Ÿï¼åªç»™å‰ä¸¤ä¸ªå•è¯ "He said"ï¼Œæ— æ³•åˆ¤æ–­ "Teddy" æ˜¯äººå è¿˜æ˜¯å…¶ä»–
- <img src="./rnn/problem.png" alt="problem" style="zoom:50%;" />

### 1.2 Example 2: terribly exciting 

<img src="/Users/Wei/Documents/NLP/NLP/rnn/bidir_example.png" alt="bidir_example" style="zoom:50%;" />

### Architecture

<img src="/Users/Wei/Documents/NLP/NLP/rnn/bidir_architecture.png" alt="bidir_architecture" style="zoom:50%;" />

<img src="/Users/Wei/Documents/NLP/NLP/rnn/bidir_equ.png" alt="bidir_equ" style="zoom:50%;" />

<img src="./rnn/BRNN.png" alt="BRNN" style="zoom:50%;" />



- (-)éœ€è¦entire sentence 

Note: bidirectional RNNs are only applicable if you have access to the **entire input sequence**

- They are **not** applicable to Language Modeling, because in LM you only have left context available. 
- If you do have entire input sequence (e.g., any kind of encoding), **bidirectionality is powerful** (you should use it by default).

## Deep RNNs (Multi-layer RNNs)

- Multi-layer RNNs are also called **stacked RNNs**
- <img src="/Users/Wei/Documents/NLP/NLP/rnn/multi-layer.png" alt="multi-layer" style="zoom:50%;" />

<img src="./rnn/deepRNN.png" alt="deepRNN" style="zoom:50%;" />

- å¯¹RNNæ¥è¯´ï¼Œ3å±‚å·²ç»å¾ˆæ·±äº†
  - å› ä¸ºæ°´å¹³å·²ç»å¾ˆé•¿äº†
- æ›´å¸¸è§çš„æ˜¯ï¼Œ3å±‚RNNåç»­åœ¨$y^{<t>}$ çš„ä½ç½®åŠ å¤šå±‚çš„NNï¼ˆæ·±ä½†æ²¡æœ‰æ°´å¹³è¿æ¥ï¼‰
- 
- 

Multi-layer RNNs in practice

- **High-performing RNNs are often multi-layer** (but arenâ€™t as deep as convolutional or feed-forward networks) 
- For example: In a 2017 paper, Britz et al find that for Neural Machine Translation, 
  - 2 to 4 layers is best for the encoder RNN, 
  - 4 layers is best for the decoder RNN 
  - Usually, skip-connections/dense-connections are needed to train deeper RNNs (e.g., 8 layers)
