# Dependency Parsing

<img src="/Users/Wei/Library/Application Support/typora-user-images/Screen Shot 2021-07-06 at 10.39.24 AM.png" alt="Screen Shot 2021-07-06 at 10.39.24 AM" style="zoom:50%;" />

# 1. Structure of Sentences

è¯­è¨€çš„ç»“æ„ï¼Œä¸€èˆ¬å¯ä»¥æœ‰ä¸¤ç§è§†è§’ï¼š

1. constituency ç»„æˆå…³ç³»ï¼š
   - ä¸»è¦å…³å¿ƒçš„æ˜¯å¥å­æ˜¯æ€ä¹ˆæ„æˆçš„ï¼Œè¯æ€ä¹ˆç»„æˆçŸ­è¯­ã€‚
   - æ‰€ä»¥ç ”ç©¶Constituencyä¸»è¦æ˜¯ç ”ç©¶å¿½ç•¥è¯­ä¹‰çš„â€œ è¯­æ³•â€ ç»“æ„ï¼ˆcontent-free grammarsï¼‰ ã€‚
2. dependency ä¾èµ–å…³ç³»ã€‚
   - ä¸»è¦å…³å¿ƒçš„æ˜¯å¥å­ä¸­çš„æ¯ä¸€ä¸ªè¯ï¼Œ éƒ½ä¾èµ–äºå“ªä¸ªå…¶ä»–çš„è¯ã€‚

## 1.1 Consistency grammer

> ğŸ’¡ Consisteny 
>
> â€‹       = Phrase structure grammar
>
> â€‹       = context-free grammars(CFGs) 
>
> â€‹      = æŠ“ä½â€œç»“æ„â€ï¼Œä¸ä¸Šä¸‹æ–‡/å•è¯å«ä¹‰æ— å…³

### 1.1.1 ç»“æ„

1. **Starting unit: words**

   - the, cat, cuddly, by, door

   <img src="./dependency_parsing/consistency_step1.png" alt="consistency_step1" style="zoom:50%;" />

2. **Words combine into phrases**

   - the cuddly cat,   by the door

   <img src="./dependency_parsing/consistency_step2.png" alt="consistency_step2" style="zoom:50%;" />

3. **Phrases can combine into bigger phrases**

   - the cuddly cat by the door  

     <img src="./dependency_parsing/consistency_step3.png" alt="consistency_step3" style="zoom:50%;" />

- `Det` æŒ‡çš„æ˜¯ **Determiner**ï¼Œåœ¨è¯­è¨€å­¦ä¸­çš„å«ä¹‰ä¸º **é™å®šè¯**
  
- `NP` æŒ‡çš„æ˜¯ **Noun Phrase** ï¼Œåœ¨è¯­è¨€å­¦ä¸­çš„å«ä¹‰ä¸º **åè¯çŸ­è¯­**

- `VP` æŒ‡çš„æ˜¯ **Verb Phrase** ï¼Œåœ¨è¯­è¨€å­¦ä¸­çš„å«ä¹‰ä¸º **åŠ¨è¯çŸ­è¯­**

- `P` æŒ‡çš„æ˜¯ **Preposition** ï¼Œåœ¨è¯­è¨€å­¦ä¸­çš„å«ä¹‰ä¸º **ä»‹è¯**

- - `PP` æŒ‡çš„æ˜¯ **Prepositional Phrase** ï¼Œåœ¨è¯­è¨€å­¦ä¸­çš„å«ä¹‰ä¸º **ä»‹è¯çŸ­è¯­**

- Rule1: "Noun Phrase (NP)"

  - æ¯”å¦‚ the catï¼Œa dog =>  `NP =  Det + N`
  - å†æ¯”å¦‚ the large cat, a barking dog => `NP = Det + (adj) + N`
  - è¿›ä¸€æ­¥ the large cat in a crate => `NP = Det + (adj) + N + Prep`

- Rule 2: Preposition Phrase 

  - `PP = Prep + NP`

  - => The cat by the large crate on the large table by the door

- Rule 3: `VP = V + PP`

ç±»ä¼¼å¯ä»¥ä¸€å±‚ä¸€å±‚çš„ï¼Œå»ºä¸€ä¸ªé•¿å¥å­.

## 1.2 Denpendency structur

è¿™ç§è§‚ç‚¹åœ¨è®¡ç®—è¯­è¨€å­¦ä¸­å ä¸»å¯¼åœ°ä½ï¼šä¸æ˜¯ä½¿ç”¨å„ç§ç±»å‹çš„çŸ­è¯­ï¼Œè€Œæ˜¯ç›´æ¥é€šè¿‡å•è¯ä¸å…¶ä»–çš„å•è¯å…³ç³»è¡¨ç¤ºå¥å­çš„ç»“æ„ï¼Œæ˜¾ç¤ºå“ªäº›å•è¯ä¾èµ–äº(ä¿®é¥°æˆ–æ˜¯å…¶å‚æ•°)å“ªäº›å…¶ä»–å•è¯

<img src="./dependency_parsing/dependency_structure.png" alt="dependency_structure" style="zoom:50%;" />

- `Look` æ˜¯æ•´ä¸ªå¥å­çš„ root, ä¾èµ–äº `dog` ï¼ˆæˆ–è€…è¯´ï¼Œ`dog` æ˜¯ `Look` çš„ä¾èµ–ï¼‰
- `for the large barking`  æ˜¯ `dog`çš„ä¿®é¥°[Question: ä¿®é¥° vs ä¾èµ–]
- `by`, `the` éƒ½æ˜¯ `door`çš„ä¾èµ–
- `by the door`æ˜¯ `dog` çš„ä¾èµ–

## 1.3 Why do we need sentence structure?

- We need to understand sentence structure in order to be able to interpret language correctly

  - ä¸ºäº†èƒ½å¤Ÿæ­£ç¡®åœ°è§£é‡Šè¯­è¨€ï¼Œæˆ‘ä»¬éœ€è¦ç†è§£å¥å­ç»“æ„

- Humans communicate complex ideas by composing words together into bigger units to convey complex meanings

  - äººç±»é€šè¿‡å°†å•è¯ç»„åˆæˆæ›´å¤§çš„å•å…ƒæ¥ä¼ è¾¾å¤æ‚çš„æ„æ€ï¼Œä»è€Œäº¤æµå¤æ‚çš„æ€æƒ³

- We need to know what is connected to what

  - æˆ‘ä»¬éœ€è¦çŸ¥é“ä»€ä¹ˆä¸ä»€ä¹ˆç›¸å…³è”

- - é™¤éæˆ‘ä»¬çŸ¥é“å“ªäº›è¯æ˜¯å…¶ä»–è¯çš„å‚æ•°æˆ–ä¿®é¥°è¯ï¼Œå¦åˆ™æˆ‘ä»¬æ— æ³•å¼„æ¸…æ¥šå¥å­æ˜¯ä»€ä¹ˆæ„æ€

    

## 1.4 Ambiguities

### 1.4.1 Prepositional phrase attachment ambiguity ä»‹è¯çŸ­è¯­ä¾é™„æ­§ä¹‰

#### ä¾‹å­1 ğŸŒ° 

San Jose cops kill man with knife.

- `cops` æ˜¯`kill` çš„ subject;
- `man` æ˜¯ `kill` çš„ object

<img src="./dependency_parsing/pp_amb1.png" alt="pp_amb1" style="zoom:50%;" />

**ç†è§£1** è­¦å¯Ÿç”¨åˆ€æ€äº†é‚£ä¸ªç”·å­

- `knife`æ˜¯ `kill` çš„ modifier(ä¿®é¥°ç¬¦-> åç§°ä¿®é¥°ç¬¦ï¼Œnmod) 
- å¦‚ä¸Šå›¾ç»¿è‰²

**ç†è§£2**  è­¦å¯Ÿæ€äº†é‚£ä¸ªæœ‰åˆ€çš„ç”·å­

- `knife`æ˜¯ `man` çš„ modifier(ä¿®é¥°ç¬¦) 

#### ä¾‹å­2 ğŸŒ° 

Scientists count whales from space

- `from space` è¿™ä¸€ä»‹è¯çŸ­è¯­ä¿®é¥°çš„æ˜¯å‰é¢çš„åŠ¨è¯ `count` è¿˜æ˜¯åè¯ `whales` ï¼Ÿ 

<img src="./dependency_parsing/pp_amb2.png" alt="pp_amb2" style="zoom:50%;" />



> A key parsing decision is **how we 'attach' various counstituents**
>
> - PPs, adverbial or participial phrases, infinitives, coordinations

#### ä¾‹å­3 ğŸŒ°

<img src="./dependency_parsing/pp_amb3.png" alt="pp_amb3" style="zoom:50%;" />

- ä¸Šè¿°å¥å­ä¸­æœ‰å››ä¸ªä»‹è¯çŸ­è¯­
- `board` æ˜¯ `approved` çš„ ä¸»è¯­ï¼Œ`acquisition` æ˜¯ `approved` çš„å®¾è¯­
- `by Royal Trustco Ltd.` æ˜¯ä¿®é¥° `acquisition` çš„ï¼Œå³è‘£äº‹ä¼šæ‰¹å‡†äº†è¿™å®¶å…¬å¸çš„æ”¶è´­
- `of Toronto` å¯ä»¥ä¿®é¥° `approved, acquisition, Royal Trustco Ltd.` ä¹‹ä¸€ï¼Œç»è¿‡åˆ†æå¯ä»¥å¾—çŸ¥æ˜¯ä¿®é¥° `Royal Trustco Ltd.` å³è¡¨ç¤ºè¿™å®¶å…¬å¸çš„ä½ç½®
- `for $27 a share` ä¿®é¥° `acquisition`
- `at its monthly meeting` ä¿®é¥° `approved` ï¼Œå³è¡¨ç¤ºæ‰¹å‡†çš„æ—¶é—´åœ°ç‚¹



é¢å¯¹è¿™æ ·å¤æ‚çš„å¥å­ç»“æ„ï¼Œæˆ‘ä»¬éœ€è¦è€ƒè™‘ **æŒ‡æ•°çº§** çš„å¯èƒ½ç»“æ„ï¼Œè¿™ä¸ªåºåˆ—è¢«ç§°ä¸º **Catalan numbers**

**Catalan numbers : ** $C_n = (2n)!/[(n+1)!n!]$ æŒ‡æ•°å¢é•¿çš„åºåˆ—

### 1.4.2 Coordination scope ambiguity åè°ƒèŒƒå›´æ¨¡ç³Š

#### 1.4.2.1 ä¾‹å­1 ğŸŒ°

<img src="./dependency_parsing/cor_amb1.png" alt="cor_amb1" style="zoom:50%;" />

- [(Shuttle veteran and longtime NASA executive) Fred Gregory] appointed to board.
- (Shuttle veteran) and (longtime NASA executive Fred Gregory) appointed to board.

#### 1.4.2.2 ä¾‹å­2

<img src="./dependency_parsing/cor_amb2.png" alt="cor_amb2" style="zoom:50%;" />

- Doctor: [No heart], [cognitive issues]
  - `,` è¡¨è¾¾ and çš„å«ä¹‰
  - [No heart] and [cognitive issues]

- Doctor: No [[heart, cognitive] issues]
  - `,` è¡¨è¾¾ or çš„å«ä¹‰
  - No [ [heart or cognitive] issues]



### 1.4.3  Adjectival Modifier Ambiguity å½¢å®¹è¯ä¿®é¥°è¯­æ­§ä¹‰

<img src="./dependency_parsing/adj_amb1.png" alt="adj_amb1" style="zoom:50%;" />

- Students get [[first hand] job experience]
  - `first hand` ç¬¬ä¸€æ‰‹çš„ï¼Œç›´æ¥çš„
  - å­¦ç”Ÿè·å¾—äº†ç›´æ¥çš„å·¥ä½œç»éªŒ
- Students get [first [hand job] experience]
  - hand job ...



### 1.4.4 Verb Phrase(VP) attachment ambiguity åŠ¨è¯çŸ­è¯­ä¾å­˜æ­§ä¹‰

<img src="./dependency_parsing/vb_amb.png" alt="vb_amb" style="zoom:50%;" />

- `to be used for Olympic beach volleyball` æ˜¯ åŠ¨è¯çŸ­è¯­ (VP)
- ä¿®é¥°çš„æ˜¯ `body` è¿˜æ˜¯ `beach`



### 1.4.5 Solution :  Dependency paths

> Dependency paths help extract semantic interpretation

ä¾‹å¥ğŸŒ° The results demonstrated that KaiC interacts rhythmically with KaiA, KaiB, and SasA.

<img src="./dependency_parsing/dependency_paths.png" alt="dependency_paths" style="zoom:50%;" />



# 2. Dependency Grammar and Dependency Structure

å…³è”è¯­æ³•å‡è®¾å¥æ³•ç»“æ„åŒ…æ‹¬è¯æ±‡é¡¹ä¹‹é—´çš„å…³ç³»ï¼Œé€šå¸¸æ˜¯äºŒå…ƒä¸å¯¹ç§°å…³ç³»(â€œç®­å¤´â€)ï¼Œç§°ä¸ºä¾èµ–å…³ç³»

## 2.1 Dependency Structure çš„ä¸¤ç§è¡¨ç°å½¢å¼

### 2.1.1 ç›´æ¥åœ¨å¥å­ä¸Šæ ‡å‡ºä¾å­˜å…³ç³»ç®­å¤´åŠè¯­æ³•å…³ç³»

<img src="./dependency_parsing/representation1.png" style="zoom:50%;" />

- Pro åŸå§‹å¥å­å†…å®¹æ¸…æ¥š

- Con å…³ç³»ç»“æ„ä¸å¤Ÿæ¸…æ¥š

### 2.1.2 Dependence Tree Graph

<img src="./dependency_parsing/representation2.png" style="zoom:50%;" />

- Pro å…³ç³»ç»“æ„æ¸…æ¥š
- Con åŸå§‹å¥å­å†…å®¹ï¼ˆé¡ºåºï¼‰ä¸æ¸…æ¥š

**å›¾é‡Š**

- ç®­å¤´ä¸Šé€šå¸¸ä¼šæ ‡è®°ï¼ˆ**type**ï¼‰è¯­æ³•å…³ç³»ï¼Œæ¯”å¦‚ subjectã€prepositional objectã€appositionç­‰ã€‚
  - è¯¾ç¨‹ä¸­åªç”¨arrowï¼Œä¸ç”¨typeï¼ˆnsubj,etc)
- å…³ç³»ï¼š
  - the arrow connects a ***head*** (governor,superior, regent) with a ***dependent*** (modifier, inferior, subordinate)
    - A $\rightarrow$ ä¾èµ–äº/ä¿®é¥° Açš„éƒ¨åˆ†
  - dependencies form a ***tree***(connected, acyclic, single-head)
    - è¿é€šï¼Œæ— ç¯ï¼Œå•å‘
- ä¾èµ–å…³ç³»æ ‡ç­¾çš„ç³»ç»Ÿï¼Œä¾‹å¦‚ **universal dependency** é€šç”¨ä¾èµ–



### 2.1.3 ä¾‹å­å’Œæ³¨æ„

- <img src="./dependency_parsing/example.png" alt="example" style="zoom:50%;" />ç®­å¤´çš„æ–¹å‘ä¸ç»Ÿä¸€ï¼Œä¸åŒpaperå¯èƒ½ä¸ä¸€è‡´ï¼›
- é€šå¸¸ï¼Œæ·»åŠ ä¼ªæ ¹èŠ‚ç‚¹ `ROOT`  æŒ‡å‘æ•´ä¸ªå¥å­çš„å¤´éƒ¨ï¼Œè¿™æ ·ï¼Œæ¯ä¸ªå•è¯éƒ½ç²¾ç¡®åœ°ä¾èµ–äºå¦ä¸€ä¸ªèŠ‚ç‚¹

## 2.2 Universal Dependencies treebanks

ref [universal dependencies](https://universaldependencies.org/)

### 2.2.1 ä¾‹å­ğŸŒ°

<img src="./dependency_parsing/treebank.png" alt="treebank" style="zoom:50%;" />

### 2.2.2 ç›®æ ‡ä¸ä¼˜ç¼ºç‚¹

- **ğŸ¯goal of  "universal dependency"**ï¼š

  - have a uniform parallel system of dependency description which could be used for any human language 

- **Cons**

  - å¼€å§‹æ—¶å€™ï¼Œæ„å»º treebank ä¼¼ä¹æ¯”æ„å»ºè¯­æ³•è¦æ…¢çš„å¤šï¼Œä¹Ÿæ²¡æœ‰é‚£ä¹ˆæœ‰ç”¨
    - è¯­æ³•å¯ä»¥ä¸€æ¡è§„åˆ™æ•æ‰å¾ˆå¤šä¸œè¥¿ï¼Œéå¸¸æ•ˆç‡
    - ä½†æ˜¯ï¼Œåœ¨å®è·µä¸­å¹¶ä¸å¥½ç”¨ï¼šè¯­æ³•è§„åˆ™ç¬¦å·è¶Šæ¥è¶Šå¤æ‚ï¼Œå¹¶ä¸”æ²¡æœ‰å…±äº«å’Œé‡ç”¨äººç±»æ‰€åšçš„å·¥ä½œ

- **Pros**

  - åŠ³åŠ¨åŠ›çš„å¯é‡ç”¨æ€§

  - - è®¸å¤šè§£æå™¨ã€è¯æ€§æ ‡è®°å™¨ç­‰å¯ä»¥æ„å»ºåœ¨å®ƒä¹‹ä¸Š
    - è¯­è¨€å­¦çš„å®è´µèµ„æº

  - å¹¿æ³›çš„è¦†ç›–é¢ï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸€äº›ç›´è§‰

  - é¢‘ç‡å’Œåˆ†å¸ƒä¿¡æ¯ï¼ˆFrequencies and distributional informationï¼‰ï¼š

    - å› ä¸ºMLæ¨¡å‹å°±æ˜¯å­¦ä¹ è¿™ç§commoners and the frequency of things 

  - ä¸€ç§è¯„ä¼°ç³»ç»Ÿçš„æ–¹æ³•

  - ä¸ â€œGrammarâ€ç›¸æ¯”çš„å¥½å¤„ï¼štelling what is the right structure for ambiguous sentences

  

- æ‰€ä»¥æˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ¨¡å‹æ¥capture what is the ***right parse***

### 2.3 Dependency parsing

### 2.3.1 Dependency parsingçš„éœ€è¦è€ƒè™‘å“ªäº›ä¿¡æ¯

1. **Bilexical affinities** = wordçš„å«ä¹‰
   1. ğŸŒ° [discussion $\rightarrow$ issues] çœ‹ä¸Šå»æ˜¯åˆç†çš„
   2.  [discussion $\rightarrow$ outstanding] çœ‹ä¸Šå»æ˜¯wierdçš„ï¼Œæ‰€ä»¥ä¸åº”è¯¥æœ‰è¿™ä¸ªä¾èµ–æ€§ ï¼ˆä¸‹å›¾ç»¿è‰²xï¼‰
2. **Dependency distance** å¤§éƒ¨åˆ†çš„ä¾èµ–å‘ç”Ÿåœ¨ç›¸é‚»è¯ä¹‹é—´
3. **Intervening material** ä¾èµ–å¾ˆå°‘è·¨è¶Šä»‹äºä¸­é—´çš„åŠ¨è¯æˆ–æ ‡ç‚¹ç¬¦å·
4. **Valency of heads**ï¼šHow many dependents on which side are usual for a head?

<img src="./dependency_parsing/dependency_preference.png" style="zoom:50%;" />

### 2.3.2 Dependency Parsingçš„æ„é€ /ç»“æ„

- A sentence is parsed by choosing for each word what other word (including ROOT) is it a dependent of.
- ä¸€äº›é™åˆ¶ => æ»¡è¶³ä¸‹é¢çš„æ¡ä»¶ä½¿å¾—å¯ä»¥æ„å»ºæˆæ ‘ç»“æ„
  - ROOT åªæœ‰ä¸€ä¸ªdependent
  - æ— ç¯

- ç®­å¤´æ˜¯å¦èƒ½ç›¸äº¤
  - æœ‰crossing dependencies => non- projective
    - [give $\rightarrow$ tomorrow] å’Œ [talk $\rightarrow$ network] ä¸¤ä¸ªç®­å¤´æœ‰ç›¸äº¤<img src="./dependency_parsing/non-projective.png" alt="non-projective" style="zoom:50%;" />
    - è‹±è¯­***å¤§éƒ¨åˆ†***æƒ…å†µä¸‹æ˜¯projectiveçš„

### 2.3.3 Method : Transition-based parsing 

> ğŸ’¡  **state machine** which defines the possible transitions to create the mapping from the input sentence to the dependency tree

#### 2.3.3.1 çŠ¶æ€ State 

**ç»“æ„**

- a stack $\sigma$, written with top to the right 
  - which starts with the ROOT symbol 
- a buffer $\beta$, written with top to the left 
  - which starts with the input sentence 
- a set of dependency arcs A 
  - which starts off empty 

**çŠ¶æ€** å¯¹ä»»æ„çš„å¥å­ $S = w_0w_1...w_n$ï¼Œä¸€ä¸ªçŠ¶æ€å¯ä»¥è¡¨è¿°ä¸ºä¸€ä¸ªä¸‰å…ƒç»„ $c = (\sigma, \beta, A)$

- a stack $\sigma$ï¼ŒåŒ…å« S ä¸­çš„å•è¯ä»¬ $w_i$ 
- a buffer $\beta$ï¼ŒåŒ…å« S ä¸­çš„å•è¯ä»¬ $w_i$
- a set of dependency arcs A 
  - å½¢å¼ï¼š$(w_i, r, w_j)$ å…¶ä¸­ $w_i$, $w_j$ æ¥è‡ªSï¼Œræè¿°äº†ä¾å­˜å…³ç³»ï¼ˆdependency relationï¼‰
- Remark
  - åˆå§‹çŠ¶æ€ $c_0 = ([w_0]_{\sigma}, [w_1,...,w_n]_{\beta}, \varnothing)$ï¼šåªæœ‰ `ROOT`  åœ¨ $\sigma$ ä¸­ï¼Œå…¶ä»–å•è¯éƒ½åœ¨ $\beta$ ä¸­ï¼›
  - ç»ˆæ­¢çŠ¶æ€ $(\sigma, []_{\beta}, A))$

#### 2.3.3.2 è½¬åŒ– Transitions

- `Shift` æŠŠbufferä¸­ç¬¬ä¸€ä¸ªå•è¯ï¼Œpushåˆ°$\sigma$ çš„æ ˆé¡¶
- `Left-Arc` ä¸­ Aä¸­æ·»åŠ dependency arc $(w_j, r, w_i)$ ï¼Œç„¶åä» $\sigma$ é‡Œå»æ‰ $w_i$
  - $w_i$ = the word second to the top of the stack
  - $w_j$ = the word at the top of the stack
- `Right-Arc` ä¸­ Aä¸­æ·»åŠ dependency arc $(w_i, r, w_j)$ ï¼Œç„¶åä» $\sigma$ é‡Œå»æ‰ $w_j$
  - $w_i$ = the word second to the top of the stack
  - $w_j$ = the word at the top of the stack

- <img src="./dependency_parsing/Basic_transition-based_dependency_parser.png" alt="Basic_transition-based_dependency_parser" style="zoom:50%;" />

#### 2.3.3.3 ä¾‹å­ğŸŒ° ï¼š Arc-standard transition-based parser

**åˆ†æ "I ata fish"**

- é»‘æ¡† stackï¼šå¤„ç†è¿‡çš„å•è¯ï¼Œèµ·å§‹çŠ¶æ€æ˜¯ [root]
- æ©™æ¡† bufferï¼šå¾…å¤„ç†çš„å•è¯ï¼Œèµ·å§‹çŠ¶æ€æ˜¯æ•´ä¸ªå¥å­ï¼Œç»ˆæ­¢çŠ¶æ€æ˜¯ [ç©º]

<img src="./dependency_parsing/Transition_based_parsing1.png" alt="Transition-based parsing1" style="zoom:50%;" />

- ç¬¬äºŒæ­¥ä¸­ï¼Œå› ä¸º `[root]` $\rightarrow$ `I` æ˜¯é”™è¯¯çš„dependencyï¼Œæ‰€ä»¥ä¸‹ä¸€æ­¥ç»§ç»­shift `ate` è¿›stack
- ç¬¬ä¸‰æ­¥ä¸­ï¼Œå› ä¸ºæœ‰ `I` $\leftarrow$ `ate` çš„dependencyï¼Œæ‰€ä»¥ä¸‹ä¸€æ­¥åšLeft Arc

<img src="./dependency_parsing/Transition_based_parsing2.png" alt="Transition-based parsing2" style="zoom:50%;" />

 

- ç¬¬äºŒæ­¥å¾—åˆ°ç»“æœä¸­ï¼Œæœ‰ `ate` $\rightarrow$ `fish`çš„dependencyï¼Œæ‰€ä»¥ä¸‹ä¸€æ­¥åšRight Arc



#### 2.3.4 å¦‚ä½•å†³å®šæ¯ä¸€æ­¥æ­£ç¡®çš„action (shift, left arc, right arc, etc)

1. éå†æ‰€æœ‰å¯èƒ½ ï¼ˆæŒ‡æ•°çº§ï¼‰

2. dynamic programmingï¼ˆè¿‡å»ä½¿ç”¨çš„æ–¹æ³•ï¼‰$\geq O(n^3)$

3. MaltParser ç”¨ML classifieré¢„æµ‹ä¸‹ä¸€æ­¥çš„action $O(n)$
   
4. NN 

   

## 2.4 MaltParser

### 2.4.1 ä»‹ç»

- Each action is predicted by a **discrimnatvie classifier** (e.g. softmax classifier) over each legal move
  - Max of 3 uptyped choices (shift, left arc, right arc);
  - Max of |R| X 2 + 1 when typed
    - put labels on the dependencies (label: subject, object, etc)
    - |R| different labels
    - ğŸŒ° (left arc + subject)ï¼Œï¼ˆright arc + object)
  - Features: top of stack word, POS; first in buffer word, POS; etc
- There is NO search (in the simplest form)
  - But you can profitably do a beam search if you wish (slower but better)
    - You keep k good parse prefixes at each time step 
- The modelâ€™s accuracy is fractionally below the state of the art in dependency parsing, but 
- ã€ä¼˜ç‚¹ï¼šå¿«ã€‘It provides **very fast linear time parsing**, with high accuracy â€“ great for parsing the web

### 2.4.2 Conventional Feature Representation

[TODO]

<img src="./dependency_parsing/conventioalfeaturerepresentation.png" alt="conventioalfeaturerepresentation" style="zoom:50%;" />

- logistic regression, SVM ç­‰ç®—æ³•å·²ç»å¯ä»¥åšçš„ä¸é”™
- 

## 2.5 Neural dependency parsing

### 2.5.1 Why train a neural dependency parserï¼Ÿ

- **Problem 1**: åœ¨conversionalé‡Œï¼Œfeatures are very sparse
- **Problem 2**: incomplete

- **Problem 1**: expensive computation
  - è¶…è¿‡95%çš„æ—¶é—´éƒ½æ˜¯ç”¨æ¥åšfeature computation

Neural Approach: learn a dense and compact feature representation

### 2.5.2 A Neural dependency parser

>  ğŸ¯ predict a **transition sequence** from some initial configuration c to a terminal configuration, in which the dependency parse tree is encoded

#### 2.5.2.1 Results

<img src="./dependency_parsing/neural_parser.png" alt="neural_parser" style="zoom:50%;" />

- NN çš„æ–¹æ³•ï¼ˆC & M 2014ï¼‰åœ¨ä¿æŒç²¾åº¦çš„æƒ…å†µä¸‹ï¼Œè¿ç®—é€Ÿåº¦éå¸¸å¿«ã€‚

#### 2.5.2.2 Model Architecture

##### a. Feature Selection

å¯¹äºç»™å®šå¥å­ S ,å®ƒçš„ç‰¹å¾é€šå¸¸åŒ…æ‹¬ä¸‹é¢çš„å­é›†

- $S_{word}$ = Vector representations for some of the words in S (and their dependents) at the top of the stack $\sigma$ and buffer $\beta$.
  - ç›¸ä¼¼å•è¯æœ‰ç›¸ä¼¼çš„å‘é‡
- $S_{tag}$ ï¼ˆè¯æ€§ï¼‰= Part-of-Speech (POS) tags for some of the words in S. 
  - POS tags comprise a small, discrete set
  - P = {NN, NNP, NNS, DT, JJ, ...}

- $S_{label}$ = The arc-labels for some of the words in S. 
  - The arc-labels comprise a small, discrete set
  - describing the dependency relation
  - L = {amod, tmod, nsubj, csubj, dobj, ...}

**æ³¨æ„**âš ï¸

1. POS å’Œ dependency labels ä¹Ÿæœ‰â€œç›¸ä¼¼æ€§â€
   - ğŸŒ° **NNS**(å¤æ•°åè¯)åº”è¯¥æ¥è¿‘**NN**(å•æ•°åè¯)
   - ğŸŒ° **num**(æ•°å€¼ä¿®é¥°è¯­)åº”è¯¥æ¥è¿‘**amod**(å½¢å®¹è¯ä¿®é¥°è¯­)
2. ä¸‰è€…éƒ½æ˜¯distributed representations

- ç»¼åˆä¹‹å

  <img src="./dependency_parsing/distributed_representations.png" alt="distributed_representations" style="zoom:50%;" />

æˆ‘ä»¬å°†å…¶è½¬æ¢ä¸ºè¯å‘é‡å¹¶å°†å®ƒä»¬è”ç»“èµ·æ¥ä½œä¸ºè¾“å…¥å±‚ï¼Œå†ç»è¿‡è‹¥å¹²éçº¿æ€§çš„éšè—å±‚ï¼Œæœ€ååŠ å…¥softmax layerå¾—åˆ°shift-reduceè§£æå™¨çš„åŠ¨ä½œ

#### b. Feature Selection Example

è€ƒè™‘å¯¹$S_{word}$ï¼Œ$S_{tag}$ï¼Œ$S_{label}$ çš„é€‰æ‹©ï¼š

1. $S_{word}$ 
   - $\sigma$ å’Œ $\beta$ å‰ä¸‰ä¸ªå•è¯ï¼š$s_1,s_2,s_3,b_1,b_2,b_3$;
   - $\sigma$ å‰ä¸¤ä¸ªå•è¯çš„å‰ä¸¤ä¸ª leftmost / rightmost å­å•è¯ï¼š$ lc_1(s_i), rc_1(s_i), lc_2(s_i), rc_2(s_i)$, $i = 1, 2$.
   - $\sigma$ å‰ä¸¤ä¸ªå•è¯çš„ leftmost of leftmost / rightmost of rightmost å­å•è¯ï¼š$ lc_1(lc_1(s_i)), rc_1(rc_1(s_i))$, $i = 1, 2$.
   - $n_{w} = 18$
2. $S_{tag}$
   - å¯¹åº”çš„è¯æ€§æ ‡æ³¨
   - $n_{t} = 18$
3. $S_{label}$
   - å¯¹åº”çš„arc labelï¼Œä½†é™¤å»åœ¨ $\sigma$ å’Œ $\beta$ çš„6æ­Œå•è¯
   - $n_{l} = 12$

- `NULL` token = ä¸å­˜åœ¨çš„å…ƒç´ ï¼šå½“stack å’Œ buffer ä¸ºç©ºï¼Œæˆ–è€… ä¾å­˜å…³ç³»è¿˜æœªè¢«æŒ‡å®šã€‚

#### c. Model Arcgutecture

<img src="./dependency_parsing/model_architecture.png" style="zoom:50%;" />

- The hidden layer re-represents the input
  - it moves inputs around in an intermediate layer vector space
  - so it can be easily classified with a (linear) softmax
- cross-entropy error will be back-propagated to the embeddings

- å›¾ä¸­ä½¿ç”¨çš„non-linear function æ˜¯ $f(x) = x^3$.(hidden layer)

### 2.5.3 Graph-based dependency parsers [TODO]

- Compute a score for every possible dependency for each word

  - Doing this well requires good â€œcontextualâ€ representations of each word token, which we will develop in coming lectures

    <img src="./dependency_parsing/graph_based1.png" alt="graph_based1" style="zoom:50%;" />

  - And repeat the same process for each other word

    <img src="./dependency_parsing/graph_based2.png" alt="graph_based2" style="zoom:50%;" />

- [Dozat and Manning 2017; Dozat, Qi, and Manning 2017]

- This paper revived interest in graph-based dependency parsing in a neural world 

  - Designed a biaffine scoring model for neural dependency parsing â€¢
  - Also crucially uses a neural sequence model, something we discuss next week

- Great results **but slower than the simple neural transition-based parsers**
  - â€¢ There are $n^2$ possible dependencies in a sentence of length n

  <img src="./dependency_parsing/res.png" alt="res" style="zoom:50%;" />

## 2.6 Evaluation

 <img src="./dependency_parsing/evaluation1.png" alt="evaluation1" style="zoom:50%;" />

- UAS (unlabeled attachment score) = å¿½ç•¥label (nsubj, root, etc)ï¼Œåªçœ‹arcçš„æ­£ç¡®ç‡ 

- LAS = åŒ…æ‹¬label+arc çš„æ­£ç¡®ç‡









