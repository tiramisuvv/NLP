# Transformers

[toc]

# 1. Motivation 

- From recurrence(RNN) to attention-based NLP models

## 1.1 Recall 

- encoder-decoder models
- encoder: encode sentences with a bidirectional LSTM
- decoder: an LSTM to generate the output
- **Use attention to allow flexible access to memory**

=> the best **building blocks** to plug into our models and enable broad progress.

<img src="/Users/Wei/Documents/NLP/NLP/transformer/rnn2transformer.png" alt="rnn2transformer" style="zoom:50%;" />

## 1.2 Issues with recurrent models

### 1.2.1 Linear interaction distance

- RNNs are unrolled â€œleft-to-rightâ€.
- **Problem**: RNNs take **O(sequence length)** steps for distant word pairs to interact.
  - ä» **chef** åˆ° **was**
  - <img src="/Users/Wei/Documents/NLP/NLP/transformer/rnn_issue.png" alt="rnn_issue" style="zoom:50%;" />
- **O(sequence length)** steps for distant word pairs to interact means:
  - Hard to learn long-distance dependencies (because gradient problems!) 
  - Linear order of words is â€œbaked inâ€; we already know linear order isnâ€™t the right way to think about sentencesâ€¦
  - ä¸Šé¢ä¾‹å­é‡Œï¼Œ**chef** çš„ä¿¡æ¯åœ¨ **was** ä¹‹å‰å°±å·²ç»æ¶ˆå¤±æ®†å°½äº†ã€‚

### 1.2.2 Lack of parallelizability

- RNN Forward and backward passes have **O(sequence length)** unparallelizable operations
- <img src="/Users/Wei/Documents/NLP/NLP/transformer/rnn_issue2.png" alt="rnn_issue2" style="zoom:50%;" />

## 1.3 Word windows

### 1.3.1 Word window models aggregate local contexts

- Also known as 1D convolution
- Number of unparallelizable operations does not increase sequence length!

<img src="/Users/Wei/Documents/NLP/NLP/transformer/word_window1.png" alt="word_window1" style="zoom:50%;" />

### 1.3.2 Long-distance dependency

- é€šè¿‡ **Stacking word window layes** å¢åŠ  interaction between farther words
- ä½†æœ‰æœ€å¤§é™åˆ¶
  - Maximum Interaction distance = **sequence length / window size**
- if your sequences are too long, youâ€™ll just ignore long-distance context
- <img src="/Users/Wei/Documents/NLP/NLP/transformer/word_window2.png" alt="word_window2" style="zoom:50%;" />
  - $h_k$ å¯ä»¥â€˜çœ‹åˆ°â€™æ ‡çº¢çš„çŠ¶æ€	
  - è¶…å‡ºèŒƒå›´çš„ï¼ˆ$h_1$â€‹ï¼‰å°±ä¸è¡Œäº†

## 1.4 Attention

### 1.4.1 Recall

- **Attention** treats each wordâ€™s representation as a **query** to access and incorporate information from **a set of values**.

- ä¹‹å‰ç”¨åˆ°attention from the decoder to the encoder; 

  - on each step of the decoder, **use direct connection to the encoder** to **focus on a particular part** of the source sequence
  - 

- <img src="/Users/Wei/Documents/NLP/NLP/transformer/seq2seq_attention.png" alt="seq2seq_attention" style="zoom:50%;" />

  - **attention output** 

    - Use the **attention distribution** to take a weighted sum of the encoder hidden states.

      - $$
        a_t = \sum_{i=1}^{N}\alpha_i^th_i \in \mathbb{R}^h
        $$

        å…¶ä¸­ $\alpha_i^t$â€‹  æ˜¯ attention distribution

    - The **attention output** mostly contains information from the hidden states that received high attention.

  - ç”¨ $[a_t; s_t] \in \mathbb{R}^{2h}$â€‹â€‹ æ¥ç”Ÿæˆ  $\hat{y}_1$â€‹, å…¶ä¸­ $s_t$â€‹â€‹ æ˜¯decoder hidden state.

- today weâ€™ll think about attention **within a single sentence**. (self-attention)

### 1.4.2 More properties 

- Number of unparallelizable operations does not increase sequence length.

- Maximum interaction distance: O(1), since all words interact at every layer!

  <img src="/Users/Wei/Documents/NLP/NLP/transformer/attention_prop.png" alt="attention_prop" style="zoom:50%;" /> 

**Idea** ğŸ’¡ ç”¨ self-attention æ¥å¡«å…… encoder å’Œ decoder çš„block.

# 2. Architecture

## 2.1 High Level 

- encoder  å’Œ decoder  éƒ½æ˜¯ç”± N = 6 ä¸ªidentical layer stack åœ¨ä¸€èµ·å¾—åˆ°çš„

<img src="/Users/Wei/Documents/NLP/NLP/transformer/architecture1.png" alt="architecture1" style="zoom:50%;" />

- encoder æ¯ä¸ªlayer åŒ…å« ä¸¤ä¸ª sub-layer :

  - multi-head self-attention 
  - A simple, position- wise fully connected feed-forward network

- decoder æ¯ä¸ªlayer åŒ…å« ä¸‰ä¸ª sub-layer :

  - multi-head self-attention 
  - Encoder- Decoder Attention
    - similar what attention does in seq2seq model 
  - A simple, position- wise fully connected feed-forward network

  <img src="/Users/Wei/Documents/NLP/NLP/transformer/encoder&decoder.png" alt="encoder&decoder" style="zoom:50%;" />

- é™¤äº†ä¸Šé¢ä¸¤å±‚ï¼Œè¿˜å¢åŠ äº† **residual connections** around each of the sub-layers, followed by layer **normalization**.

<img src="/Users/Wei/Documents/NLP/NLP/transformer/architecture2.jpeg" alt="architecture2" style="zoom:100%;" />

## 2.1 Self-Attention (1, 2, 3)

### 2.1.1 Self-Attention

#### 2.1.1.1 åŸç†

- Attention operates on **queries**, **keys**, and **values**.

- Self-Attention æŠŠä¸€ä¸ªå•è¯çš„åµŒå…¥å‘é‡æ˜ å°„æˆä¸‰ä¸ªç›¸åŒç»´åº¦çš„æ–°å‘é‡ï¼š**æŸ¥è¯¢ Query**ã€**é”® Key**ã€**å€¼ Value**

- **Focus on word $x_i$: ** 

  - Step 1. Compute **key-query** affinities (dot-product)

    - 
      $$
      e_{ij} = q_i^Tk_j
      $$

    -  å½“å¤„ç†æŸä¸€ä¸ªå•è¯æ—¶ï¼Œè¿™ä¸ªå•è¯å°±ä¼šç”¨è‡ªå·±çš„ **Query** ($q_i$) è¿›è¡ŒæŸ¥è¯¢ï¼Œè·Ÿå½“å‰åºåˆ—ä¸­ï¼Œæ‰€æœ‰å•è¯çš„é”® **Key** è¿›è¡Œç›¸ä¼¼åº¦æ¯”å¯¹ï¼Œå¾—åˆ°ç›¸ä¼¼åº¦å‘é‡ã€‚è¿™ä¸ªå‘é‡çš„ç»´åº¦å’Œå½“å‰å¥å­é‡Œçš„å•è¯æ•°é‡ç›¸ç­‰

  - Step 2. Compute attention weights from affinities (softmat)

    - $$
      \alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{j'}\exp(e_{ij'})}
      $$

    - æŠŠä¸Šä¸€æ­¥çš„value é€šè¿‡ softmax è½¬åŒ–æˆ probabilityã€‚æ¯ä¸€ä¸ªå…ƒç´ å°±ä»£è¡¨å¯¹åº”ä½ç½®çš„å•è¯å’Œå‘å‡º Query çš„å•è¯ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œå€¼è¶Šå¤§ï¼Œç›¸ä¼¼åº¦è¶Šé«˜

  - Step 3 Compute outputs as weighted sum of **values** 

    - $$
      \text{output}_i = \sum_{j}\alpha_{ij}v_j
      $$

    - ç”¨ç›¸ä¼¼åº¦å‘é‡å¯¹æ¯ä¸ªå•è¯çš„å€¼ **Value** è¿›è¡ŒåŠ æƒæ±‚å’Œäº†ï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ä¸ªæ–°çš„å‘é‡ï¼Œè¿™ä¸ªå‘é‡å°±èå…¥äº†ç›¸å…³å•è¯çš„ä¿¡æ¯

- Matrix version

  - Let $x_1,...,x_T$â€‹ be input vectors to the Transformer encoder;  $x_i \in \mathbb{R}^d$â€‹

  - $X = [x_1; ...; x_T] \in \mathbb{R}^{T\times d}$â€‹â€‹ 

  - The keys, queries and values are 

    - $q_i = W^Q x_i$ , where $W^Q \in \mathbb{R}^{d \times d}$ is the query weight matrix
      - $Q = XW^Q \in \mathbb{R}^{T\times d}$ ä»£è¡¨ Query çŸ©é˜µ
    - $k_i = W^K x_i$â€‹â€‹â€‹â€‹ , where $W^K \in \mathbb{R}^{d \times d}$â€‹â€‹â€‹â€‹â€‹ is the key weight matrix
      - $K = XW^K \in \mathbb{R}^{T\times d}$ ä»£è¡¨ Key çŸ©é˜µ
    - $v_i = W^V x_i$â€‹ , where $W^V \in \mathbb{R}^{d \times d}$â€‹ is the value weight matrix
      - $ V = XW^V \in \mathbb{R}^{T\times d}$ ä»£è¡¨ Value çŸ©é˜µ
    - These matrices allow different aspects of the ğ‘¥ vectors to be used/emphasized in each of the three roles. 

  - **Step1** query-key dot products

  - **Step 2** Softmax & weighted average
    $$
    \text{Attention}(Q, K, V) = \text{softmax}(QK^T)V.
    $$
    ä¸‹å›¾æˆªè‡ªPPTï¼ŒXQ å¯¹åº” Qï¼ŒXK å¯¹åº” Kï¼ŒXV å¯¹åº” V

    <img src="/Users/Wei/Documents/NLP/NLP/transformer/matrix_version.png" alt="matrix_version" style="zoom:50%;" />

### 2.1.2 Scaled Dot-Product Attention

#### 2.1.2.1 Architecture

<img src="/Users/Wei/Documents/NLP/NLP/transformer/scaled_dot_product.png" alt="scaled_dot_product" style="zoom:30%;" />

- æ•´ä½“ç»“æ„å’Œä¸Šé¢ä¸€è‡´ï¼Œå¢åŠ scale å±‚

#### 2.1.2.2 Why scaled?

- **Problem 1** in self-attention:  $QK^T$â€‹ æ–¹å·®å¤§ï¼Œå½±å“è®­ç»ƒç¨³å®šæ€§.
  - è®¾ $q\in \mathbb{R}^d$ æ˜¯ Q ä¸­ä¸€ä¸ªå•è¯çš„å‘é‡ï¼Œ $k\in \mathbb{R}^d$æ˜¯ K ä¸­ä¸€ä¸ªå•è¯çš„å‘é‡ã€‚
  - å‡è®¾ $q, k$ ä¸­çš„æ¯ä¸€ç»´éƒ½æ˜¯å‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸º 1 çš„ç‹¬ç«‹åŒåˆ†å¸ƒã€‚
  - ç”±ç»Ÿè®¡å­¦çŸ¥è¯†å¯å¾— $q\cdot k = \sum{i=1}^dq_ik_i$ çš„å‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸º $d_k$ 
  - å¦‚æœæ¨¡å‹ä¸­æŸä¸€å±‚çš„æ–¹å·®è¿‡å¤§æ—¶ï¼Œè®­ç»ƒå°±ä¼šä¸ç¨³å®šã€‚

- **Problem 2** When dimensionality ğ‘‘ becomes large, dot products between vectors tend to become large.

  - Because of this, inputs to the softmax function can be large, making the gradients small.

- **Solution: Scaled by $\sqrt{d}$â€‹** ä½¿å¾—æ–¹å·®å½’ 1

  - $$
    \text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d}})V.
    $$

#### 2.1.2.3 Illustration 

##### a. Elementwise

<img src="/Users/Wei/Documents/NLP/NLP/transformer/self-attention-output.png" alt="self-attention-output" style="zoom:80%;" /> 

##### b. Matrix

- Step 1<img src="/Users/Wei/Documents/NLP/NLP/transformer/self-attention-matrix-calculation.png" alt="self-attention-matrix-calculation" style="zoom:45%;" />-Step2<img src="/Users/Wei/Documents/NLP/NLP/transformer/self-attention-matrix-calculation-2.png" alt="self-attention-matrix-calculation-2" style="zoom:40%;" />

### 2.1.3 Multi-head attention (1)

#### 2.1.3.1 Why need multi-head?

-  **Value åŠ æƒæ±‚å’Œä¼šé™ä½è¯è¯­åˆ†è¾¨ç‡**
  - å¯¹äºä¸€ä¸ªå•è¯ï¼Œå¦‚æœä»–ä¾èµ–çš„å…¶ä»–å•è¯åªæœ‰ä¸€ä¸ªï¼Œé‚£åŠ æƒæ±‚å’Œæ•ˆæœå¾ˆå¥½ï¼Œä½†å®é™…æƒ…å†µæ˜¯å¤šä¸ªã€‚å³ä½¿å¤šä¸ªä¾èµ–å•è¯çš„æƒé‡éƒ½å¾ˆé«˜ï¼Œåœ¨åŠ æƒæ±‚å’Œåï¼Œç¥ç»ç½‘ç»œä¹Ÿæ— æ³•æ¸…æ™°åœ°ä½¿ç”¨å¤šä¸ªå•è¯ï¼Œåªèƒ½ç¬¼ç»Ÿåœ°ä½¿ç”¨ä¸€ä¸ªâ€œè¯­å¢ƒâ€ã€‚
- What if we want to look in multiple places in the sentence at once? 
  - For word ğ‘–, self-attention â€œlooksâ€ where $q_i^Tk_j$â€‹ is high, but maybe we want to focus on different ğ‘— for different reasons (query)?

#### 2.1.3.2 Solution

<img src="/Users/Wei/Documents/NLP/NLP/transformer/multi-head_attention.jpeg" alt="multi-head_attention" style="zoom:29%;" />

- Equation:
  $$
  \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h) W^O
  $$
  å…¶ä¸­  $\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$, 

### 2.1.4 Masked Multi-Head Attention (2)

#### 2.1.4.1 Why 'masked'?

- Need to ensure we donâ€™t â€œlook at the futureâ€ when predicting a sequence
- 

#### 2.1.4.2 Solution : masked

- <img src="/Users/Wei/Documents/NLP/NLP/transformer/masked.jpeg" alt="masked" style="zoom:50%;" />
- è®©å½“å‰å•è¯åŠä¹‹å‰çš„å•è¯å‘é‡é€šè¿‡ï¼Œå½“å‰å•è¯ä»¥åçš„è®¾ä¸º $-\infty$â€‹
- ä¸Šå›¾ä¸­ï¼Œåœ¨ Mask é‡Œé¢ï¼Œç°è‰²å¡«å……è¡¨ç¤ºé€šè¿‡ï¼Œç™½è‰²è¡¨ç¤ºå±è”½ï¼ŒAttention matrix ç»è¿‡ Mask åï¼Œé€šè¿‡çš„å…ƒç´ ä¸å˜ï¼Œä¸é€šè¿‡çš„å…ƒç´ ç”¨è´Ÿæ— ç©·æ›¿æ¢ã€‚

### 2.1.5 Encoder-Decoder Attention

<img src="/Users/Wei/Documents/NLP/NLP/transformer/encoder-decoder.png" alt="encoder-decoder" style="zoom:50%;" />

#### 2.1.5.1 Why need this layer?

**Ans** æŠŠ encoder çš„ç¼–ç ä¿¡æ¯ï¼ˆK, Vï¼‰èå…¥ decoder ä¸­ï¼š

- Kï¼ŒV æ¥è‡ªEncoder
- Q æ¥è‡ª Decoder

#### 2.1.5.2 Details

- Let $h_1, ..., h_T \in \mathbb{R}^d$â€‹â€‹ be output vectors from the Transformer **encoder**;
- Let $z_1, ..., z_T \in \mathbb{R}^d$â€‹ be input vectors from the Transformer **decoder**;
- keys and values are drawn from the **encoder** (like a memory): 
  - $ğ‘˜_ğ‘– = W^ğ¾â„_ğ‘– , ğ‘£_ğ‘– = W^ğ‘‰h_ğ‘–$
- queries are drawn from the decoder, $q_i = W^Qz_i$ .

## 2.2 Feed Forward Network 

- æ™®æ™®é€šé€šçš„å…¨è¿æ¥å±‚ï¼Œç”¨æ¥æ˜ å°„**æ¯ä¸ªå•è¯**çš„å‘é‡ã€‚Transformer ç”¨äº†ä¸¤ä¸ªå…¨è¿æ¥å±‚ï¼Œä¸­é—´ä½¿ç”¨äº†ä¸€ä¸ª ReLU æ¿€æ´»å‡½æ•°ï¼Œå…¬å¼å¦‚ä¸‹ï¼š
  $$
  \text{FFN}(x) = \max(0, xW_1+b_1)W_2+b_2
  $$

- **Question** ä¸ºä»€ä¹ˆéœ€è¦FFN

- **Ans** Note that there are **no elementwise nonlinearities** in self-attention

- stacking more self-attention layers just re-averages value vectors

<img src="/Users/Wei/Documents/NLP/NLP/transformer/ffn.png" alt="ffn" style="zoom:30%;" />

è¿™é‡Œéœ€è¦æ³¨æ„ä¸‹ï¼š

- Attention å¤„ç†å•è¯ä¹‹é—´çš„äº¤äº’;
- FFN åˆ™å¤„ç†ä¸€ä¸ªå•è¯å‘é‡çš„è¡¨ç¤ºï¼Œä¸æ¶‰åŠå•è¯äº¤äº’ï¼Œä»å…¬å¼ä¸Šå°±å¯ä»¥çœ‹å‡ºæ¥ï¼Œå…¬å¼é‡Œé¢åªæœ‰ä¸€ä¸ªå•è¯å‘é‡ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=x) ï¼Œæ²¡æœ‰å…¶ä»–å•è¯å‘é‡ã€‚

## 2.3 Add & Norm

### 2.3.1 Residual

- **ç›®çš„**ï¼šResidual connections are a trick to help models train better.

- <img src="/Users/Wei/Documents/NLP/NLP/transformer/residual.png" alt="residual" style="zoom:50%;" />

- $$
  X^{(i)} = X^{(i-1)} + \text{Layer}(X^{(i-1)})
  $$

- Residual connections are thought to make the loss landscape considerably smoother (thus easier training!)

- <img src="/Users/Wei/Documents/NLP/NLP/transformer/residual2.png" alt="residual2" style="zoom:50%;" />

### 2.3.2 Layer Normalization

- **ç›®çš„**ï¼šLayer normalization is a trick to help models **train faster**

- **Idea**: cut down on uninformative variation in hidden vector values by normalizing to unit mean and standard deviation **within each layer**

- **Equation**

  - $x\in \mathbb{R}^d$ : an individual (word) vector

  - $\mu = \frac{1}{d} \sum_{j=1}^d x_j \in \mathbb{R}$ is the mean

  - $\sigma = \sqrt{\frac{1}{d}\sum_{j=1}^d(x_j-\mu)^2} \in \mathbb{R}$â€‹â€‹â€‹ is the standard deviation

  - $\gamma \in \mathbb{R}^d$â€‹, $\beta \in \mathbb{R}^d$â€‹â€‹ are learned "gain" and "bias" parameters (can omit!)

  - Layer normalization:

    <img src="/Users/Wei/Documents/NLP/NLP/transformer/layer_normal.png" alt="layer_normal" style="zoom:50%;" />



### 2.3.3 Illutration

<img src="/Users/Wei/Documents/NLP/NLP/transformer/transformer_resideual_layer_norm_2.png" alt="transformer_resideual_layer_norm_2" style="zoom:50%;" />

## 2.4 Positional Encoding

### 2.4.1 Why need this?

- **Problem** self-attention is an **unordered function of its inputs**
- Idea:  Add position representations to the input to specify the sequence order
  - **sequence index** -- vector

### 2.4.2 Positional Encoding

Let $p_i \in \mathbb{R}^d$, for $i \in \{1, 2,...,T\}$â€‹â€‹ are the position vectors

=> word i (position = i) input æ›´æ–°ä¸º $x_i + p_i$â€‹

<img src="/Users/Wei/Documents/NLP/NLP/transformer/transformer_positional_encoding_vectors.png" alt="transformer_positional_encoding_vectors" style="zoom:50%;" />

### 2.4.3 Sinusoidal position representations

<img src="/Users/Wei/Documents/NLP/NLP/transformer/position1.png" alt="position1" style="zoom:50%;" />

- Pros:

  - Periodicity indicates that maybe â€œabsolute positionâ€ isnâ€™t as important 

    - ä¸¤ä¸ªé—´éš”ä¸ºkçš„ä½ç½®å‘é‡ï¼Œä»–ä»¬å¯ä»¥ç”±ä¸€ä¸ªåªä¸ k ç›¸å…³çš„çŸ©é˜µè¿›è¡Œè½¬æ¢

    - $$
      p_{i+k} = T(k)p_i
      $$

      å…¶ä¸­ $\lambda_{2i} = \frac{1}{10000^{2i/d}}$ , $T(k)$ å¦‚ä¸‹ åªä¸kç›¸å…³

      <img src="/Users/Wei/Documents/NLP/NLP/transformer/tk.png" alt="tk" style="zoom:50%;" />

      

  - Maybe can extrapolate to longer sequences as periods restart! 

- Cons:

  - Not learnable; also the extrapolation doesnâ€™t really work!



### 2.4.4 Learned absolute position representations

- Let all $p_i$ be learnable parameters! 
- Learn a matrix $p \in \mathbb{R}^{d\times T}$ , and let each $p_i$ be a column of that matrix!
- Pros:
  - Flexibility: each position gets to be learned to fit the data
- Cons: 
  - Definitely canâ€™t extrapolate to indices outside 1, â€¦ , ğ‘‡. 
- Most systems use this!

## 2.5 Output

<img src="/Users/Wei/Documents/NLP/NLP/transformer/transformer_decoder_output_softmax.png" alt="transformer_decoder_output_softmax" style="zoom:50%;" />



## 2.6 Summary

<img src="/Users/Wei/Documents/NLP/NLP/transformer/transformer_resideual_layer_norm_3.png" alt="transformer_resideual_layer_norm_3" style="zoom:80%;" />

# 3. Performance

- Machine Translation from the original Transformers paper!

  <img src="/Users/Wei/Documents/NLP/NLP/transformer/result_MT.png" alt="result_MT" style="zoom:50%;" />

-  Document generation

  <img src="/Users/Wei/Documents/NLP/NLP/transformer/result_DG.png" alt="result_DG" style="zoom:50%;" />

# 4. Drawbacks and variants of Transformers

## 4.1 Drawbacks

- **Quadratic compute in self-attention**: 
  - Computing all pairs of interactions means our computation grows **quadratically** with the sequence length! 
  - For recurrent models, it only grew linearly! 

- **Position representations**:
  - Are simple absolute indices the best we can do to represent position? 
  - Relative linear position attention [Shaw et al., 2018]
  - Dependency syntax-based position [Wang et al., 2019]



## 4.2 Quadratic computation

- The total number of operations grows as $O(T^2d)$, where T is the sequence length, d is the dimension

  â€‹	<img src="/Users/Wei/Documents/NLP/NLP/transformer/computation.png" alt="computation" style="zoom:50%;" />

- Bad when T is large (e.g. work on long documents, $T \geq 10,000$â€‹â€‹) 

- Can we build models like Transformers without paying the  $O(T^2)$â€‹ all-pairs self-attention cost? 
  - For example, Linformer [Wang et al., 2020]
    - <img src="/Users/Wei/Documents/NLP/NLP/transformer/linformer.png" alt="linformer" style="zoom:50%;" />
  - For example, BigBird [Zaheer et al., 2021]
    - <img src="/Users/Wei/Documents/NLP/NLP/transformer/bigbird.png" alt="bigbird" style="zoom:50%;" />







Transformer  = ä¸€ä¸ªå®Œå…¨ä¾èµ– Attention çš„æ¨¡å‹ï¼Œç”¨ Positional Encoding æ›¿ä»£äº† RNN çš„å¾ªç¯ç»“æ„ã€‚



## Attention

ä¸€å¥è¯ä¸­çš„è¯è¯­å­˜åœ¨ç›¸äº’ä¾èµ–å…³ç³»ï¼Œæ¯”å¦‚æŒ‡ä»£å…³ç³»ã€ä¿®é¥°å…³ç³»ç­‰ï¼Œæ‰€ä»¥å•ä¸ªè¯è¯­æ˜¯å¾ˆéš¾è¡¨è¾¾å®Œæ•´å«ä¹‰çš„ã€‚å› æ­¤åœ¨ NLP å¤„ç†ä¸­éœ€è¦æœ‰ä¸€ç§æœºåˆ¶æ¥åˆ»ç”»è¯è¯­ä¾èµ–ï¼Œè¿™ä¸ªæœºåˆ¶å°±æ˜¯ **Attention**ã€‚

æˆ‘ä»¬çŸ¥é“åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œæ¯ä¸ªå•è¯éƒ½è¢«æ˜ å°„æˆäº†ä¸€ä¸ªåµŒå…¥å‘é‡ï¼Œè¿™ä¸ªå‘é‡è•´å«ç€ä¸€ä¸ªå•è¯çš„å…¨éƒ¨å«ä¹‰ã€‚é‚£ä¹ˆ Attention è¦åšçš„å°±æ˜¯æŠŠå…¶ä»–ç›¸å…³å•è¯çš„ä¿¡æ¯èå…¥åˆ°å½“å‰çš„åµŒå…¥å‘é‡ä¸­ï¼Œè®©è¿™ä¸ªå•è¯çš„åµŒå…¥å‘é‡éšç€æ¨¡å‹å±‚æ•°çš„åŠ æ·±ï¼Œè¡¨è¿°åœ°æ›´åŠ æ¸…æ™°ã€å®Œæ•´ã€‚ 



self-attention:

you're re-expressing yourself in certain terms of a weighted combination of your entire neighborhood.

- attention is permutation invariantï¼ˆå› ä¸ºç”¨attentionæ¥è®°å½•ä¿¡æ¯ï¼Œä½†attentionä¸ä½ç½®æ— å…³ï¼Œæ‰€ä»¥è¦åŠ position representationï¼‰
  - å¢åŠ äº†position representation æ¥maintain order
  - 

## Transformer Network

### Motivation 

<img src="/Users/Wei/Documents/NLP/NLP/seq2seq/motivation.png" alt="motivation" style="zoom:50%;" />

- ä¸Šé¢çš„æ¨¡å‹éƒ½è¯´**sequential** çš„ï¼Œå³ä¸ºäº†é¢„æµ‹æœ€ç»ˆç»“æœï¼Œéœ€è¦ä¸€æ¬¡é¢„æµ‹ä¸€ä¸ªï¼›
- transformerå¯ä»¥å¹¶è¡Œè®¡ç®—



### Intuition : Attention + CNN

<img src="/Users/Wei/Documents/NLP/NLP/seq2seq/transformer_intuition.png" alt="transformer_intuition" style="zoom:50%;" />

### self-attention

<img src="/Users/Wei/Documents/NLP/NLP/seq2seq/self_attention_1.png" alt="self_attention_1" style="zoom:50%;" />

- query q: let you ask a question about the word 

  - $q^{<3>}$ = question at `I'Afrique` ,e.g. what's happending there

- key k: looks at all of the other words, and by the similarity to the query, 

  helps you figure out which words gives the most relevant answer to that question.  

  - $q^{<3>} \cdot k^{<1>}$ = how good is `Jane` as an answer to the question $q^{<3>}$

- value allows the representation to plug in how visite should be represented within A^3, within the representation of Africa. 

  - This allows you to come up with a representation for the word Africa that says this is Africa and someone is visiting Africa. 

- <img src="/Users/Wei/Documents/NLP/NLP/seq2seq/self_attention_2.png" alt="self_attention_2" style="zoom:50%;" />



### muti-head attention

- **Idea** is basically just a big for-loop over the self attention mechanism

<img src="/Users/Wei/Documents/NLP/NLP/seq2seq/multihead_attention_1.png" alt="multihead_attention_1" style="zoom:50%;" />

<img src="/Users/Wei/Documents/NLP/NLP/seq2seq/multihead_attention_2.png" alt="multihead_attention_2" style="zoom:50%;" />

- heads å¯ä»¥å¹¶è¡Œè®¡ç®—

### Transformer



