# 3. How to evaluate word vectors

## 3.1 æ¦‚è¿°

**Answer** Intrinsic vs. extrinsic å†…åœ¨ä¸å¤–åœ¨

- Intrinsic:
  - Evaluation on a specific/intermediate **subtask**
  - Fast to compute 
  - Helps to understand that system 
  - Not clear if really helpful unless correlation to real task is established 
- Extrinsic: 
  - Evaluation on a real task
  - Can take a long time to compute accuracy
  - Unclear if the subsystem is the problem or its interaction or other subsystems 
  - If replacing exactly one subsystem with another improves accuracy -> Winning!

## 3.2 Intrinsic word vector evaluation

### 3.2.1 Word Vector Analogies

<img src="./glove/word_vector_analogies.png" alt="word_vector_analogies" style="zoom:50%;" />

dea ğŸ’¡** Evaluate word vectors by how well their cosine distance after addition captures intuitive semantic and syntactic analogy questions ï¼ˆä½™å¼¦è·ç¦»èƒ½å¤šå¥½çš„æ•æ‰åˆ°è¯­ä¹‰å’Œå¥æ³•ç±»æ¯”é—®é¢˜ï¼‰

<img src="./glove/word_vector_analogies2.png" alt="word_vector_analogies2" style="zoom:30%;" />

- è¦ä»æœç´¢ä¸­å»æ‰inputå•è¯
- **Problem** å¦‚æœä¿¡æ¯éçº¿æ€§ï¼Œæ€ä¹ˆåŠï¼Ÿ[TODO]



## 3.3 Extrinsic word vector evaluation

- One example where good word vectors should help directly: **named entity recognition**

  - identifying references to a person, organization or location 

    <img src="/Users/Wei/Documents/NLP/NLP/glove/extrinsic_example.png" alt="extrinsic_example" style="zoom:50%;" />

    

    Table 4: F1 score on NER task with 50d vectors. Discrete is the baseline without word vectors. We use publicly-available vectors for HPCA, HSMN, and CW. See text for deta

# 1. å…±ç°çŸ©é˜µ Co-occurrence Matrix 

## 1.1. ä¸¤ç§å…±ç°çŸ©é˜µ

### 1.1.1 Word- Document Matrix

**å‡è®¾/çŒœæƒ³** å…³è”çš„å•è¯ä¼šç»å¸¸å‡ºç°åœ¨åŒä¸€ä¸ªæ–‡ç« ä¸­ã€‚

**ä¾‹å­**ğŸŒ° 

- "banks", "bonds", "stocks", "money", etc. å¯èƒ½ç»å¸¸å‡ºç°åœ¨åŒä¸€ä¸ªæ–‡ç« ä¸­ï¼›
-  "banks", "octopus", "banana", and "hockey"ä¸å¤§å¯èƒ½ä¼šè¿ç»­å‡ºç°ã€‚

**å®šä¹‰** $X  = \{X_{ij}\}$ æ˜¯ä¸€ä¸ªword- document matrixï¼š

1. åˆå§‹åŒ–Xä¸ºä¸€ä¸ª$\mathbb{R}^{|V|\times M}$çš„0çŸ©é˜µï¼ˆ$X_{ij} = 0$)ï¼Œå…¶ä¸­$|V|$æ˜¯è¯æ±‡é‡ï¼ŒMæ˜¯æ–‡ç« æ•°

2. éå†æ‰€æœ‰æ–‡ç« ï¼Œæ¯å½“å•è¯ $w_i$ å‡ºç°åœ¨æ–‡ç« jä¸­ï¼Œå°±$X_{ij}+= 1$ 

 **ç”¨é€”ä¸¾ä¾‹** general topics (all sports terms will have similar entries) leading to â€œLatent Semantic Analysisâ€

### 1.1.2 Window base co-occurence matrix

**Idea ğŸ’¡** è®¡ç®—æ¯ä¸ªå•è¯åœ¨ç‰¹å®šå¤§å°çš„çª—å£ä¸­å‡ºç°çš„æ¬¡æ•°ï¼Œå¾—åˆ°$\mathbb{R}^{|V|\times |V|}$çš„å…±ç°çŸ©é˜µ

1. ä¸word2vecç±»ä¼¼
2. å…³æ³¨å•è¯å‘¨å›´çš„æŸä¸ªwindowå†…çš„å•è¯ï¼Œè·å–ä¸€äº›è¯­æ³•å’Œè¯­ä¹‰ï¼ˆsyntactic and semanticï¼‰ä¿¡æ¯

**ä¾‹å­ğŸŒ°**

çª—å£é•¿åº¦ä¸º1ï¼Œæ•°æ®ä¸ºä»¥ä¸‹å¥å­

- I like deep learning

- I like NLP

- I enjoy flying

  <img src="./glove/cooccirence_example.png" alt="cooccirence_example" style="zoom:50%;" />



**ç¼ºç‚¹**

1. çŸ©é˜µç»´åº¦ç»å¸¸å‘ç”Ÿå˜åŒ–ï¼ˆæ–°å•è¯ï¼Œæ–°è¯­æ–™åº“çš„å¢åŠ ï¼‰
2. çŸ©é˜µç¨€ç–ï¼ˆå¾ˆå¤šå•è¯ä¸ä¼šå…±ç°ï¼‰
3. é«˜ç»´åº¦ $\approx 10^6 \times 10^6$
4. åŸºäºSVDçš„æ–¹æ³•è®¡ç®—å¤æ‚åº¦é«˜ï¼ˆ$m\times n$ çš„çŸ©é˜µè®¡ç®—SVDçš„å¤æ‚åº¦æ˜¯ $O(mn^2)$)
5. éœ€è¦åœ¨$X$ä¸ŠåŠ å…¥ä¸€äº›æŠ€å·§æ¥å¤„ç†è¯é¢‘çš„æç«¯ä¸å¹³è¡¡

**éƒ¨åˆ†è§£å†³æ–¹æ³•**

- å¿½ç•¥åŠŸèƒ½æ¬¡ï¼Œå¦‚"the","he","has"ç­‰
- ä½¿ç”¨ramp windowï¼Œå³æ ¹æ®æ–‡æ¡£ä¸­å•è¯ä¹‹é—´çš„è·ç¦»å¯¹å…±ç°è®¡æ•°è¿›è¡ŒåŠ æƒ(ä¸æ‡‚ï¼ŒTODO)
- ä½¿ç”¨ Pearson correlationå¹¶å°†è´Ÿè®¡æ•°è®¾ç½®ä¸º0ï¼Œè€Œä¸æ˜¯åŸå§‹è®¡æ•°



## 1.2 æ€ä¹ˆå®šä¹‰å…±ç°å‘é‡ ï¼ˆco-occurence vectorsï¼‰

### 1.2.1 ä¼ ç»Ÿæ–¹æ³• Dimensionality Reduction on X 

#### 1.2.1.1 Idea ğŸ’¡ 

- åœ¨ä¸€ä¸ªå›ºå®šçš„ä½ç»´çš„ç¨ å¯†å‘é‡ä¸­ï¼Œä¿å­˜***å¤§éƒ¨åˆ†***é‡è¦ä¿¡æ¯

#### 1.2.1.2 æ„é€ æ–¹æ³•

ä½¿ç”¨SVDæ–¹æ³•å°†å…±ç°çŸ©é˜µXåˆ†è§£ä¸º $U\Sigma V^T$ï¼Œå…¶ä¸­$\Sigma$æ˜¯ç‰¹å¾å€¼çŸ©é˜µï¼ŒUï¼ŒVæ˜¯å¯¹åº”äºè¡Œå’Œåˆ—çš„æ­£äº¤åŸºã€‚

<img src="./glove/svd.png" alt="svd" style="zoom:50%;" />

é€šè¿‡å–å‰kä¸ªæœ€å¤§çš„ç‰¹å¾å€¼ï¼Œå¯¹Xè¿›è¡Œé™ç»´ã€‚

<img src="./glove/svd2.png" alt="svd2" style="zoom:50%;" />



### 1.2.2 Hacks on X

- åœ¨åŸå§‹è®¡æ•°çŸ©é˜µä¸Šï¼Œç”¨SVDæ•ˆæœä¸å¥½ï¼
- æŒ‰æ¯”ä¾‹è°ƒæ•´è®¡æ•°å¯¹æ•ˆæœæœ‰å¾ˆå¤§æå‡-- **Scaling the counts**
  - Problem: åŠŸèƒ½è¯ï¼ˆå¦‚"the","he","has"ç­‰ï¼‰å‡ºç°é¢‘ç‡å¤ªé«˜ï¼Œå¯¼è‡´å¯¹è¯­æ³•æœ‰å¤ªå¤šå½±å“
    - $\log(\text{frequencies})$
    - $\min(x,t),  \text{ with } t = 100$ ï¼šå¯¹é«˜é¢‘è¯ï¼ˆé¢‘æ¬¡>tï¼‰è®¾ç½®å›ºå®šé¢‘æ¬¡
    - å¿½ç•¥åŠŸèƒ½è¯
- ä½¿ç”¨ramp windowï¼Œå³åŸºäºåœ¨æ–‡æ¡£ä¸­è¯ä¸è¯ä¹‹é—´çš„è·ç¦»ç»™å…±ç°è®¡æ•°åŠ ä¸Šä¸€ä¸ªæƒå€¼
- ä½¿ç”¨ Pearson correlationå¹¶å°†è´Ÿè®¡æ•°è®¾ç½®ä¸º0

**=> Idea ğŸ’¡** å¯¹è®¡æ•°è¿›è¡Œå¤„ç†æ˜¯å¯ä»¥å¾—åˆ°æœ‰æ•ˆçš„è¯å‘é‡çš„

<img src="./glove/coals_model.png" alt="coals_model" style="zoom:50%;" />

âš ï¸**Interesting semantic patterns emerge in the vectors**:

è¯­ä¹‰å‘é‡åŸºæœ¬ä¸Šæ˜¯***çº¿æ€§***çš„ï¼Œè™½ç„¶æœ‰ä¸€äº›æ‘†åŠ¨ï¼Œä½†æ˜¯åŸºæœ¬æ˜¯å­˜åœ¨åŠ¨è¯å’ŒåŠ¨è¯å®æ–½è€…çš„æ–¹å‘ã€‚

å¦‚æœèƒ½å¤Ÿå»ºç«‹è¿™æ ·çš„â€œçº¿æ€§å…³ç³»â€ï¼Œé‚£ä¹ˆå¯ä»¥å¾—åˆ°å¥½çš„ç±»æ¯”



### 1.2.3 æ€»ç»“&å¯¹æ¯”

ğŸ‘ˆï¼šåŸºäºè®¡æ•°çš„æ–¹æ³•ï¼šä½¿ç”¨æ•´ä¸ªçŸ©é˜µçš„å…¨å±€ç»Ÿè®¡æ•°æ®æ¥ç›´æ¥ä¼°è®¡

ğŸ‘‰ï¼šåŸºäºé¢„æµ‹çš„æ–¹æ³•ï¼šå®šä¹‰æ¦‚ç‡åˆ†å¸ƒå¹¶è¯•å›¾é¢„æµ‹å•è¯

<img src="./glove/count_baased_vs_prediction.png" alt="count_baased_vs_prediction" style="zoom:50%;" />

# 

# 2 Glove

## 2.1 Idea

ç›®æ ‡ï¼šæŠŠä¸Šè¿°ä¸¤ç§æ–¹æ³•ç»“åˆèµ·æ¥ï¼Œç”¨NN+æŸç§è®¡æ•°çŸ©é˜µ

**Crucial Insight** Ratio of co-occurrence probablilities can encode meaning components

<img src="./glove/co-occurrence_prob1.png" alt="co-occurrence_prob1" style="zoom:50%;" />



<img src="./glove/co-occurrence_prob2.png" alt="co-occurrence_prob2" style="zoom:50%;" />



é‡ç‚¹æ˜¯Difference between  co- occurrence probabilities

[TODO]**å¸Œæœ› Ratio of co- occurrence probabilities TO BE linear!**

Encoding meaning components in vector differences

## 2.2 æ¨¡å‹

**Question**: How can we capture ratios of co-occurrence probabilities as linear meaning components in a word vector space

**Answer**: Use a **Log-bilinear model**:
$$
w_i\cdot w_j = \log P(i|j)
$$
è¿™æ ·ï¼Œ vector differenceså°±æ˜¯
$$
w_x\cdot (w_a- w_b) = \log \frac{P(x|a)}{P(x|b)}
$$


## 2.3 ç›®æ ‡å‡½æ•°

[TODO]æ€ä¹ˆå¾—åˆ°çš„ï¼ˆref https://zhuanlan.zhihu.com/p/60208480ï¼‰
$$
J = \sum_{i, j = 1}^{V} f(X_{ij})\left(w_i^T \tilde{w}_j+b_i+\tilde{b}_j-\log X_{ij}\right)^2
$$

- Bias term å¦‚æœword common



## 2.4 æ¨¡å‹ç»“æœ

### 2.4.1 Nearest words

<img src="./glove/glove_result.png" alt="glove_result" style="zoom:50%;" />


### 2.4.2 Visualizations

#### 2.4.2.1 Women -- man

<img src="./glove/glove_visualization1.png" alt="glove_visualization1" style="zoom:50%;" />

#### 2.4.2.2 Company-CEO

<img src="./glove/glove_visualization2.png" alt="glove_visualization2" style="zoom:50%;" />

#### 2.4.2.3 Comparatives and Superlatives

<img src="./glove/glove_visualization3.png" alt="glove_visualization3" style="zoom:50%;" />



## 2.5 ä¸å…¶ä»–æ¨¡å‹å¯¹æ¯”

### 2.5.1 Word analogy task

#### 2.5.1.1 Taskæè¿°

Questions in the task like, 

>  â€œa is to b as c is to __ ?â€

The dataset contains 19,544 such questions,

- a semantic subset 
  - about people or places
  - ğŸŒ° â€œAthens is to Greece as Berlin is to __?â€
- a syntactic subset
  - verb tenses or forms of adjectives,
  - ğŸŒ°  â€œdance is to dancing as fly is to __ ?â€

æ¨¡å‹é€šè¿‡æ‰¾åˆ°æ»¡è¶³ä¸‹é¢æ¡ä»¶çš„å•è¯dï¼Œå›ç­”é—®é¢˜  â€œa is to b as c is to __ ?â€

- $w_d$ æ˜¯cosine similarity ä¸‹ï¼Œè·ç¦»$w_b-w_a+w_c$ æœ€è¿‘çš„å•è¯

#### 2.5.1.2 Accuracyç»“æœæ¯”è¾ƒ 

<img src="/Users/Wei/Documents/NLP/NLP/glove/results.png" alt="results" style="zoom:50%;" />

- Results= percent accuracy. 
- <u>Underlined scores</u> = best within groups of similarly-sized models;
- **Bold scores** are best overall. 
- HPCA vectors are publicly available
  - vLBL results are from (Mnih et al., 2013)
  - skip-gram (SG) and CBOW results are from (Mikolov et al., 2013a,b); 
  - we trained SGâ€  and CBOWâ€  using the word2vec tool
- Glove è¡¨ç°æœ€å¥½ï¼›
- SVDè¡¨ç°ä¸å¥½ï¼Œä½†å¯¹countè¿›è¡Œæ“ä½œåçš„SVD-Læ•ˆæœæ˜¾è‘—æé«˜
- dim å¢åŠ ï¼Œæ•ˆæœæ›´å¥½ï¼›



#### 2.5.1.3 Gloveçš„å‚æ•°

##### a. Dimension & window size

<img src="/Users/Wei/Documents/NLP/NLP/glove/parameters1.png" alt="parameters1" style="zoom:50%;" /> 

- Good dimension is ~300
- Semanticæ•ˆæœéšwindow sizeå¢åŠ è€Œæå‡
- asymmetric(ç”¨one-side window)æ•ˆæœä¸å¥½



##### b. More training time helps

<img src="/Users/Wei/Documents/NLP/NLP/glove/parameters2.png" alt="parameters2" style="zoom:30%;" />  More training time helps



##### c. Wikipedia is better than news text!

<img src="/Users/Wei/Documents/NLP/NLP/glove/parameters3.png" alt="parameters3" style="zoom:40%;" /> 

- More data helps
- Wikipedia is better than news text!
  - Wikipedia æœ¬èº«åŒ…å«å„ç§â€œå…³ç³»â€
- 

### 2.5.2 Word similarity task

#### 2.5.2.1 Taskæè¿°

Word vector distances and their correlation with human judgments

<img src="/Users/Wei/Documents/NLP/NLP/glove/word_similarity.png" alt="word_similarity" style="zoom:50%;" />

**ä¾‹å­**ğŸŒ°

<img src="/Users/Wei/Documents/NLP/NLP/glove/word_similarity2.png" alt="word_similarity2" style="zoom:20%;" />



<img src="/Users/Wei/Documents/NLP/NLP/glove/word_similarity_result.png" alt="word_similarity_result" style="zoom:50%;" />Table 3: Spearman rank correlation on word similarity tasks. All vectors are 300-dimensional. The CBOWâˆ— vectors are from the word2vec website and differ in that they contain phrase vectors.

Table 3 shows results on five different word similarity datasets. A similarity score is obtained from the word vectors by first normalizing each feature across the vocabulary and then calculating the cosine similarity. We compute Spearmanâ€™s rank correlation coefficient between this score and the human judgments. CBOWâˆ— denotes the vectors available on the word2vec website that are trained with word and phrase vectors on 100B words of news data. GloVe outperforms it while using a corpus less than half the size. Table 4 shows results on the NER task



# è¯­ä¹‰å’Œæ­§ä¹‰ 

- Word senses and word senses ambiguity

**Most words have lots of meanings!**

- Especially common words 
- Especially words that have existed for a long time 

**Question**  Does one vector capture all these meanings or do we have a mess?

**Example**: pike

- A sharp point or staff 
- A type of elongated fish 
- A railroad line or system 
- A type of road
- The future (coming down the pike)
- A type of body position (as in diving) 
- To kill or pierce with a pike 
- To make oneâ€™s way (pike along) 
- In Australian English, pike means to pull out from doing something: I reckon he could have climbed that cliff, but he piked!

**Solution** ç”¨pseudo word Improving Word Representations Via Global Context And Multiple Word Prototypes

**Idea**ğŸ’¡: Cluster word windows around words, retrain with each word assigned to multiple different clusters bank1, bank2, etc 

<img src="/Users/Wei/Documents/NLP/NLP/glove/ambiguity.png" alt="ambiguity" style="zoom:50%;" />

å¦‚ä¸Šå›¾ä¸­æ¡ƒçº¢è‰²å•è¯jaguar: jaguar1,...,jaguar4

**Problem** å¤§éƒ¨åˆ†æƒ…å†µä¸‹ï¼Œè¯ä¹‰ä¹‹å‰çš„åŒºåˆ†ä¸æ˜æ˜¾ï¼å¾ˆå¤šå•è¯çš„è¯æ„æ˜¯ç›¸å…³æˆ–è€…overlapçš„



**Anthoter Solution** Linear Algebraic Structure of Word Senses, with Applications to Polysemy 

- Different senses of a word reside in a linear superposition (weighted sum) in standard word embeddings like word2vec 

- $$
  v_{\text{pike}} = \alpha_1v_{\text{pike}_1}+\alpha_2v_{\text{pike}_2}+\alpha_3v_{\text{pike}_3}
  $$

-  å…¶ä¸­$\alpha_1 = \frac{f_1}{f_1+f_2+f_3}$ï¼Œfor frequency f

- **result** Because of ideas from sparse coding you can actually seperate out the sense 

-  [TODO]

- <img src="/Users/Wei/Documents/NLP/NLP/glove/ambiguity2.png" alt="ambiguity2" style="zoom:50%;" />

