# 1. å…±ç°çŸ©é˜µ Co-occurrence Matrix 

## 1.1. ä¸¤ç§å…±ç°çŸ©é˜µ

### 1.1.1 Word- Document Matrix

**å‡è®¾/çŒœæƒ³** å…³è”çš„å•è¯ä¼šç»å¸¸å‡ºç°åœ¨åŒä¸€ä¸ªæ–‡ç« ä¸­ã€‚

**ä¾‹å­**ğŸŒ° 

- "banks", "bonds", "stocks", "money", etc. å¯èƒ½ç»å¸¸å‡ºç°åœ¨åŒä¸€ä¸ªæ–‡ç« ä¸­ï¼›
-  "banks", "octopus", "banana", and "hockey"ä¸å¤§å¯èƒ½ä¼šè¿ç»­å‡ºç°ã€‚

**å®šä¹‰** $X  = \{X_{ij}\}$ æ˜¯ä¸€ä¸ªword- document matrixï¼š

1. åˆå§‹åŒ–Xä¸ºä¸€ä¸ª$\mathbb{R}^{|V|\times M}$çš„0çŸ©é˜µï¼ˆ$X_{ij} = 0$)ï¼Œå…¶ä¸­$|V|$æ˜¯è¯æ±‡é‡ï¼ŒMæ˜¯æ–‡ç« æ•°

2. éå†æ‰€æœ‰æ–‡ç« ï¼Œæ¯å½“å•è¯ $w_i$ å‡ºç°åœ¨æ–‡ç« jä¸­ï¼Œå°±$X_{ij}+= 1$ 

 **ç”¨é€”ä¸¾ä¾‹** general topics (all sports terms will have similar entries) leading to â€œLatent Semantic Analysisâ€

### 1.1.2 Window base co-occurence matrix

**Idea ğŸ’¡** è®¡ç®—æ¯ä¸ªå•è¯åœ¨ç‰¹å®šå¤§å°çš„çª—å£ä¸­å‡ºç°çš„æ¬¡æ•°ï¼Œå¾—åˆ°$\mathbb{R}^{|V|\times |V|}$çš„å…±ç°çŸ©é˜µ

1. ä¸word2vecç±»ä¼¼
2. å…³æ³¨å•è¯å‘¨å›´çš„æŸä¸ªwindowå†…çš„å•è¯ï¼Œè·å–ä¸€äº›è¯­æ³•å’Œè¯­ä¹‰ï¼ˆsyntactic and semanticï¼‰ä¿¡æ¯

**ä¾‹å­ğŸŒ°**

çª—å£é•¿åº¦ä¸º1ï¼Œæ•°æ®ä¸ºä»¥ä¸‹å¥å­

- I like deep learning

- I like NLP

- I enjoy flying

  <img src="/Users/weiwang/Documents/NLP/glove/cooccirence_example.png" alt="cooccirence_example" style="zoom:50%;" />



**ç¼ºç‚¹**

1. çŸ©é˜µç»´åº¦ç»å¸¸å‘ç”Ÿå˜åŒ–ï¼ˆæ–°å•è¯ï¼Œæ–°è¯­æ–™åº“çš„å¢åŠ ï¼‰
2. çŸ©é˜µç¨€ç–ï¼ˆå¾ˆå¤šå•è¯ä¸ä¼šå…±ç°ï¼‰
3. é«˜ç»´åº¦ $\approx 10^6 \times 10^6$
4. åŸºäºSVDçš„æ–¹æ³•è®¡ç®—å¤æ‚åº¦é«˜ï¼ˆ$m\times n$ çš„çŸ©é˜µè®¡ç®—SVDçš„å¤æ‚åº¦æ˜¯ $O(mn^2)$)
5. éœ€è¦åœ¨$X$ä¸ŠåŠ å…¥ä¸€äº›æŠ€å·§æ¥å¤„ç†è¯é¢‘çš„æç«¯ä¸å¹³è¡¡

**éƒ¨åˆ†è§£å†³æ–¹æ³•**

- å¿½ç•¥åŠŸèƒ½æ¬¡ï¼Œå¦‚"the","he","has"ç­‰
- ä½¿ç”¨ramp windowï¼Œå³æ ¹æ®æ–‡æ¡£ä¸­å•è¯ä¹‹é—´çš„è·ç¦»å¯¹å…±ç°è®¡æ•°è¿›è¡ŒåŠ æƒ(ä¸æ‡‚ï¼ŒTODO)
- ä½¿ç”¨ Pearson correlationå¹¶å°†è´Ÿè®¡æ•°è®¾ç½®ä¸º0ï¼Œè€Œä¸æ˜¯åŸå§‹è®¡æ•°



## 1.2 æ€ä¹ˆå®šä¹‰å…±ç°å‘é‡ ï¼ˆco-occurence vectorsï¼‰

### 1.2.1 ä¼ ç»Ÿæ–¹æ³• Dimensionality Reduction on X 

#### 1.2.1.1 Idea ğŸ’¡ 

- åœ¨ä¸€ä¸ªå›ºå®šçš„ä½ç»´çš„ç¨ å¯†å‘é‡ä¸­ï¼Œä¿å­˜***å¤§éƒ¨åˆ†***é‡è¦ä¿¡æ¯

#### 1.2.1.2 æ„é€ æ–¹æ³•

ä½¿ç”¨SVDæ–¹æ³•å°†å…±ç°çŸ©é˜µXåˆ†è§£ä¸º $U\Sigma V^T$ï¼Œå…¶ä¸­$\Sigma$æ˜¯ç‰¹å¾å€¼çŸ©é˜µï¼ŒUï¼ŒVæ˜¯å¯¹åº”äºè¡Œå’Œåˆ—çš„æ­£äº¤åŸºã€‚

<img src="/Users/weiwang/Documents/NLP/glove/svd.png" alt="svd" style="zoom:50%;" />

é€šè¿‡å–å‰kä¸ªæœ€å¤§çš„ç‰¹å¾å€¼ï¼Œå¯¹Xè¿›è¡Œé™ç»´ã€‚

<img src="/Users/weiwang/Documents/NLP/glove/svd2.png" alt="svd2" style="zoom:50%;" />



### 1.2.2 Hacks on X

- åœ¨åŸå§‹è®¡æ•°çŸ©é˜µä¸Šï¼Œç”¨SVDæ•ˆæœä¸å¥½ï¼
- æŒ‰æ¯”ä¾‹è°ƒæ•´è®¡æ•°å¯¹æ•ˆæœæœ‰å¾ˆå¤§æå‡-- **Scaling the counts**
  - Problem: åŠŸèƒ½è¯ï¼ˆå¦‚"the","he","has"ç­‰ï¼‰å‡ºç°é¢‘ç‡å¤ªé«˜ï¼Œå¯¼è‡´å¯¹è¯­æ³•æœ‰å¤ªå¤šå½±å“
    - $\log(\text{frequencies})$
    - $\min(x,t),  \text{ with } t = 100$ ï¼šå¯¹é«˜é¢‘è¯ï¼ˆé¢‘æ¬¡>tï¼‰è®¾ç½®å›ºå®šé¢‘æ¬¡
    - å¿½ç•¥åŠŸèƒ½è¯
- ä½¿ç”¨ramp windowï¼Œå³åŸºäºåœ¨æ–‡æ¡£ä¸­è¯ä¸è¯ä¹‹é—´çš„è·ç¦»ç»™å…±ç°è®¡æ•°åŠ ä¸Šä¸€ä¸ªæƒå€¼
- ä½¿ç”¨ Pearson correlationå¹¶å°†è´Ÿè®¡æ•°è®¾ç½®ä¸º0

**=> Idea ğŸ’¡** å¯¹è®¡æ•°è¿›è¡Œå¤„ç†æ˜¯å¯ä»¥å¾—åˆ°æœ‰æ•ˆçš„è¯å‘é‡çš„

<img src="/Users/weiwang/Documents/NLP/glove/coals_model.png" alt="coals_model" style="zoom:50%;" />

âš ï¸**Interesting semantic patterns emerge in the vectors**:

è¯­ä¹‰å‘é‡åŸºæœ¬ä¸Šæ˜¯***çº¿æ€§***çš„ï¼Œè™½ç„¶æœ‰ä¸€äº›æ‘†åŠ¨ï¼Œä½†æ˜¯åŸºæœ¬æ˜¯å­˜åœ¨åŠ¨è¯å’ŒåŠ¨è¯å®æ–½è€…çš„æ–¹å‘ã€‚

å¦‚æœèƒ½å¤Ÿå»ºç«‹è¿™æ ·çš„â€œçº¿æ€§å…³ç³»â€ï¼Œé‚£ä¹ˆå¯ä»¥å¾—åˆ°å¥½çš„ç±»æ¯”



### 1.2.3 æ€»ç»“&å¯¹æ¯”

ğŸ‘ˆï¼šåŸºäºè®¡æ•°çš„æ–¹æ³•ï¼šä½¿ç”¨æ•´ä¸ªçŸ©é˜µçš„å…¨å±€ç»Ÿè®¡æ•°æ®æ¥ç›´æ¥ä¼°è®¡

ğŸ‘‰ï¼šåŸºäºé¢„æµ‹çš„æ–¹æ³•ï¼šå®šä¹‰æ¦‚ç‡åˆ†å¸ƒå¹¶è¯•å›¾é¢„æµ‹å•è¯

<img src="/Users/weiwang/Documents/NLP/glove/count_baased_vs_prediction.png" alt="count_baased_vs_prediction" style="zoom:50%;" />

# 

# 2 Glove

## 2.1 Idea

ç›®æ ‡ï¼šæŠŠä¸Šè¿°ä¸¤ç§æ–¹æ³•ç»“åˆèµ·æ¥ï¼Œç”¨NN+æŸç§è®¡æ•°çŸ©é˜µ

**Crucial Insight** Ratio of co-occurrence probablilities can encode meaning components

<img src="/Users/weiwang/Documents/NLP/glove/co-occurrence_prob1.png" alt="co-occurrence_prob1" style="zoom:50%;" />



<img src="/Users/weiwang/Documents/NLP/glove/co-occurrence_prob2.png" alt="co-occurrence_prob2" style="zoom:50%;" />



é‡ç‚¹æ˜¯Difference between  co- occurrence probabilities

[TODO]**å¸Œæœ› Ratio of co- occurrence probabilities TO BE linear!**

Encoding meaning components in vector differences

## 2.2 æ¨¡å‹

**Question**: How can we capture ratios of co-occurrence probabilities as linear meaning components in a word vector space

**Answer**: Use a **Log-bilinear model**:
$$
w_i\cdot w_j = \log P(i|j)
$$
è¿™æ ·ï¼Œ vector differenceså°±æ˜¯
$$
w_x\cdot (w_a- w_b) = \log \frac{P(x|a)}{P(x|b)}
$$


## 2.3 ç›®æ ‡å‡½æ•°

[TODO]æ€ä¹ˆå¾—åˆ°çš„ï¼ˆref https://zhuanlan.zhihu.com/p/60208480ï¼‰
$$
J = \sum_{i, j = 1}^{V} f(X_{ij})\left(w_i^T \tilde{w}_j+b_i+\tilde{b}_j-\log X_{ij}\right)^2
$$

- Bias term å¦‚æœword common



## 2.4 æ¨¡å‹ç»“æœ

### 2.4.1 Nearest words

<img src="/Users/weiwang/Documents/NLP/glove/glove_result.png" alt="glove_result" style="zoom:50%;" />


### 2.4.2 Visualizations

#### 2.4.2.1 Women -- man

<img src="/Users/weiwang/Documents/NLP/glove/glove_visualization1.png" alt="glove_visualization1" style="zoom:50%;" />

#### 2.4.2.2 Company-CEO

<img src="/Users/weiwang/Documents/NLP/glove/glove_visualization2.png" alt="glove_visualization2" style="zoom:50%;" />

#### 2.4.2.3 Comparatives and Superlatives

<img src="/Users/weiwang/Documents/NLP/glove/glove_visualization3.png" alt="glove_visualization3" style="zoom:50%;" />
# 3. How to evaluate word vectors

## 3.1 æ¦‚è¿°

**Answer** Intrinsic vs. extrinsic å†…åœ¨ä¸å¤–åœ¨

- Intrinsic:
  - Evaluation on a specific/intermediate **subtask**
  - Fast to compute 
  - Helps to understand that system 
  - Not clear if really helpful unless correlation to real task is established 
- Extrinsic: 
  - Evaluation on a real task
  - Can take a long time to compute accuracy
  - Unclear if the subsystem is the problem or its interaction or other subsystems 
  - If replacing exactly one subsystem with another improves accuracy -> Winning!

## 3.2 Intrinsic word vector evaluation

### 3.2.1 Word Vector Analogies
<img src="/Users/weiwang/Documents/NLP/glove/word_vector_analogies.png" alt="word_vector_analogies" style="zoom:50%;" />

dea ğŸ’¡** Evaluate word vectors by how well their cosine distance after addition captures intuitive semantic and syntactic analogy questions ï¼ˆä½™å¼¦è·ç¦»èƒ½å¤šå¥½çš„æ•æ‰åˆ°è¯­ä¹‰å’Œå¥æ³•ç±»æ¯”é—®é¢˜ï¼‰

<img src="/Users/weiwang/Documents/NLP/glove/word_vector_analogies2.png" alt="word_vector_analogies2" style="zoom:30%;" />

- è¦ä»æœç´¢ä¸­å»æ‰inputå•è¯
- **Problem** å¦‚æœä¿¡æ¯éçº¿æ€§ï¼Œæ€ä¹ˆåŠï¼Ÿ[TODO]