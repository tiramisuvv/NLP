# Neural Networ & Backpropagation



<img src="/Users/Wei/Library/Application Support/typora-user-images/Screen Shot 2021-07-03 at 5.37.57 PM.png" alt="Screen Shot 2021-07-03 at 5.37.57 PM" style="zoom:50%;" />

# Classification in traditional ML and NLP

<img src="/Users/Wei/Library/Application Support/typora-user-images/Screen Shot 2021-07-03 at 5.42.28 PM.png" alt="Screen Shot 2021-07-03 at 5.42.28 PM" style="zoom:50%;" />

word window classification - named entity recognition 

Nll - negtive log likelihood

cross-entropy loss æ›´å¥½ç”¨ ã€TODOã€‘why

**Def**

True probability distribution p

Predicte probability distribution q

H(p,q )ã€TODOã€‘ meaning?

ã€TODOã€‘ä¸ºä»€ä¹ˆlogistic regressionç”»å‡ºæ¥ æ˜¯linear çš„ï¼Ÿ

word vector Representation learning 

$\theta$= W :weight + $v_w$:word vector

ã€Qã€‘ åœ¨NNé‡Œè®­ç»ƒword vectorï¼Ÿ



## Named Entity Recognition(NER)

Taskï¼šfind the classify names in text:

<img src="/Users/Wei/Library/Application Support/typora-user-images/Screen Shot 2021-07-03 at 5.44.49 PM.png" alt="Screen Shot 2021-07-03 at 5.44.49 PM" style="zoom:50%;" />

Why difficult 

Solution: Idea Window classification

Binary classification









## Gradient by hand - Matrix calculate 





1. n input &1 output $f(x) = f(x1, ..., x_n)$, $\frac{\part f}{\part x} = \left[\frac{\part f}{\part x_1},...,\frac{\part f}{\part x_n}\right]$
2. n input & m output $f(x) = [f_1(x1, ..., x_n),...,f_m(x1, ..., x_n)]$, ...
3. $\delta$ error signal = åå¯¼ above parameter = error signal coming above
4. shape convition 
5. æ ¹æ®$\delta_{window}$ æ¯æ¬¡æ›´æ–°word vector

**Questions** æ˜¯å¦ç”¨â€œpre-trainedâ€ word vectorsï¼Ÿ

**Answer** Almost always, yes!

![Screen Shot 2021-07-03 at 7.26.15 AM](/Users/Wei/Library/Application Support/typora-user-images/Screen Shot 2021-07-03 at 7.26.15 AM.png)

- small data set <= å‡ ç™¾k
- large dataset > 1M



### Computation Graph

<img src="/Users/Wei/Documents/NLP/NLP/NN/computation_graph.png" alt="computation_graph" style="zoom:50%;" />

- Node with one input & one output
- [Downstram gradient] = [upstream gradient] x [local graident]

<img src="/Users/Wei/Documents/NLP/NLP/NN/single_node_1in1out.png" alt="single_node" style="zoom:50%;" />

- Node with multiple input & one output 

  <img src="/Users/Wei/Documents/NLP/NLP/NN/single_node_mulin1out.png" alt="single_node_mulin1out" style="zoom:50%;" />

- ğŸŒ°

<img src="/Users/Wei/Documents/NLP/NLP/NN/backprog_example.png" alt="backprog_example" style="zoom:50%;" />

- Back prop = if you wiggle x a little bit, how big an effect does that have on the outcome of the whole thing

- æ¯”å¦‚ xä»$1\rightarrow 1.1$, é‚£ä¹ˆæœŸå¾…output ä¼šå˜åŒ– $\frac{\part f}{\part x}\cdot \Delta x = 2 \times 0.1 = +0.2$ï¼Œå®é™…output = (1.1+2) max(2, 0) = 6.2ï¼Œå˜åŒ–ä¸º $+0.2$

- æ¯”å¦‚ y ä» $2\rightarrow 2.1$ï¼Œ

  - æœŸå¾…outputå˜åŒ–ï¼š$5\times 0.1 = 0.5$
  - å®é™… $\Delta \text{output} = (1 + 2.1)\max(2.1, 0) - 6= 6.51 - 6 = 0.51$
  - æ³¨æ„è¿™é‡Œæ˜¯é¢„ä¼°å˜åŒ–ï¼Œä¸æ˜¯å‡†ç¡®å€¼

- å„ä¸ªè®¡ç®—çš„back propçš„å«ä¹‰

  - **plus +**  â€œdistributesâ€ the upstream gradient to each summand
  - **max** â€œroutesâ€ the upstream gradient
  - **Product *** â€œswitchesâ€ the upstream gradient

- multiple output

  <img src="/Users/Wei/Documents/NLP/NLP/NN/multi_output.png" alt="multi_output" style="zoom:50%;" />

- Efficiency: compute all gradients at once

<img src="/Users/Wei/Documents/NLP/NLP/NN/Efficiency.png" alt="Efficiency" style="zoom:50%;" />

- Backprop
  1. æŒ‰ç…§Topological sortæ’åº
     1. ä¾èµ–åœ¨ä¸Š

<img src="/Users/Wei/Documents/NLP/NLP/NN/backprop.png" alt="backprop" style="zoom:50%;" />

<img src="/Users/Wei/Documents/NLP/NLP/NN/AutomaticDifferentiation.png" alt="AutomaticDifferentiation" style="zoom:50%;" />

