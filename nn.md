# Neural Networ & Backpropagation



word window classification - entity recognition 

Nll - negtive log likelihood

cross-entropy loss 更好用 【TODO】why

**Def**

True probability distribution p

Predicte probability distribution q

H(p,q )【TODO】 meaning?

【TODO】为什么logistic regression画出来 是linear 的？

word vector Representation learning 

$\theta$= W :weight + $v_w$:word vector

【Q】 在NN里训练word vector？



Named Entity Recognition

Task：find the classify names in text:

Why difficult 



Gradient by hand - Matrix calculate 

1. n input &1 output $f(x) = f(x1, ..., x_n)$, $\frac{\part f}{\part x} = \left[\frac{\part f}{\part x_1},...,\frac{\part f}{\part x_n}\right]$
2. n input & m output $f(x) = [f_1(x1, ..., x_n),...,f_m(x1, ..., x_n)]$, ...
3. $\delta$ error signal = 偏导 above parameter
4. 

